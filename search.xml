<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记录之Linux常用命令]]></title>
    <url>%2F2019%2F04%2F07%2F%E8%AE%B0%E5%BD%95%E4%B9%8BLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[好记性不如烂笔头，记录一下常用的Linux 命令 1、find语法 1sudo find path -name file_name 案例：查找 根目录下的mysql相关文件 1sudo find / -name mysql 2、lsof语法 1sudo lsof -i:port 案例：查找8000端口对应的进程PID号 1sudo lsof -i:8000 3、nc nc是netcat的简写，能够实现任意TCP/UDP端口的监听,nc可以作为server以TCP或UDP方式侦听指定端口 语法 1nc -l port 案例：服务器监听9000端口 1nc -l 9000 4、telnet命令 用于远程登陆连接 语法 1telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 实例:登陆IP为xx的远程主机 1telnet 192.168.36.1]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之19年学习技术栈]]></title>
    <url>%2F2019%2F01%2F01%2F%E8%AE%B0%E5%BD%95%E4%B9%8B18%E5%B9%B4%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[记录一下今年的学习目标，Fighting！！！ [x] 爬虫回顾 requests scrapy 目标：基本使用 [ ] Web框架延伸 1、Django&amp;Django-rest-framework 目标：源码尝试阅读2、第三方组件集成学习（channel）3、websocket Flask 目标Flask复习 Tornado 目标：学习使用 Bottle 目标：基本使用 [ ] 数据分析 numpy pandas 目标：基本使用API语法熟悉 [ ] 机器学习 常用算法 特征工程 推荐算法 基本使用 [ ] 数据结构&amp;算法 常用的设计模式 基本算法解决思路 多了解 [ ] 新语言学习 C语言 Java语言 Go语言 目标：入门即可，了解底层语言帮助自己对Python理解 [ ] 数据库 SQL redis 反复练习 [ ] 运维 Docker K8S Supervisor Shell 基本使用]]></content>
      <categories>
        <category>记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[纪念金庸]]></title>
    <url>%2F2018%2F10%2F30%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[飞雪连天射白鹿，笑书神侠倚碧鸳。 侠之大者，为国为民。 红颜弹指老，刹那芳华，与其天涯思君，恋恋不舍，莫若相忘于江湖。——金庸《天龙八部》 四张机，鸳鸯织就欲双飞，可怜未老头先白，春波碧草，晓寒深处，相对浴红衣。——金庸《射雕英雄传》 一座山，隔不了两两相思，一天涯，断不了两两无言，且听风吟，吟不完我一生思念。——金庸《神雕侠侣》 他强由他强，清风拂山岗；他横任他横，明月照大江。——金庸《倚天屠龙记》 情不知所起，一往情深；恨不知所终，一笑而泯。——金庸《笑傲江湖》]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之常见的内置函数]]></title>
    <url>%2F2018%2F10%2F24%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Python当中有很多的内置函数，在写代码的过程当中如果能够灵活运用，可以提高开发效率,今天就总结一下～ 文章Python环境为Py3.x filter 语法123# 第一个参数：是一个函数名称# 第二个参数：是一个可迭代对象filter(function, sequence) 执行过程function函数会一个一个调用sequence里面的item参数，然后执行function(item)，最后将执行结果为True的元素组成新的迭代器返回 案例说明12345678910111213foo = [1, 2, 3, 4, 5, 6]new_foo = list(filter(lambda x: x &gt; 3, foo))print(new_foo) # [4, 5, 6]# 等同于def func(item): return item &gt; 3new_iterable = filter(func, foo)print(new_iterable) # &lt;filter object at 0x105e23748&gt;new_foo = list(new_iterable)print(new_foo) # [4, 5, 6] zip() 语法1zip(iterable1, iteable2, ...) 执行过程zip会将 所有iterable里面的 item元素按照索引顺序，一一打包成元组，最终将元组组合成列表 案例说明12345l1 = [1, 2, 3, 4]l2 = [9, 10, 11, 12]ret = list(zip(l1, l2))print(ret) # [(1, 9), (2, 10), (3, 11), (4, 12)] Map()语法1map(func, sequence) 执行过程func函数会遍历sequence里面所有的 item元素，逐个执行 func(item)，并将返回的结果组成新的 map对象返回 案例说明12345# 列表所有元素平方l1 = [1, 2, 3, 4]m = map(lambda x: x * x, l1)print(list(m)) reduce()语法12from functools import reducereduce(func, sequence) 执行过程func函数会先从 sequence中取出前2个 item元素执行 func(item1,item2),把拿到的返回结果当作下一次执行 func函数的第一个参数，然后再从 sequence中取第三个 item当执行的第二个参数，后面以此类推 案例说明123456# l1列表所有元素的乘积l1 = [1, 2, 3, 4, 5, 6]from functools import reduceret = reduce(lambda x, y: x * y, l1)print(ret) # 720]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>zip</tag>
        <tag>filter</tag>
        <tag>map</tag>
        <tag>reduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于AWS服务器利用sandowsocks科学上网]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%9F%BA%E4%BA%8EAWS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%A9%E7%94%A8sandowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一直都想找Google和AWS这种大BOSS撸羊毛，但是苦于没有信用卡，也不想走淘宝买卡害怕被套路，偶然机会需要出国旅游，所以匆匆去办了一张中信的 Visa信用卡（欧洲用现金、美国用信用卡、中国在用支付宝，国外很流行信用卡），回国以后就赶紧草草注册了AWS，开启科学上网～ 一、注册AWS并创建Ec2实例1、注册地址：https://www.amazonaws.cn/sign-up/&gt; 2、注册需要条件 信用卡（必备，我用的visa双币卡，注册后扣除1$） 手机号 邮箱 备注：注册成功以后需要等待一天验证身份什么的，反正我当初等了一天才能去创建Ec2 3、申请成功，创建Ec2实例 创建实例我选择的是 Ubuntu16.40因为对ubuntu系统比较熟悉 参考链接：https://blog.csdn.net/jewely/article/details/78030057 保存好私钥文件，文件结尾是xxx.pem 二、连接服务器1、修改一下本地私钥文件权限 12chmod 400 xxx.pem# 备注 权限太高，后面ssh的时候连接不了会提示不安全 2、ssh连接 1ssh -i "xxx.pem" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com xxx.pem 就是你下载下来的私钥文件 xxx这个名字是当时下载时写的，大家按自己的来 @前面的那个ubuntu应该是固定的，我选的就是ubuntu系统 @后面的就是AWS上你创建的实例的公有DNS（IPv4）名称 或者写 IPv4的公有IP也可以 我两个都尝试了 It`s OK 3、修改一下root用户密码 1sudo passwd root 三、安装shadowsocks并配置默认ubuntu16.04已经预装了Python3 1、更新apt-get 1apt-get update 2、安装pip3（我用惯了Python3） 1apt-get install python-pip3 3、安装shadowsocks 1pip3 install shadowsocks 4、创建配置文件 1sudo touch /etc/shadowsocks.json 5、添加配置内容 1234567891011&#123; "server": "0.0.0.0", // 服务端IP 0.0.0.0即可方便连接 "server_port": 50003, // 服务器端口方便后期客户端连接 "local_address": "127.0.0.1", "local_port": 1080, "password": "******", //连接服务器的密码，自己设置，客户端连接时候要填写 "timeout": 300, "method": "aes-256-cfb", // 加密方式 "fast_open": false, "workers": 1&#125; 配置以后启动shadowsocks 1sudo ssserver -c /etc/shadowsocks.json -d start 如果遇到permission denied错误解决方法如下 1234# 第一步查看sserver的位置 which ssserver # 第二步sudo 填写完整的ssserver路径 -c /etc/shadowsocks.json -d start 6、修改AWS上EC2实例的入站端口 配置好 shaodowsocks 后，还需要将配置中的端口打开, 这样客户端的服务才能链接得上 EC2 中的 shadowsocks 服务，首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则 四、客户端连接&amp;科学上网1、打开本地shadowsocks客户端 2、填写信息 3、上网 遇到问题的可以留言区留言，一起看看 配置多用户上网]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>sandowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之shell学习]]></title>
    <url>%2F2018%2F10%2F09%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bshell%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[最近在学习 shell，开一篇文章记录一下 目的：掌握shell的基本语法，编程思想是不变的，不同的语言只是语法不一样 变量的定义 局部变量局部变量作用域只是在当前的sh文件or当前的终端内 1234# 新建var.sh#!/bin/basha=helloworldecho $a 执行 123456# 第一种方式执行bash var.sh# 第二种方式执行 ./var.sh 前提是文件有可执行权限 chmod u+x var.sh# 第三种方式执行source var.sh 输出 1helloworld 备注1：shell变量赋值 ‘=’两边不能有空格 备注2: shell变量赋的值如果不是连续的，有空格的需要使用单/双引号 a=&#39;hello world&#39;,这时候需要引号包裹‘ 备注3: shell变量赋的值如果里面有另外的变量 12345key='test'a="hello $key world" # 注意这里面一定要用双引号包裹echo a# 输出hello test world #####系统命令变量定义 先执行命令，将命令执行之后的结果存到变量当中 1234cmd=`ls`orcmd=$(ls)echo $cmd 输出 1当前目录文件效果同 `ls` 全局变量 系统所有环境都可以使用的变量 查看方法：env 定义方式 分步骤定义 定义一个本地变量 12# 变量名=变量值name=qiangzai 声明为全局变量 12# export 变量名export name 同时定义 export 变量名=变量值 1export 变量名=变量值 内置变量 bash命令内部已经定义好的变量，可以直接使用，不需要定义 使用方法 和shell脚本有关的内置变量 $0 获取当前脚本名称 $# 获取当前脚本的参数个数 $n 获取当前脚本获取到的第n个参数 $? 获取上一次命令的执行情况，0表示成功 $$ 脚本运行时使用的进程号 $@ 获取所有的参数 和字符串相关的内置变量 字符串切割 12$&#123;var_name:start:n&#125;# var_name是字符串变量名 start正数 从开头开始 n表示截取字符的个数 案例：截取字符串’abcdefg’,从c开始截取2个字符 12alpha="abcdefg"echo $&#123;alpha:3:2&#125; 和默认值相关的内置变量 第一种 1234# 获取脚本第一个参数var_name=$1# defalut表示默认值，如果没有输入参数，default将会被使用$&#123;var_name:-default&#125; 第二种 123var_name=$1# 将会无视输入参数，直接输出设定好的默认值$&#123;var_name:+default&#125; 查看变量的3种方式123456# 定义一个变量name=qinagzai# 打印一下变量echo $name echo "$name"echo "&#123;$name&#125;" # 推荐使用，最规范 删除变量&amp;设置变量只读12unset 变量名readonly 变量名 语法学习 验证表达式 方式一 1[ 表达式 ] 方式二 1test 表达式 表达式两侧必须要有空格，表达式之间需要有空格，不然表达式表示的是一个整体 案例 12345678910# 获取参数个数arg_nums=$## 方式一验证[ "$&#123;arg_nums&#125;" -eq 3 ] # 表示脚本参数不等于3# 方式二test "$&#123;arg_nums&#125;" -eq 3 # 意思同上# 打印执行结果 1表示验证通过，0表示失败echo $? 逻辑表达式 -eq //等于 -ne //不等于 -gt //大于 （greater ） -lt //小于 （less） -ge //大于等于 -le //小于等于 命令的逻辑关系： 在linux 中 命令执行状态：0 为真，其他为假 &amp;&amp;并 命令1&amp;&amp;命令2 如果命令1执行成功，则执行命令2 如果命令1执行失败，则不执行命令2 || 或 命令1||命令2 如果命令1执行成功，则不执行命令2 如果命令1执行不成功，则执行命令2 shell文件表达式 -e 判断输入的内容表示的文件是否存在 -f判断输入的内容是否是一个文件 -d判断输入的内容是否是一个目录 -x判断输入的文件是否有可执行权限 -r判断文件是否可读 -w判断文件是否可写 12# 使用方式 注意有空格[ 文件表达式 文件名 ] sehll字符串表达式 == 判断两个字符串是否相等 ！= 判断两个字符串是否不一致 -z判断字符串是否为0 -n判断字符串长度是否不为0 shell流程控制 单if语句 1234if 条件语句then 执行语句fi if else语句 123456if 条件语句1then 执行语句1else 执行语句2fi 双if语句 123456789if 条件语句1then 执行语句1elif 条件语句2then 执行语句2else 执行语句3fi case语句 1234567891011case 值 in 值1） 执行语句1 ;; 值2） 执行语句2 ;; 值3） 执行语句3 ;;esac 案例 1234567891011121314151617#!/usr/bininstruct=$1case "$&#123;instruct&#125;" in start) echo "系统启动" ;; stop) echo "系统关闭" ;; reload) echo "系统重启" ;; *) echo "启动方式是：sh $0 [start|stop|reload]"esac 循环控制语句1234for 条件do 执行语句done Shell中的函数 1234567891011121314# 无参数格式func_name()&#123; 函数体&#125;#调用func_name# 有参数格式func_name()&#123; args=$n 函数体&#125;# 调用func_name args1 args2]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之常见问题总结]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目的：记录爬虫中遇到的坑…… xpath提取数据遇到tbody标签 问题描述：今天在用scrapy爬取网页时，当爬取表格内容时，发现使用插件 xpath helper获取正常，程序中的xpath解析不到 问题原因：浏览器会在table标签下添加tbody标签，浏览器会对html文件进行一定的规范化，但是源码当中是没有tbody标签的。 解决办法：将分析的xpath语句中把tbody去掉即可 Scrapyd运行后被拒绝访问 问题描述：我在腾讯服务器运行scrapyd项目部署，启动以后访问6800端口被拒绝，第一反应就是腾讯云的安全组配置问题，后来发现自己的出站端口6800早已打开。翻了翻日志，看到下面这句代码 1[-] Scrapyd web console available at http://127.0.0.1:6800/ 哈哈，尴尬的127.0.0.1，判断需要修改scrapyd的配置文件 解决问题 1、找到配置文件，先找Python的第三方库 scrapyd 安装目录 我这里找了半天没找到，就是用 find大法了 sudo find / -name scrapyd 2、找到目录，下一步就是 cd到安装目录下 1cd /usr/....../scrapyd 3、找到 default_scrapyd.conf，并打开修改 1bind_address = 127.0.0.1 为 1bind_address = 0.0.0.0 然后保存退出，重试一下就ok！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>spider_error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Scrapy框架]]></title>
    <url>%2F2018%2F05%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Scrapy是一个高性能的爬虫框架，主要是为了提取结构化数据而编写的应用框架，我们只需要实现少量代码就可以实现数据的快速获取，框架底层使用的Twisted异步网络框架,所以爬取速度非常的快。 文章目的：能够利用scrapy爬取数据 安装1pip install scrapy 查看常用操作(检查是否安装成功) 1scrapy --help 基本使用流程 第一步：创建项目（固定步骤） 1scrapy startproject 项目名称 第二步：创建爬虫（固定步骤） 12cd 项目目录scrapy genspider 爬虫名称 目标域名 第三步：编写爬虫组件和管道（主要编写的部分） 1# 在第二步创建的爬虫文件当中编写 第四步：启动爬虫（固定步骤） 1scrapy crawl 爬虫名称 scrapy组成 运行流程图（scrapy官方公布） 各部件作用 五大组件 引擎 中央协调 爬虫组件 提取数据 or url，生成请求对象 请求调度器 存放请求对象 下载器 发送请求获取响应 管道 保存数据 三大对象 Request请求对象 Response响应对象 Item数据对象 两大中间件 下载中间件 爬虫中间件 项目目录结构介绍 spider-project项目目录 __init__.py items.py定义数据模型 middlewares.py自定义中间件 pipelines.py自定义管道，保存数据 settings.py项目配置文件 scrapy.cfg项目部署文件 spiders爬虫组件目录 xxx.py具体实现爬虫的文件 通过 scrapy genspider 爬虫名称–&gt;生成的爬虫都在这个目录下 五大组件之爬虫组件 作用：提取数据 | 提取url返回请求对象 具体编写步骤1、继承爬虫类 123456# -*- coding: utf-8 -*-# 导入模块import scrapy# 继承爬虫类class ExampleSpider(scrapy.Spider): ... 2、定义爬虫名称 1name = 'example' 3、设置允许爬取的范围 1allowed_domains = ['example.com'] 4、设置开始爬取的请求地址 1start_urls = ['https://wxample.com/xxxx'] 5、实现解析函数 ​ 1、提取数据 ​ 2、提取url，返回请求对象 12def parse(self, response): pass 响应对象response基本信息 response.status &gt;&gt; 响应状态码 response.headers &gt;&gt; 响应头 response.text&gt;&gt; 响应内容 response.request.cookies&gt;&gt;获取cookies Selector对象基本操作 extract()提取数据，提取不到会抛出下标异常 extract_first()提取第一条数据，提取不到返回None 提取&amp;构建请求对象12345# 构建请求对象request = scrapy.Request(url='xxxx')# 返回请求对象&gt;&gt;引擎&gt;&gt;请求调度（scheduler）器&gt;&gt;请求下载器yield request 参数callback指定请求对象的解析函数 参数 meta把数据可以传递给解析函数 12345yield scrapy.Request(url=detail_url, callback=self.parse_detail, meta=&#123; 'item':item &#125;) 五大组件之管道 系统管道1、提取函数中使用 yield返回函数 2、命令行导出 1scrapy crawl 爬虫名称 -o 导出文件名称 自定义管道1、在 pipelines.py编写管道 ​ 1、创建管道类 ​ 2、实现管道处理数据方法 ​ 2.1 process_item必须实现 ​ 2.2 open_spider 可选实现，当爬虫启动时调用一次 ​ 2.3 close_spider 可选实现，当爬虫结束时调用一次 12345678# pipelines.pyclass JsonDataPipeline(object): '''创建管道类''' def process_item(self, item, spider): '''实现管道处理数据方法 --&gt;yield item--&gt;引擎--&gt;触发process_item方法 ''' pass 2、在 settings.py中开启管道 1234ITEM_PIPELINEs = &#123; # 在里面配置自己定义的管道类 'xxx.pipelines.定义的管道类名称':score # score 表示数字和权重&#125; 注意点 1、数字越小越优先执行 2、多管道 item传递必须在 process_item函数中返回 item 自定义数据模型 作用：规范数据的格式 定义文件位置 items.py 123456import scrapyclass dataItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pass 使用 123456# 导入items.py里面定义的模型类from xxx.items import dataItem# 创建模型对象data = dataItem()# 给对象添加数据和字典操作一样data['name'] = 'xxxx' 模型对象转字典 1dict(模型对象) 项目配置 settings.py可以配置项目中常用信息 注意：scrapy.settings.default_settings.py才是所有的配置信息，如果在settings.py中没有配置，就是用默认的配置文件 LOG_LEVEL= WARNING 表示显示的日志级别 LOG_FILE= 自定义的日志PATH 表示日志的存储位置 使用方式在 defalut_settings.py中寻找配置，然后把配置选项在 settings.py中重新配置； scrapy shell介绍 方便开发过程中的调试 使用1scrapy shell 请求地址 常用方法 request response xpath css text fetch请求新的地址 CrawlSpider crawlSpider是一个已经实现了爬取流程的爬虫； 创建1scrapy genspider -t crawl 爬虫名称 爬虫域名 参数 -t表示采用的爬虫模版 爬取流程提取符合条件的链接—&gt;跟进符合条件的链接—&gt;提取符合条件的链接—&gt;循环 使用说明 rules所有规则、元组、存放了提取页面的规则列表 Rule对象，条件规则对象 LinkExtactor链接提取器 callback当提取完页面后的回调处理函数 follow是否跟进提取,默认是 True LinkExtractor链接提取器 allow&gt;&gt;内容符合条件规则被提取 deny&gt;&gt;内容符合条件规则被排除（优先） allow_domains&gt;&gt;符合条件的域名被提取 deny_domians&gt;&gt;符合条件的域名被排除 restrict_xpaths&gt;&gt;根据xpath提取 tags&gt;&gt;默认(‘a’,’area’)符合链接提取的标签名 attrs&gt;&gt;默认(href)，链接提取标签的属性名 restrict_css&gt;&gt;根据css样式提取器提取 strip&gt;&gt;提取后的内容去除空格 链接去重把链接地址放到 seen--&gt;set() 注意：如果url的参数位置发生变化会导致无法去重 crawlspider编写代码流程1.编写提取规则—&gt;提取链接规则 2.编写提取具体页面解析代码 scrapy中间件 1.爬虫中间件 引擎和爬虫组件交互时触发中间件 2.下载中间件 引擎和下载器交互时触发的中间件 中间件的使用流程 在 middlewares.py创建中间件类 实现所需要的拦截的函数 在 settings.py中配置开启中间件 和管道一样，配置的数字(score)越小越优先执行 下载中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_request 123456789引擎 -&gt; 下载中间件 -&gt; 下载器:param request: 请求对象:param spider: 请求来自的爬虫:return: return None 继续处理这个请求return Response 直接把响应提交给引擎 -&gt; 爬虫 return Request 直接返回引擎raise IgnoreRequest 触发 process_exception 回调函数 process_response 123456下载器 -&gt; 下载中间件 -&gt; 引擎 :param request: :param response: :param spider: :return: raise IgnoreRequest 把这请求忽略 process_exception 12345678910当下载中间件异常异常时回调 :param request: :param exception: :param spider: :return: return None 继续处理异常，向下一个中间件传递异常return a Response 停止异常链，把响应返回给引擎return a Request 停止异常链，把请求返回给引擎 爬虫中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_spider_input引擎 -&gt; 爬虫中间件 -&gt; 爬虫 参数 response 响应对象 spider 爬虫对象 process_spider_output 当爬虫提交数据或者请求给引擎时触发 process_spider_exception 当 process_spider_input 异常异常时触发 process_start_requests 当引擎向爬虫所要 start_requests 时触发 scrapyd部署 专门用于部署scrapy项目框架，scrapyd帮助我们运行scrapy代码 服务端 安装 1pip install scrapyd 启动服务 1scrapyd 远程访问 客户端远程访问6800端口 开发端 安装 1pip install scrapyd-client 配置部署 scrapy.cfg文件 123[deploy:部署名称]url = http://xxx.xxx.xxx.xxx:6800/ # 服务器的ipproject = 项目名称 上传项目到服务器 1scrapyd-deploy 部署名称 -p 项目名称 启动爬虫 1curl http://server-iP:6800/schedule.json -d project=项目名称 -d spider=爬虫名称 停止爬虫 1curl http://server-ip:6800/cancel.json -d project=项目名称 -d job=jobid scrapy redis 主要是利用scrapy+redis实现 断点续爬功能 实现分布式爬虫功能 安装1pip install scrapy-redis 断点续爬实现 现实情况中很多时候我们的一旦中断爬虫就需要重新再请求之前的url，scrapy-redis能够实现断点续爬 1、在 settings.py配置 1234567891011121314# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式REDIS_HOST = "127.0.0.1"REDIS_PORT = 6379 分布式爬虫实现流程1、在 settings.py 1234567891011121314151617181920# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器## REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式# REDIS_HOST = "目标redis服务器" # 不能编写 127.0.0.1# REDIS_PORT = 6379# 管道配置ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 400,&#125; 2、修改爬虫代码，修改集成类 CrawlSpider==&gt;RedisCrawlSpider Spider==&gt;RedisSpider 3、添加监控的 redis_key体现首个请求地址]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题之字典扁平化]]></title>
    <url>%2F2018%2F05%2F24%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E5%AD%97%E5%85%B8%E6%89%81%E5%B9%B3%E5%8C%96%2F</url>
    <content type="text"><![CDATA[实际开发中存储的数据结构很多都是树状形状，现在需要写一个功能实现从树形结构到扁平化结构的转换 123456789101112131415161718192021'''有若干种不同的树形结构，需要映射为扁平化的dict字典，需要写一个函数，完成从树形结构到扁平化结构的转换如输入in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;# 输出&#123; 'jack_English_socre': '90', 'jack_English_tearcher': 'Xu', 'jack_Math_socre': '75', 'jack_Math_tearcher': 'zhang', 'jack_height': '172' &#125;''' 题目考查的其实就是递归的应用，字典嵌套字典，这里还有用到Python的 isinstance() 方法 12345678Python 中的isinstance函数，isinstance是Python中的一个内建函数。是用来判断一个对象的变量类型和type类似。语法:isinstance(object, classinfo)&gt;&gt;&gt; isinstance(1, int)True&gt;&gt;&gt; isinstance(&#123;'age': 18&#125;, dict)True&gt;&gt;&gt; isinstance('1', int)False 答案 1234567891011121314151617181920212223in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;target = &#123;&#125; # 定义一个全局变量用来存储返回结果def dict2flat(data, targetKey=''): for k, v in data.items(): if isinstance(v, dict): dict2flat(v, targetKey + k + '_') else: target[targetKey + k] = vif __name__ == '__main__': dict2flat(in_data) from pprint import pprint pprint(target)]]></content>
      <categories>
        <category>面试题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬虫之selenium]]></title>
    <url>%2F2018%2F04%2F27%2F%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%2F</url>
    <content type="text"><![CDATA[Selenium介绍 Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏。程序脚本可以通过 Selenium API 控制浏览器。 运行流程 环境搭建 安装selenium 1pip install selenium 下载对应的驱动 淘宝驱动地址：http://npm.taobao.org/mirrors/chromedriver/ 备注：一定要下载本地浏览器版本对应的驱动 基本操作 12345678910111213141516171819# 导入模块from selenium import webdriverimport time# 加载浏览器驱动，创建浏览器对象browse = webdriver.Chrome('./chromedriver')# 访问百度browse.get('https://www.baidu.com')# 定位百度搜索输入框，输入关键字“电影”browse.find_element_by_name('wd').send_keys('电影')# 点击‘百度一下’browse.find_element_by_id('su').click()time.sleep(2) # 退出窗口browse.quit() selenium驱动寻找方式 1、通过指定浏览器驱动路径 1browser = webdriver.Chrome('浏览器驱动位置') 2、通过 $PATH环境变量指定浏览器驱动 12# 通过 $PATH 寻找驱动，如果寻找不到就报错 browser = webdriver.Chrome() 添加网址：https://www.jianshu.com/p/e50a49f86070 其实就是找到Python解释器的 bin目录，把驱动放进去即可 selenium控制浏览器操作访问URL1browser.get('https://www.baidu.com/') 定位元素 find_element_by_xxx返回第一个符合条件 WebElement find_elements_by_xxx返回符合条件的 WebElement列表 xxx说明 find_elements_by_class_name通过标签的class属性定位元素 find_element_by_id通过标签的ID属性定位元素 find_element_by_name通过标签的name属性定位 find_element_by_css_selectorcss样式选择 find_element_by_link_text通过链接内容查找 find_element_by_partial_link_text 通过链接内容包含的内容查找，模糊查询 find_element_by_xpath 通过xpath查找数据 xpath只能获取webelement对象，不能直接获取属性和文本内容 获取元素属性和文本内容1234# 获取属性WebElement.get_attribute('属性名')# 获取文本内容WebElement.text 输入框Input输入内容1Input_element.send_keys('xxx') 点击1element.click() 使用无界面浏览器 使用无界面的驱动 下载地址：http://phantomjs.org/download.html 使用方式 browser = webdriver.PhantomJS(&#39;驱动路径‘) 设置chrome启动参数 123456789101112131415from selenium import webdriver# 创建浏览器启动参数options = webdriver.ChromeOptions()options.add_argument('--headless') # 无界面options.add_argument('--disable-gpu') # 禁用gpu# 创建浏览器对象browser = webdriver.Chrome(chrome_options=options)# 访问网址browser.get('https://www.baidu.com')# 截图证明browser.save_screenshot('百度截图.png')# 退出browser.quit() 设置User-Agent和Proxy代理123456options = webdriver.ChromeOptions()# 切换User-Agentoptions.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1')# 设置代理options.add_argument('--proxy-server=代理服务器地址') # 设置代理browser = webdriver.Chrome('chrome_options=options) 获取网页源码1browser.page_source 注意：获取的网页源码是经过JS页面执行后的结果源码 cookies 操作（非常重要） 获取所有Cookies 1browser.get_cookies() 通过名字获取Cookie 1browser.get_cookie() 添加Cookie 1browser.add_cookie() 通过名字删除Cookie 1browser.delete_cookie() 删除所有Cookie 1browser.delete_all_cookies() 执行JS代码1browser.execute_script("js-code") 等待加载 方式一：强制等待，浪费时间 1time.sleep(秒数) 方式二：隐性等待 1browser.implicitly_wait(等待时间) 方式三：显性等待，每个元素都可以自己定义检查条件 手动编写方式 123456789101112131415# 显性等待-手动编写t = time.time()# 定义超时时间timeout = 60while True: try: # 超时时间间隔 time.sleep(0.1) url_element = browser.find_element_by_class_name("favurl") break except: # 超时处理 if time.time() - t &gt; timeout: break pass 系统提供显性等待API 1234567891011121314151617# 导入显性等待的API需要的模块# 1&gt; 等待对象模块from selenium.webdriver.support.wait import WebDriverWait# 2&gt; 导入等待条件模块from selenium.webdriver.support import expected_conditions as EC# 3&gt; 导入查询元素模块from selenium.webdriver.common.by import By# 使用selenium api 实现显性等待# 1&gt; 创建等待对象# 参数一 浏览器对象# 参数二 超时时间# 参数三 检查元素时间间隔wait = WebDriverWait(browser,60,0.1)# presence_of_element_located 检查元素是否存在，参数是一个元祖，元祖内部描述等待元素查询方案# visibility_of_element_located 检查元素是否可见url_element= wait.until(EC.presence_of_element_located((By.CLASS_NAME,"favurl"))) 窗口切换 获取所有的窗口（列表） 1browser.window_handles 切换窗口 1browser.switch_to_window(all_windowsp['index']) 备注：browser.title可以通过打印当前窗口title调试 iframe切换 1、获取iframe标签对象 1iframe_element = browser.find_element_by_xxx('') 2、切换到iframe窗口 1browser.switch_to.frame(iframe_element) 3、切换到主窗口 1browser.switch_to.default_content()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv和json之间的转换]]></title>
    <url>%2F2018%2F04%2F26%2Fcsv%E5%92%8Cjson%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[CSV（逗号分隔值文件格式），是一种通用的，相对简单的文件格式，被用户、商业、科学广泛使用，最广泛的应用是在程序之间转移表格数据。 文章目标：了解CSV的基本操作以及json格式数据如何写入CSV文件并转换Excel 使用场景爬虫拿到的大部分都是json格式数据，但是客户或者数据分析员一般更需要Excel这种格式的数据 基本操作 1、导入模块 12# python3.X内置模块不需要下载import csv 2、创建csv写入对象 12345# 创建文件对象f = open('03-test.csv', 'w', encoding='utf-8')# 创建csv写入对象csv_writer = csv.writer(f) 3、写入数据 123csv_writer.writerow(['姓名', '年龄', '性别'])csv_writer.writerow(['强仔', '18', '男'])csv_writer.writerow(['彤彤', '17', '女']) 4、关闭文件 1f.close() json2csv 1、先转换json格式为python数据类型 12345import csvimport json# data.json 为事先准备好的json数据with open('data.json', 'r') as f: data_list = json.load(f) # 转换成pyton数据类型 2、创建csv对象 12f = open('data.csv', 'w', encoding='utf-8')csv_writer = csv.writer(f) 3、写入数据 1234567# 3.1 写入标题也就是字典的关键字keycsv_writer.writerow(data_list[0].keys())# 3.2 遍历写入字典元素valuesfor data in data_list: csv_writer.writerow(data.values())# 3.3 关闭文件句柄f.close() csv转Execl见链接：https://jingyan.baidu.com/article/3c343ff7faa59b0d3779633e.html]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：求列表当中最大的三个元素]]></title>
    <url>%2F2018%2F04%2F15%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98-%E4%BA%8C-%EF%BC%9A%E6%B1%82%E5%88%97%E8%A1%A8%E5%BD%93%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[在牛客网https://www.nowcoder.com/上刷题遇到如何从list中取得最大的三个值：自己写的方法复杂度太高，放上大牛的方法，复杂度很低。看了好几遍才体会到大概的精髓。 12345678910111213141516171819202122232425262728'''从list中取出最大的三个值__author__:无名'''def FindList3MaxNum(foo): max1, max2, max3 = None, None, None for num in foo: if max1 is None or max1 &lt; num: max1, num = num, max1 if num is None: continue if max2 is None or num &gt; max2: max2, num = num, max2 if num is None: continue if max3 is None or num &gt; max3: max3 = num return max1, max2, max3if __name__ == '__main__': foo = [78, 23, 10, 56, 4, 103, 89, 14] max1, max2, max3 = FindList3MaxNum(foo) print(max1, max2, max3)]]></content>
      <categories>
        <category>面试题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之list常见用法]]></title>
    <url>%2F2018%2F04%2F11%2Fpython%E7%BC%96%E7%A8%8B%E4%B9%8Blist%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[列表是Python最常用的数据类型之一，本文整理一下列表最常用的10种操作，如果开发过程当中遇到了，可以尝试从这些找答案 1、迭代列表时如何访问列表下标索引普通版：12345678items = [8, 23, 45]for index in range(len(items)): print(index, "--&gt;", items[index])&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 Level Up版：1234567for index, item in enumerate(items): print(index, "--&gt;", item)&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 enumerate还可以指定元素的第一个元素从第几个索引开始，默认是0，也可以指定从1开始： 1234567for index, item in enumerate(items, start=1): print(index, "--&gt;", item)&gt;&gt;&gt;1 --&gt; 82 --&gt; 233 --&gt; 45 2、append与extend方法有什么区别append表示把某个数据当做新元素追加到列表的最后面，它的参数可以是任意对象 1234567x = [1, 2, 3]y = [4, 5]x.append(y)print(x)&gt;&gt;&gt;[1, 2, 3, [4, 5]] extend 的参数必须是一个可迭代对象，表示把该对象里面的所有元素逐个地追加到列表的后面 1234567891011x = [1, 2, 3]y = [4, 5]x.extend(y)print(x)&gt;&gt;&gt;[1, 2, 3, 4, 5]# 等价于：for i in y: x.append(i) 3、检查列表是否为空普通版：1234567if len(items) == 0: print("空列表")或者if items == []: print("空列表") Level Up版：12if not items: print("空列表") 4、如何理解切片 切片用于获取列表中指定范的子集，语法非常简单 1items[start:end:step] 从 start 到 end-1 位置之间的元素。step 表示步长，默认为1，表示连续获取，如果 step 为 2 就表示每隔一个元素获取。 12345678910111213141516a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[3:8] # 第3到第8位置之间的元素[4, 5, 6, 7, 8]&gt;&gt;&gt; a[3:8:2] # 第3到第8位置之间的元素，每隔一个元素获取[4, 6, 8]&gt;&gt;&gt; a[:5] # 省略start表示从第0个元素开始[1, 2, 3, 4, 5]&gt;&gt;&gt; a[3:] # 省略end表示到最后一个元素[4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[::] # 都省略相当于拷贝一个列表，这种拷贝属于浅拷贝[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 5、如何拷贝一个列表对象第一种方法：1new_list = old_list[:] 第二种方法：1new_list = list(old_list) 第三种方法：12345import copy# 浅拷贝new_list = copy.copy(old_list)# 深拷贝new_list = copy.deepcopy(old_list) 扩展，了解即可 6、如何获取列表中的最后一个元素 索引列表中的元素不仅支持正数还支持负数，正数表示从列表的左边开始索引，负数表示从列表的右边开始索引，获取最后一个元素有两种方法。 12345&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[len(a)-1]10&gt;&gt;&gt; a[-1]10 7、如何对列表进行排序 列表排序有两种方式，一种是列表自带的方式 sort，一种是内建函数 sorted。复杂的数据类型可通过指定 key参数进行排序。由字典构成的列表，根据字典元素中的age字段进行排序： 12345678910items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]items.sort(key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'age': 10, 'name': 'Bart'&#125;, &#123;'age': 20, 'name': 'cater'&#125;, &#123;'age': 39, 'name': 'Homer'&#125;] 列表有 sort方法，用于对原列表进行重新排序，指定 key 参数，key 是匿名函数，item 是列表中的字典元素，我们根据字典中的age进行排序，默认是按升序排列，指定 reverse=True 按降序排列 1234items.sort(key=lambda item: item.get("age"), reverse=True)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Bart', 'age': 10&#125;] 如果不希望改变原列表，而是生成一个新的有序列表对象，那么可以内置函数 sorted ，该函数返回新列表 12345678910111213items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]new_items = sorted(items, key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;]print(new_items)&gt;&gt;&gt;[&#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Homer', 'age': 39&#125;] 8、如何移除列表中的元素删除列表中的元素有三种方式 remove 移除某个元素，而且只能移除第一次出现的元素 12345678910&gt;&gt;&gt; a = [0, 2, 2, 3]&gt;&gt;&gt; a.remove(2)&gt;&gt;&gt; a[0, 2, 3]# 如果要移除的元素不在列表中，则抛出 ValueError 异常&gt;&gt;&gt; a.remove(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: list.remove(x): x not in list· del根据指定的索引移除元素 12345678910&gt;&gt;&gt; a = [3, 2, 2, 1]# 移除第一个元素&gt;&gt;&gt; del a[1][3, 2, 1]# 当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; del a[7]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list assignment index out of range pop与del类似，但是pop方法可以返回移除的元素 1234567891011&gt;&gt;&gt; a = [4, 3, 5]&gt;&gt;&gt; a.pop(1)3&gt;&gt;&gt; a[4, 5]# 同样，当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; a.pop(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: pop index out of range 9、如何连接两个列表12345678listone = [1, 2, 3]listtwo = [4, 5, 6]mergedlist = listone + listtwoprint(mergelist)&gt;&gt;&gt;[1, 2, 3, 4, 5, 6] 列表实现了 + 的运算符重载，使得 + 不仅支持数值相加，还支持两个列表相加，只要你实现了 对象的 __add__操作，任何对象都可以实现 + 操作，例如： 123456789101112131415161718192021class User(object): def __init__(self, age): self.age = age def __repr__(self): return 'User(%d)' % self.age def __add__(self, other): age = self.age + other.age return User(age)user_a = User(10)user_b = User(20)c = user_a + user_bprint(c)&gt;&gt;&gt;User(30) 10、如何随机获取列表中的某个元素123456789import randomitems = [8, 23, 45, 12, 78]&gt;&gt;&gt; random.choice(items)78&gt;&gt;&gt; random.choice(items)45&gt;&gt;&gt; random.choice(items)12]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具之Charles使用问题记录]]></title>
    <url>%2F2018%2F04%2F07%2F%E5%B7%A5%E5%85%B7%E4%B9%8BCharles%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近研究爬虫需要进行HTTP抓包，由于 Fiddler没有Mac版本，选择使用Charles（青花瓷）来抓包，文章 记录一下 Charles的使用和使用过程中遇到的问题 目标：Charles抓包&amp;问题记录 安装 安装包百度云链接：链接:https://pan.baidu.com/s/1QYaEVwgKSH8znwORotBOvg 密码:s7jz 安装步骤1、点击dmg安装—&gt;应用程序 2、打开charles应用程序—&gt;显示包信息—&gt;替换其中的 charles.jar包 基本使用 这个我就不过多介绍网上教程一大堆，需要注意的是如果要抓 https需要安装证书，并使证书可信任 问题记录 安装证书以后还是无法抓HTTPS的包，显示unknown?解决步骤 1、点击Proxy—&gt;SSL Proxying settings—&gt;SSL Proxying—&gt;Add—&gt; HOST和PORT都填写 *即可 2、重启 charles 安装Charles 抓包信息当中以后不显示 request和response解决步骤 1、点击Preferences—&gt;Viewers—&gt;把 Combine request and response 取消勾选—&gt;点击OK即可 2、重启charles 抓包手机配置链接：https://blog.csdn.net/pansanday/article/details/80347632]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Charles</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之数据提取]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[今天主要记录一下数据提取的几种方法，在爬虫获取到响应数据以后我们改怎么处理响应的数据呢？从网上拿到的响应数据各种类型都有：JSON类型的，XML格式的,html格式的等等。 常用的数据提取方式 jsonpath re xpath beautifulsoup 一、jsonpath基本使用 json是什么JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。 json模块常用操作 json.loads(json_str) JSON字符串—&gt;转换成Python数据类型 json.dumps(dict_data) Python数据类型—&gt;JSON字符串 json.load(f) json文件—&gt;获取Python数据类型 json.dump(dict_data, f) Python数据类型—&gt;写入json文件 ensure_ascii=False 显示中文 indent=数字表示空格 Jsonpath是什么？ jsonpath是一种语法规则快速从JSON数据当中提取数据 在线调试网址：https://jsonpath.com/ 在线解析工具：https://www.json.cn/# 语法规则 语法 描述 案例 $ 根节点 @ 现行节点 . 取子节点 $.store.book .. 取子孙节点 $..book [] 设置筛选条件 $..book[0] [,] 支持多选选择内容 $..book[1,3] () 支持表达式计算 $..book[(@.length - 1)] ?() 支持过滤操作 $..book[?(@.price&lt;10)] Python中怎么使用123import jsonpathdata = jsonpath('字典类型数据', 'jsonpath提取语法')# data是list类型 如果提取不出来返回False 二、re基本使用 用事先定义好的一些特定字符，及这些特定字符的组合，组成一个规则字符串，这个规则字符串用来表达对字符串的一种过滤逻辑 基本语法 预定义字符集 \d表示数字：[0-9] \D表示非数字：[^\d] \s表示空白字符 \S非空白字符 \w单词字符：[A-Z,a-z,0-9] \W非单词字符：[^\w] .匹配任意除换行符\n外的字符，备注：在DoTALL模式中也能匹配换行符 \转义字符，使后一个字符表示字符本身 []表示字符的选取范围 数量词 *表示任意个 +表示至少一个 ?表示至多一个（1个或者没有） {}表示范围区间 {1} 表示匹配一个字符 {1,6}表示匹配1到6个字符 {,6}表示最多匹配6个字符 贪婪模式和非贪婪模式 贪婪模式(.*)尽可能多的匹配 非贪婪模式(.*?)一旦匹配到就结束 DOTALL模式 让.匹配\n 1re.RegexFlag.DOTALL、re.RegexFlag.S、re.S、re.DOTALL 忽略大小写 1re.RegexFlag.I re.RegexFlag.IGNORECASE re.I re.IGNORECASE 支持DOTALL 和忽略大小写 用| 原始字符串使用r不用\ 正则前面使用r正则语法 四种匹配检索方法 match match 从头开始匹配，仅匹配一次 search search 全局匹配，仅匹配一次 findall findall 获取符合条件的所有数据，返回列表 finditer finditer 获取符合条件的所有数据，返回迭代器对象 数据量大的时候推荐使用，提高内存使用效率 分组&amp;替换 sub split 通用分组格式：[分隔符]+ 1234567import redata = 's,fqw;fwe, fds;fsda,afds;'pattern = re.compile(r'[, ;]+')ret = pattern.split(data)print(ret) 输出 1['s', 'fqw', 'fwe', 'fds', 'fsda', 'afds', ''] sub 12345678import redata = 's,fqw;fwe,fds;fsda,afds;'# 把, ; 都替换成 #pattern = re.compile(r'[,;]+')ret = pattern.sub('#', data)print(ret) 输出 1s#fqw#fwe#fds#fsda#afds# 三、Xpath基本使用 xpath是一门在HTML/XML文本中查找信息的语言，可以用在HTML/XML文档中对元素和属性进行遍历，简单来说就类似于正则通过一定的语法规则从文本中提取数据 基本语法 节点选择 /表示从根节点选取 //从匹配选择的当前节点选择文档中的节点 .表示选取当前节点 ..选取当前节点的父节点 选取属性 @ //节点/@属性名称 文本选择 text()选取文本 /text() 高级语法-筛选条件 //node[筛选条件] 通过下标（数字）筛选（下标从1开始） /node[1] 通过属性筛选 /node[@属性] /node[@属性=值] 通过文本内容筛选 //node[text()=&#39;筛选的值&#39;] 通过内置函数 last() position() containes包含内容 contains(text(), &#39;文本&#39;) contains(@属性， ‘属性值’) starts-with 从头匹配内容 语法和containes一样 通过子节点来筛选 /node[node_子节点&gt;子节点的值] 通配符 *匹配任何元素节点 //*[text()=&#39;某值&#39;] //*[@category=’某value’] @*匹配node任意属性的value //node[@*=&#39;某value&#39;] 多个条件同时满足 使用 | lxml模块 Python lxml模块去使用xpath语法 安装1pip install lxml lxml基本使用1、导入模块 1from lxml import etree 2、构建根元素对象 1eroot = etree.HTML(html_data) # eroot 是一个element对象 3、操作根元素对象提取数据 1eroot.xpath('xpath语法规则') 4、打印调试查看element对象内容 1etree.tostring(eroot).decode('utf-8') 四、beautifulsoup基本使用 介绍 Beautifulsoup也是一个HTML/XMl的解析器，跟上面的 lxml一样 优点 用来解析HTML比较简单，API非常人性化，支持CSS选择器 缺点 lxml只会遍历局部，而Beautifusoup是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml 安装1pip install beautifulsoup4 官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0 基本使用1、导入模块 1from bs4 import BeautifulSoup 2、创建beautifulsoup对象 12soup = BeautifulSoup(html,'lxml')soup = BeautifulSoup(open('文件')) 3、操作node 获取元素标签内容 1soup.标签名称 获取子元素 1234# 返回列表soup.node_name.contents# 返回迭代器soup.node_name.children 获取元素内容 1soup.node_name.get_text() 获取元素属性值 1soup.a.get('href') 注意如果获取元素的属性是 class返回的是列表 find 和find_all介绍 find返回符合条件的第一个元素 find_all返回符合条件的所有元素列表 备注：find和find_all提供参数都一样，下面以find_all()来展示 1、通过标签和标签列表查找元素 12soup.find_all('a') # 查询所有的a标签soup.find_all(['a', 'b']) # 查询所有的a标签和b标签 2、通过正则表达式查找元素 12# 以b开头的标签查找soup.find_all(re.compile('^b')) 3、通过属性来查找 12345soup.find_all(&#123; attrs=&#123; '属性名'：'值' &#125;&#125;) 4、通过文本内容查找元素 1soup.find_all(text='查询的文本内容') 6、混合使用 1234567soup.find_all( '标签名', attrs=&#123; '属性名'：'值' &#125;， text='内容') CSS样式选择器 返回值是一个list 类选择器 使用 .开头 1soup.select('.class_name') ID选择器，使用#开头 1soup.select('#id_name') 标签选择器，使用标签 名称 1soup.select('p_name') 属性选择器 1soup.select('p[属性=值]') 层级选择器 div p 选择 &lt;div&gt; 元素内部的所有 &lt;p&gt; 元素。 div , p 选择所有 &lt;div&gt; 元素和所有 &lt;p&gt; 元素。 div &gt; p 选择父元素为 &lt;div&gt; 元素的所有 &lt;p&gt; 元素。 .cls1.cls2 选择类名是cls1 并且 类名是cls2]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>jsonpath</tag>
        <tag>bs4</tag>
        <tag>xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之js逆向百度翻译]]></title>
    <url>%2F2018%2F03%2F22%2F%E7%88%AC%E8%99%AB%E4%B9%8Bjs%E9%80%86%E8%A1%8C%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[爬虫小试身手之js逆向百度翻译，目前百度翻译在PC和移动端都做了反扒措施，今天我们一看究竟～ 什么是js逆向？所谓的js逆向其实就是破解对方反扒的一种方式，目前很多网站反扒都使用js来做，百度翻译的反扒主要是利用了请求参数中的 sign这个签名来做的，下面就来详细的讲解 sign怎么变动了？当你更换输入框输入的翻译目标时，对比请求携带的参数发现sign这个参数随着输入的单词所变动，判断应该是js代码在请求前生成了这个sign，需要去定位具体哪个JS文件的生成了这个sign 根据上图我们可以看出sign参数是由js代码 m(a)来生成的，a参数是as也就是我们要翻译的目标，这个是我们可以拿到的，现在要做的就是如何执行m()这段js代码，Python被称为胶水语言，在Python当中调用JS代码自然也是可以实现的，Python调用js代码目前主流的两个模块 pyexecjs 、js2py，今天我们讲的主要是通过 js2py来实现。 下载模块1pip install js2py 基本使用 在python当中执行js 12345678# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在Python当中执行js代码content.execute("console.log('abc')") 输出1'abc' 在JS代码中调用Python程序 123456789101112131415161718# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在js当中调用Python程序content.a = 1content.b = 'abc'content.c = [1, 3, 4]content.d = &#123; "name": 'qwe'&#125;content.execute('console.log(a)')content.execute('console.log(b)')content.execute('console.log(c)')content.execute('console.log(d)') 输出1234'1''abc'[1, 3, 4]&#123;'name': 'qwe'&#125; 实战既然Pythoncode中可以调用js我们只要找到这段js在代码中执行就解决了这个问题，上代码 1234567891011121314151617181920212223242526272829303132333435363738394041# Python代码# 1.导入模块import requestsimport js2py# 2.发送请求，获取响应内容# 请求地址 请求头 请求方式 请求参数url = 'https://fanyi.baidu.com/v2transapi'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36', 'Referer': 'https://fanyi.baidu.com/?aldtype=16047', 'Cookie': 'BAIDUID=DAD75821AD7D4663D00D9C0E7F9C8011:FG=1; BIDUPSID=DAD75821AD7D4663D00D9C0E7F9C8011; PSTM=1553134744; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1439_21121_18559_28723_28557_28697_28585_28518_28627_22157; PSINO=2; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553218276,1553218292; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553218292'&#125;# 需要翻译的内容kw = 'thank'# 构建上下文对象content = js2py.EvalJs()with open('test.js', 'r') as f: content.execute(f.read()) sign = content.e(kw)data = &#123; "from": "en", "to": "zh", "query": kw, "transtype": "translang", "simple_means_flag": "3", "sign": sign, "token": "88d322490309c4abee0496f0dbab1a4b",&#125;response = requests.post( url=url, headers=headers, data=data)print(response.json()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445// test.js 文件var i = '320305.131321201';function n(r, o) &#123; for (var t = 0; t &lt; o.length - 2; t += 3) &#123; var a = o.charAt(t + 2); a = a &gt;= "a" ? a.charCodeAt(0) - 87 : Number(a), a = "+" === o.charAt(t + 1) ? r &gt;&gt;&gt; a : r &lt;&lt; a, r = "+" === o.charAt(t) ? r + a &amp; 4294967295 : r ^ a &#125; return r &#125;function e(r) &#123; var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g); if (null === o) &#123; var t = r.length; t &gt; 30 &amp;&amp; (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10)) &#125; else &#123; for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h &gt; C; C++) "" !== e[C] &amp;&amp; f.push.apply(f, a(e[C].split(""))), C !== h - 1 &amp;&amp; f.push(o[C]); var g = f.length; g &gt; 30 &amp;&amp; (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join("")) &#125; var u = void 0 , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107); u = null !== i ? i : (i = window[l] || "") || ""; for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v &lt; r.length; v++) &#123; var A = r.charCodeAt(v); 128 &gt; A ? S[c++] = A : (2048 &gt; A ? S[c++] = A &gt;&gt; 6 | 192 : (55296 === (64512 &amp; A) &amp;&amp; v + 1 &lt; r.length &amp;&amp; 56320 === (64512 &amp; r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 &amp; A) &lt;&lt; 10) + (1023 &amp; r.charCodeAt(++v)), S[c++] = A &gt;&gt; 18 | 240, S[c++] = A &gt;&gt; 12 &amp; 63 | 128) : S[c++] = A &gt;&gt; 12 | 224, S[c++] = A &gt;&gt; 6 &amp; 63 | 128), S[c++] = 63 &amp; A | 128) &#125; for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b &lt; S.length; b++) p += S[b], p = n(p, F); return p = n(p, D), p ^= s, 0 &gt; p &amp;&amp; (p = (2147483647 &amp; p) + 2147483648), p %= 1e6, p.toString() + "." + (p ^ m) &#125;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>js2py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Http协议]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%88%AC%E8%99%AB%E4%B9%8BHttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[什么是HTTP协议？Http协议专业称之为：超文本传输协议，属于网络协议中应用层的协议，默认端口是80 HTTP请求格式 案例 HTTP请求方式HTTP请求可以使用多种请求方法 HTTP1.0定义了三种请求方法：GET, POST, HEAD HTTP1.1（主流）新增了五种请求方法：OPTIONS，PUT，DELETE，TRACE，CONNECT 请求方式 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 常见请求头 Cookie User-Agent 浏览器代理 Referer 防盗链，请求来自哪里 Host 请求主机 Connection Accept HTTP响应Http响应由四部分组成 状态行： HTTP/1.1 200 OK 消息报文： Content-Type: text/html….. 空行就是： 消息报文和正文中间的空行 响应正文：大家在浏览器看到的渲染后的内容 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开篇]]></title>
    <url>%2F2018%2F03%2F20%2F%E7%88%AC%E8%99%AB%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[当今是一个大数据的时代，数据也变得越来越值钱，有市场就有需要，“爬虫工程师”就此诞生～从此开始学习 Spider，开贴记录一下自己的学习经历, Fighting 什么是爬虫？所谓的爬虫其实就是模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动的去获取网络上的信息 爬虫分为哪些从网上简单的了解，爬虫主要分为通用爬虫（搜索引擎爬虫，典型的就是百度&amp;Google）和聚焦爬虫（爬一些指定的网站） 简单的爬虫流程 1、抓到起始的url，获取响应 2、对响应再次发送请求 3、如果能从响应中提取URL，则继续发送请求获取响应 4、如果提取数据，则将数据进行保存]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：Python操作Excel]]></title>
    <url>%2F2018%2F03%2F19%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9APython%E6%93%8D%E4%BD%9CExcel%2F</url>
    <content type="text"><![CDATA[前几天帮朋友做了一道面试题，感觉用Python做起来会方便很多，话不多说上图 一、模块介绍Python社区的强大和活跃，也为Python成为主流语言奠定了基础，丰富的第三方库使一切变成了可能，主要使用Python的两个模块 xlrd和xlwt 模块，顾名思义xlrd是读excel，xlwt是用来写excel的库，另外这两个库不是Python内置的模块需要自行安装。建议使用 pip 来进行安装 12# -i 可以指定下载的源地址，默认是使用的国外的源比较慢pip install xlrd -i https://pypi.douban.com/simple/ 二、解决题目的思路 1、要获取excel表格里面的数据 2、获取到每一行的数据，然后拼接成字典 3、把每一行拼接的字典放到列表当中 看着是不是很容易，但是如果不熟悉xlrd的语法操作起来也是很困难的，下面就罗列一下基本使用 获取Excel文件对象1data = xlrd.open_workbook(filename)#文件名以及路径，如果路径或者文件名有中文给前面加一个r原生字符。 常用的操作 1、获取data中某一个工作表（sheet） 12345table = data.sheets()[0] # 通过索引顺序获取table = data.sheet_by_index(sheet_indx)) #通过索引顺序获取table = data.sheet_by_name(sheet_name)#通过名称获取 2、行的操作 1234567891011nrows = table.nrows # 获取该sheet中的有效行数table.row(rowx) #返回由该行中所有的单元格对象组成的列表table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表table.row_len(rowx) #返回该列的有效单元格长度 3、列的操作 123456789ncols = table.ncols #获取列表的有效列数table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 4、单元格的操作 1234567table.cell(rowx,colx) #返回单元格对象table.cell_type(rowx,colx) #返回单元格中的数据类型table.cell_value(rowx,colx) #返回单元格中的数据table.cell_xf_index(rowx, colx) # 暂时还没有搞懂 三、代码奉上 ——&gt;So simple1234567891011121314151617181920212223242526272829# 导入模块import xlrd def Execl2List(): # 1.获取表格 excel = xlrd.open_workbook(r"./lianxi.xlsx") # 2.获取Sheet1表格页 sheet = excel.sheet_by_name("Sheet1") # 3.获取当前页的行数 row = sheet.nrows lists = [] for i in range(1, row): dic = &#123;&#125; list = sheet.row_values(i) # 获取表格对应行的数据 dic["姓名"] = list[0] dic["年龄"] = list[1] dic["性别"] = list[2] lists.append(dic) return listsif __name__ == '__main__': data = Execl2List() print(data)]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程池]]></title>
    <url>%2F2018%2F01%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[作用 计算机每次创建线程和销毁线程需要额外占用资源，频繁创建和销毁线程会导致计算机性能下降 处理过程 优点 重复利用线程，减少因为创建线程和销毁线程带来不必要的性能销毁 让程序更加的稳定，不会因为同一时间创建线程过多而导致内存不够使程序引发系统一系列的问题 代码实现流程 1、导入线程池模块 1from multiprocessing.dummy import Pool 2、创建线程池 1pools = Pool(5) 3、定义线程池要执行的任务 12def exec_task(): print('---要执行的任务---') 4、任务完成的回调函数 12def exec_task_finish(self, result): print('执行任务完成回调函数') 注意回调函数必须要有result参数，result参数表示执行任务代码的返回值 5、线程池执行任务 1pools.apply_async(self.exec_task,callback=self.exec_task_finish) 扩展进程池 12# 导入进程池库from multiprocessing import Pool 协程池 12345# 打补丁import gevent.monkeygevent.monkey.patch_all()# 导入协程池库from gevent.pool import Pool 备注，默认线程池里面的线程是守护线程，进程池&amp;协程池同理]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之队列Queue]]></title>
    <url>%2F2017%2F11%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E9%98%9F%E5%88%97Queue%2F</url>
    <content type="text"><![CDATA[Queue Queue是Python标准库中的线程安全的队列(FIFO)实现，提供了一个适用于多线程编程的先进先出的数据结构，即队列，用来在生产者和消费者之间的信息传递。 二种队列形式 FIFO队列FIFO即First in First Out，先进先出。Queue提供了一个基本的FIFO容器，使用方法很简单。 参数：maxsize是一个整数，指明了队列中能存放的数据个数的上限。一旦达到上限，插入会导致堵塞，直到队列中的数据被消费掉。 案例 创建队列 12from queue import Queueq = Queue(maxsize=3) 存取数据 1234# 存放数据q.put('aaa')# 获取数据q.get() LIFO队列LIFO即Last in First Out,后进先出。与栈的类似，使用也很简单,maxsize用法同上 创建队列 12from queue import LifoQueueq = LifoQueue() 存放数据API语法同上 常用方法 task_done() 告诉队列该任务已经处理完毕，队列的unfinished_tasks的属性-1 join() 堵塞调用线程，直到队列中的所有任务被处理完毕，一旦有数据被加入队列，未完成的任务数就会增加。当消费者线程调用task_done()（意味着有消费者取得任务并完成任务），未完成的任务数就会减少。当未完成的任务数降到0（内部就是当队列的unfinished_tasks属性为0时），join()解除阻塞。 put(item[, block[, timeout]]) 将item放入队列中。 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常 get() 从队列中移除并返回一个数据。block跟timeout参数同put方法 其非阻塞方法为｀get_nowait()｀相当与get(False) empty() 如果队列为空，返回True，反之返回False]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程守护和线程同步问题]]></title>
    <url>%2F2017%2F11%2F26%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E5%AE%88%E6%8A%A4%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是守护线程？什么是非守护线程？什么是线程同步？ 目标：搞懂Python中的上面三个问题 非守护线程 线程概念当一个进程启动以后，默认会产生一个主线程，因为线程是程序执行的最小单位，在Python当中线程默认情况下就是setDaemon(False)（非守护线程）,也就是主线程执行完自己的任务退出以后，子线程会继续执行自己的任务，不会随主线程退出受影响 案例1234567891011121314151617import threadingimport timedef run(): time.sleep(2) # 延时等待2s print('---子线程结束执行---')def main(): t1 = threading.Thread(target=run) t1.start() print('---主线程结束执行---')if __name__ == '__main__': main() 输出 12---主线程结束执行------子线程结束执行--- 总结:非守护线程，子线程不随主线程结束而立马结束，而是继续执行 守护线程 当守护线程时，子线程会守护主线程，主线程一旦退出，全部子线程都会被强制终止 案例123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 设置子线程守护主线程 t1.start() print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 线程同步 线程同步就是让线程处于堵塞状态，等待子线程执行以后主线程再执行，也可以设置堵塞时间 案例（主线程堵塞，等待子线程结束以后再执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join() # 主线程堵塞 print('---主线程结束---')if __name__ == '__main__': main() 输出 12---子线程结束------主线程结束--- 案例（主线程堵塞1s，主线程继续执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 12---主线程结束------子线程结束--- 案例（主线程堵塞1s，然后守护主线程）12345678910111213141516171819import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 守护主线程 t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 主线程等待1s后主线程结束，子线程随着主线程结束而结束]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用操作]]></title>
    <url>%2F2017%2F10%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之assert&raise]]></title>
    <url>%2F2017%2F04%2F03%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8Bassert-raise%2F</url>
    <content type="text"><![CDATA[看Python某些库的源码经常会看到 assertor raise这两个内置函数，但是自己平常写代码好像没怎么用过，使用 try except会多一点，可能是自己太low 。。。。 语法介绍 assert assert主要的作用就是给程序断言，声明条件必须为真的判断，如果条件为假则抛出异常，并可以编写异常描述。 语法格式12assert 表达式 '错误描述# 如果表达式为真则继续往下执行，如果为假则抛出断言异常&gt;&gt;AssertionError 案例123foo = [1, 2, 3, 4]# 断言foo列表的长度大于5assert len(foo) &gt; 5, '列表长度不大于5' 输出 1234Traceback (most recent call last): File "/Users/qiangzai/Desktop/practiceCode/ElementaryClassCode/day01/01-list-questions-answers.py", line 3, in &lt;module&gt; assert len(foo) &gt; 5, '列表长度不大于5'AssertionError: 列表长度不大于5 raise raise也是用来抛出异常的，一般用在自己觉得会报错的代码当中使用 语法格式123raise TypeError('错误描述')# 注意这里的TypeError不是固定的，可以是任意自定义的内置错误类型 ValueError,NameError,PermissionError等一堆 案例123456num = 1try: num += '1'except Exception as e: raise TypeError('字符串类型不能和整型相加') 输出 1TypeError: 字符串类型不能和整型相加]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>assert</tag>
        <tag>raise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机之UEFI和Legacy]]></title>
    <url>%2F2017%2F03%2F23%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8BUEFI%E5%92%8CLegacy%2F</url>
    <content type="text"><![CDATA[今天主要讲一下UEFI 和Legacy这两种引导启动方式的区别 由于经常帮同学装电脑系统，看到boot模式装UEFI BIOS感觉得心应手，但是有的电脑（尤其是w7系统）老是出现：无法将Windows安装到磁盘0的分区这种问题，后来才明白是 UEFI+GPT 和Legacy+MBR这两种模式区别造成的 解决办法：https://wenku.baidu.com/view/6b6dfb5ef7ec4afe04a1dfb9.html 两种模式运行方式 对比采用传统的BIOS引导启动方式，UEFI BIOS减少了BIOS自检的步骤，节省了大量的时间。UEFI BIOS比传统的BIOS先进得很多，它的标准已经制定了很多年，目前新出厂的电脑基本清一色 UEFI BIOS，但是电脑磁盘的格式必须是GPT格式，这完全不同于传统的MBR格式，所以在使用UEFI BIOS格式给电脑磁盘格式为MBR的装系统时候就会出现上面的问题，需要把电脑磁盘格式从MBR—&gt;GPT]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>UEFI</tag>
        <tag>Legacy</tag>
      </tags>
  </entry>
</search>
