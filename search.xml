<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[纪念金庸]]></title>
    <url>%2F2018%2F10%2F30%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[飞雪连天射白鹿，笑书神侠倚碧鸳。 侠之大者，为国为民。 红颜弹指老，刹那芳华，与其天涯思君，恋恋不舍，莫若相忘于江湖。——金庸《天龙八部》 四张机，鸳鸯织就欲双飞，可怜未老头先白，春波碧草，晓寒深处，相对浴红衣。——金庸《射雕英雄传》 一座山，隔不了两两相思，一天涯，断不了两两无言，且听风吟，吟不完我一生思念。——金庸《神雕侠侣》 他强由他强，清风拂山岗；他横任他横，明月照大江。——金庸《倚天屠龙记》 情不知所起，一往情深；恨不知所终，一笑而泯。——金庸《笑傲江湖》]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于AWS服务器利用sandowsocks科学上网]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%9F%BA%E4%BA%8EAWS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%A9%E7%94%A8sandowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一直都想找Google和AWS这种大BOSS撸羊毛，但是苦于没有信用卡，也不想走淘宝买卡害怕被套路，偶然机会需要出国旅游，所以匆匆去办了一张中信的 Visa信用卡（欧洲用现金、美国用信用卡、中国在用支付宝，国外很流行信用卡），回国以后就赶紧草草注册了AWS，开启科学上网～ 一、注册AWS并创建Ec2实例1、注册地址：https://www.amazonaws.cn/sign-up/&gt; 2、注册需要条件 信用卡（必备，我用的visa双币卡，注册后扣除1$） 手机号 邮箱 备注：注册成功以后需要等待一天验证身份什么的，反正我当初等了一天才能去创建Ec2 3、申请成功，创建Ec2实例 创建实例我选择的是 Ubuntu16.40因为对ubuntu系统比较熟悉 参考链接：https://blog.csdn.net/jewely/article/details/78030057 保存好私钥文件，文件结尾是xxx.pem 二、连接服务器1、修改一下本地私钥文件权限 12chmod 400 xxx.pem# 备注 权限太高，后面ssh的时候连接不了会提示不安全 2、ssh连接 1ssh -i "xxx.pem" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com xxx.pem 就是你下载下来的私钥文件 xxx这个名字是当时下载时写的，大家按自己的来 @前面的那个ubuntu应该是固定的，我选的就是ubuntu系统 @后面的就是AWS上你创建的实例的公有DNS（IPv4）名称 或者写 IPv4的公有IP也可以 我两个都尝试了 It`s OK 3、修改一下root用户密码 1sudo passwd root 三、安装shadowsocks并配置默认ubuntu16.04已经预装了Python3 1、更新apt-get 1apt-get update 2、安装pip3（我用惯了Python3） 1apt-get install python-pip3 3、安装shadowsocks 1pip3 install shadowsocks 4、创建配置文件 1sudo touch /etc/shadowsocks.json 5、添加配置内容 1234567891011&#123; "server": "0.0.0.0", // 服务端IP 0.0.0.0即可方便连接 "server_port": 50003, // 服务器端口方便后期客户端连接 "local_address": "127.0.0.1", "local_port": 1080, "password": "******", //连接服务器的密码，自己设置，客户端连接时候要填写 "timeout": 300, "method": "aes-256-cfb", // 加密方式 "fast_open": false, "workers": 1&#125; 配置以后启动shadowsocks 1sudo ssserver -c /etc/shadowsocks.json -d start 如果遇到permission denied错误解决方法如下 1234# 第一步查看sserver的位置 which ssserver # 第二步sudo 填写完整的ssserver路径 -c /etc/shadowsocks.json -d start 6、修改AWS上EC2实例的入站端口 配置好 shaodowsocks 后，还需要将配置中的端口打开, 这样客户端的服务才能链接得上 EC2 中的 shadowsocks 服务，首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则 四、客户端连接&amp;科学上网1、打开本地shadowsocks客户端 2、填写信息 3、上网 遇到问题的可以留言区留言，一起看看 配置多用户上网]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>sandowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之数据提取]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[今天主要记录一下数据提取的几种方法，在爬虫获取到响应数据以后我们改怎么处理响应的数据呢？从网上拿到的响应数据各种类型都有：JSON类型的，XML格式的,html格式的等等。 常用的数据提取方式 jsonpath re xpath beautifulsoup jsonpath基本使用1、json是什么JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。 2、jsonpath是什么jsonpath就是方便我们去对 json格式数据进行处理，帮助我们快速对json格式数据定位、获取 re基本使用xpath基本使用beautifulsoup基本使用###]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>jsonpath</tag>
        <tag>bs4</tag>
        <tag>xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之js逆向百度翻译]]></title>
    <url>%2F2018%2F03%2F22%2F%E7%88%AC%E8%99%AB%E4%B9%8Bjs%E9%80%86%E8%A1%8C%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[爬虫小试身手之js逆向百度翻译，目前百度翻译在PC和移动端都做了反扒措施，今天我们一看究竟～ 什么是js逆向？所谓的js逆向其实就是破解对方反扒的一种方式，目前很多网站反扒都使用js来做，百度翻译的反扒主要是利用了请求参数中的 sign这个签名来做的，下面就来详细的讲解 sign怎么变动了？当你更换输入框输入的翻译目标时，对比请求携带的参数发现sign这个参数随着输入的单词所变动，判断应该是js代码在请求前生成了这个sign，需要去定位具体哪个JS文件的生成了这个sign 根据上图我们可以看出sign参数是由js代码 m(a)来生成的，a参数是as也就是我们要翻译的目标，这个是我们可以拿到的，现在要做的就是如何执行m()这段js代码，Python被称为胶水语言，在Python当中调用JS代码自然也是可以实现的，Python调用js代码目前主流的两个模块 pyexecjs 、js2py，今天我们讲的主要是通过 js2py来实现。 下载模块1pip install js2py 基本使用 在python当中执行js 12345678# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在Python当中执行js代码content.execute("console.log('abc')") 输出1'abc' 在JS代码中调用Python程序 123456789101112131415161718# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在js当中调用Python程序content.a = 1content.b = 'abc'content.c = [1, 3, 4]content.d = &#123; "name": 'qwe'&#125;content.execute('console.log(a)')content.execute('console.log(b)')content.execute('console.log(c)')content.execute('console.log(d)') 输出1234'1''abc'[1, 3, 4]&#123;'name': 'qwe'&#125; 实战既然Pythoncode中可以调用js我们只要找到这段js在代码中执行就解决了这个问题，上代码 1234567891011121314151617181920212223242526272829303132333435363738394041# Python代码# 1.导入模块import requestsimport js2py# 2.发送请求，获取响应内容# 请求地址 请求头 请求方式 请求参数url = 'https://fanyi.baidu.com/v2transapi'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36', 'Referer': 'https://fanyi.baidu.com/?aldtype=16047', 'Cookie': 'BAIDUID=DAD75821AD7D4663D00D9C0E7F9C8011:FG=1; BIDUPSID=DAD75821AD7D4663D00D9C0E7F9C8011; PSTM=1553134744; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1439_21121_18559_28723_28557_28697_28585_28518_28627_22157; PSINO=2; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553218276,1553218292; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553218292'&#125;# 需要翻译的内容kw = 'thank'# 构建上下文对象content = js2py.EvalJs()with open('test.js', 'r') as f: content.execute(f.read()) sign = content.e(kw)data = &#123; "from": "en", "to": "zh", "query": kw, "transtype": "translang", "simple_means_flag": "3", "sign": sign, "token": "88d322490309c4abee0496f0dbab1a4b",&#125;response = requests.post( url=url, headers=headers, data=data)print(response.json()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445// test.js 文件var i = '320305.131321201';function n(r, o) &#123; for (var t = 0; t &lt; o.length - 2; t += 3) &#123; var a = o.charAt(t + 2); a = a &gt;= "a" ? a.charCodeAt(0) - 87 : Number(a), a = "+" === o.charAt(t + 1) ? r &gt;&gt;&gt; a : r &lt;&lt; a, r = "+" === o.charAt(t) ? r + a &amp; 4294967295 : r ^ a &#125; return r &#125;function e(r) &#123; var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g); if (null === o) &#123; var t = r.length; t &gt; 30 &amp;&amp; (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10)) &#125; else &#123; for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h &gt; C; C++) "" !== e[C] &amp;&amp; f.push.apply(f, a(e[C].split(""))), C !== h - 1 &amp;&amp; f.push(o[C]); var g = f.length; g &gt; 30 &amp;&amp; (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join("")) &#125; var u = void 0 , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107); u = null !== i ? i : (i = window[l] || "") || ""; for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v &lt; r.length; v++) &#123; var A = r.charCodeAt(v); 128 &gt; A ? S[c++] = A : (2048 &gt; A ? S[c++] = A &gt;&gt; 6 | 192 : (55296 === (64512 &amp; A) &amp;&amp; v + 1 &lt; r.length &amp;&amp; 56320 === (64512 &amp; r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 &amp; A) &lt;&lt; 10) + (1023 &amp; r.charCodeAt(++v)), S[c++] = A &gt;&gt; 18 | 240, S[c++] = A &gt;&gt; 12 &amp; 63 | 128) : S[c++] = A &gt;&gt; 12 | 224, S[c++] = A &gt;&gt; 6 &amp; 63 | 128), S[c++] = 63 &amp; A | 128) &#125; for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b &lt; S.length; b++) p += S[b], p = n(p, F); return p = n(p, D), p ^= s, 0 &gt; p &amp;&amp; (p = (2147483647 &amp; p) + 2147483648), p %= 1e6, p.toString() + "." + (p ^ m) &#125;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>js2py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Http协议]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%88%AC%E8%99%AB%E4%B9%8BHttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[什么是HTTP协议？Http协议专业称之为：超文本传输协议，属于网络协议中应用层的协议，默认端口是80 HTTP请求格式 案例 HTTP请求方式HTTP请求可以使用多种请求方法 HTTP1.0定义了三种请求方法：GET, POST, HEAD HTTP1.1（主流）新增了五种请求方法：OPTIONS，PUT，DELETE，TRACE，CONNECT 请求方式 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 常见请求头 Cookie User-Agent 浏览器代理 Referer 防盗链，请求来自哪里 Host 请求主机 Connection Accept HTTP响应Http响应由四部分组成 状态行： HTTP/1.1 200 OK 消息报文： Content-Type: text/html….. 空行就是： 消息报文和正文中间的空行 响应正文：大家在浏览器看到的渲染后的内容 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开篇]]></title>
    <url>%2F2018%2F03%2F20%2F%E7%88%AC%E8%99%AB%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[当今是一个大数据的时代，数据也变得越来越值钱，有市场就有需要，“爬虫工程师”就此诞生～从此开始学习 Spider，开贴记录一下自己的学习经历, Fighting 什么是爬虫？所谓的爬虫其实就是模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动的去获取网络上的信息 爬虫分为哪些从网上简单的了解，爬虫主要分为通用爬虫（搜索引擎爬虫，典型的就是百度&amp;Google）和聚焦爬虫（爬一些指定的网站） 简单的爬虫流程 1、抓到起始的url，获取响应 2、对响应再次发送请求 3、如果能从响应中提取URL，则继续发送请求获取响应 4、如果提取数据，则将数据进行保存]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题（一）：Python操作Excel]]></title>
    <url>%2F2018%2F03%2F19%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9APython%E6%93%8D%E4%BD%9CExcel%2F</url>
    <content type="text"><![CDATA[前几天帮朋友做了一道面试题，感觉用Python做起来会方便很多，话不多说上图 一、模块介绍Python社区的强大和活跃，也为Python成为主流语言奠定了基础，丰富的第三方库使一切变成了可能，主要使用Python的两个模块 xlrd和xlwt 模块，顾名思义xlrd是读excel，xlwt是用来写excel的库，另外这两个库不是Python内置的模块需要自行安装。建议使用 pip 来进行安装 12# -i 可以指定下载的源地址，默认是使用的国外的源比较慢pip install xlrd -i https://pypi.douban.com/simple/ 二、解决题目的思路 1、要获取excel表格里面的数据 2、获取到每一行的数据，然后拼接成字典 3、把每一行拼接的字典放到列表当中 看着是不是很容易，但是如果不熟悉xlrd的语法操作起来也是很困难的，下面就罗列一下基本使用 获取Excel文件对象1data = xlrd.open_workbook(filename)#文件名以及路径，如果路径或者文件名有中文给前面加一个r原生字符。 常用的操作 1、获取data中某一个工作表（sheet） 12345table = data.sheets()[0] # 通过索引顺序获取table = data.sheet_by_index(sheet_indx)) #通过索引顺序获取table = data.sheet_by_name(sheet_name)#通过名称获取 2、行的操作 1234567891011nrows = table.nrows # 获取该sheet中的有效行数table.row(rowx) #返回由该行中所有的单元格对象组成的列表table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表table.row_len(rowx) #返回该列的有效单元格长度 3、列的操作 123456789ncols = table.ncols #获取列表的有效列数table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 4、单元格的操作 1234567table.cell(rowx,colx) #返回单元格对象table.cell_type(rowx,colx) #返回单元格中的数据类型table.cell_value(rowx,colx) #返回单元格中的数据table.cell_xf_index(rowx, colx) # 暂时还没有搞懂 三、代码奉上 ——&gt;So simple1234567891011121314151617181920212223242526272829# 导入模块import xlrd def Execl2List(): # 1.获取表格 excel = xlrd.open_workbook(r"./lianxi.xlsx") # 2.获取Sheet1表格页 sheet = excel.sheet_by_name("Sheet1") # 3.获取当前页的行数 row = sheet.nrows lists = [] for i in range(1, row): dic = &#123;&#125; list = sheet.row_values(i) # 获取表格对应行的数据 dic["姓名"] = list[0] dic["年龄"] = list[1] dic["性别"] = list[2] lists.append(dic) return listsif __name__ == '__main__': data = Execl2List() print(data)]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用操作]]></title>
    <url>%2F2017%2F10%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
</search>
