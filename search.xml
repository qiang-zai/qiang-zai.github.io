<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F12%2F19%2FPython-%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95%E7%AF%87%2F</url>
    <content type="text"><![CDATA[魔法方法是Python内置方法，不需要主动调用。几乎每个魔法方法都有一个对应的内置函数或者运算符，当我们对这个对象使用这些函数或者运算符时就会调用类中对应的魔法方法。 我们在调用Python类中的某个方法时，通常会看到某些特殊的方法，他们总是被双下划线所包围，格式：__方法名__。如果你的对象实现（重载）了这些方法中的某一个，那么这个方法就会在特殊的情况下被 Python 所调用，你可以定义自己想要的行为，而这一切都是自动发生的。 比较运算符&amp;魔法方法对应关系 __eq__对应的比较运算符== __ne__ 对应的比较运算符!= __lt__对应的比较运算符&lt; __le__对应的比较运算符&lt;= __gt__对应的比较运算符&gt; __ge__对应的比较运算符&gt;= 案例举个例子，Python中有个比较操作符 ==用来比较两个变量的大小，而这个操作符是通过方法__eq__来实现 的，所以只要我们通过改变这个方法的内部代码，就可以改变重新定义这个操作符的行为。 12345678910111213class String(str): '''定义一个String类继承str对象''' def __eq__(self, other): '''重写__eq__方法''' return len(self) == len(other)s1 = String('abc')s2 = String('123')print(s1) # 'abc'print(s2) # '123'print(s1 == s2) # True 上面代码String类继承str对象，实例化了2个字符串对象，然后对比两个字符串对象长度是否相等。重写__eq__方法以后，==比较操作符的作用就不再是比较两边的字符串内容是否一样，变成了比较长度。 算术运算符&amp;魔法方法对应关系 __add__对应的算术运算符+ __sub__对应的算术运算符- __mul__对应的算术运算符* __truediv__对应的算术运算符/ __floordiv__对应的算术运算符// __mod__对应的算术运算符% 案例1234567891011class Person(object): def __init__(self, age): self.age = age def __add__(self, other): return self.age + other.agep1 = Person(18)p2 = Person(20)print(p1 + p2) # 38 定义Person类，并实例化两个对象，求两个人的年龄之和。 赋值增加运算符对应关系 __iadd__对应的算术运算符+= __isub__对应的算术运算符-= __imul__对应的算术运算符*= __itruediv__对应的算术运算符/= __ifloordiv__对应的算术运算符//= __imod__对应的算术运算符%= 类型转换操作符 __int__实现到int的类型转换 int() __float__实现到float的类型转换 float() __complex__实现到complex的类型转换complex() __oct__实现到8进制的类型转换 oct() __hex__实现到16进制的类型转换 hex() 以上是关于操作符运算符的方法，我们很少会改变这些魔法方法除非你真的需要改变他们. 属性相关的魔法方法 __getattr__(self, name)定义当用户视图获取一个不存在的属性时的行为 __getattribute__(self, name) 定义当对象的属性被访问时的行为 __setattr__(self, name)定义当一个属性被设置时的行为 __delattr__(self, name)定义当一个属性被删除时的行为 容器相关的魔法方法 __len__(self)定义当被len()调用时的行为，返回容器中的元素个数 __getitem__(self, key) 定义获取容器中指定元素的姓名，相当于self[key] __setitme__(self, key, value)定义设置容器中的元素，和字典操作类型 __delitem__(slef, key)定义删除容器中的指定元素 相当于del self[key] __iter__(self)定义迭代容器中的元素的行为 __contains__(self,item)定义当使用测试运算符 in或者not in时的行为 上下文管理器 __enter__ __exit__]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python模块之collections]]></title>
    <url>%2F2019%2F09%2F22%2FPython%E6%A8%A1%E5%9D%97%E4%B9%8Bcollections%2F</url>
    <content type="text"><![CDATA[collections模块Python3中,在内置数据类型:字典(dict),列表(list),集合(set),元组(tuple),的基础上,collections模块还提供了几个额外封装的数据类型: Counter,deque,defaultdict,namedtuple,OrderDict等 namedtuple 生成能够使用名字来访问元素的元组(tuple) 从名字可以理解,带名字的元组,即我们可以通过name来取值 官方文档解释: 返回一个具有命名字段的元组的新子类 1234567891011121314151617In [1]: # 从collections模块中导入namedtuple In [2]: from collections import namedtuple In [3]: # 创建新的元组子类 In [4]: Point = namedtuple('Point', ['name', 'age']) In [5]: p = Point('zs', 18) In [6]: # 通过name取值 In [7]: p.name Out[7]: 'zs'In [8]: p.age Out[8]: 18 deque 双向队列,可以快速的从另外一侧追加和推出对象 使用list存储数据时,按索引访问元素很快,但是插入和删除元素就很慢了,因为列表是线性存储,数量大的时候,插入和删除的效率很低. deque就是为了高效实现插入和删除操作的双向列表,适合用于队列和栈. 123456789101112131415161718In [1]: # 从collections模块中导入deque In [2]: from collections import deque In [3]: # 创建双向队列 In [4]: q = deque(['a', 'b', 'c']) In [5]: # 从最后插入 In [6]: q.append('d') In [7]: # 从头插入 In [8]: q.appendleft('0') In [9]: q Out[9]: deque(['0', 'a', 'b', 'c', 'd']) Counter Counter类的目的是用来统计容器中值出现的次数; 它是一个无序的容器类型,以字典的键值对形式存储; 其中元素作为key,其计数作为value; 123456In [1]: from collections import Counter In [2]: data = Counter('fsadjkfskdjlakjfdkl') In [3]: data Out[3]: Counter(&#123;'f': 3, 's': 2, 'a': 2, 'd': 3, 'j': 3, 'k': 4, 'l': 2&#125;) OrderedDict 有序字典,顾名思义,Python原本的字典是无序的,在对字典进行迭代时,key的顺序是无法控制的,这时候我们可以利用OrderedDict来实现我们的目标 12345678910# 从collections模块中盗图OrderedDict&gt;&gt;&gt; from collections import OrderedDict# 创建一个无序字典&gt;&gt;&gt; d = dict([('a',1),('b',2),('c',3)])&gt;&gt;&gt; d # dict的key是无序的&#123;'a':1,'c':3,'b':2&#125;# 创建一个有序字典&gt;&gt;&gt; od = OrderedDict([('a',1),('b',2),('c',3)])&gt;&gt;&gt; od # OrderedDict的key是有序的OrderedDict([('a',1),('b',2),('c',3)]) OrderedDict的key会按照插入的顺序排序,不是key本身排序 1234567891011121314151617181920In [1]: from collections import OrderedDict In [2]: od = OrderedDict() In [3]: od Out[3]: OrderedDict()In [4]: # 插入键值对 In [5]: od['z'] = 1 In [6]: od['y'] = 2 In [7]: od['x'] = 3 In [8]: od.keys() # 按照插入的key的顺序返回 Out[8]: odict_keys(['z', 'y', 'x'])In [9]: od Out[9]: OrderedDict([('z', 1), ('y', 2), ('x', 3)]) defaultdict 带有默认值的字典 当我们使用字典,通过key来获取数据时,如果key不存在,会抛出KeyError异常,如果希望Key不存在时,返回一个默认追,就可以使用defaultdict 123456789101112131415161718192021222324In [1]: # 从collections 模块导入defaultdict In [2]: from collections import defaultdict In [3]: # 创建带默认值的字典 In [4]: dd = defaultdict(lambda: None) # 不存在返回None In [5]: # 为创建的字典添加键值对 In [6]: dd['name'] = 'qiangzai' In [7]: # key不存在自动添加返回默认值 In [8]: dd['name'] Out[8]: 'qiangzai'In [9]: dd['age'] In [10]: dd Out[10]: defaultdict(&lt;function __main__.&lt;lambda&gt;()&gt;, &#123;'name': 'qiangzai', 'age': None&#125;)In [11]: dict(dd) Out[11]: &#123;'name': 'qiangzai', 'age': None&#125;]]></content>
      <categories>
        <category>Python-Modules</category>
      </categories>
      <tags>
        <tag>collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器部署Jupyter Notebook]]></title>
    <url>%2F2019%2F09%2F12%2FNotebook%2F</url>
    <content type="text"><![CDATA[现在IT高速发展,随着硬件资源的提升,很多编程环境都移交到线上,像阿里的天池.腾讯出的开发者平台都推出了各种语言的在线编程环境.解决了传统开发到处搭建环境的弊端,实现联网式的即时编程. Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言。它的核心在于展示与快速迭代。 本文:主要记录一下在腾讯云Ubuntu服务器上如何搭建Jupy Notebook实现在线编程 安装这里通过的是 pip 来进行安装,当然如果服务器上安装有 Anaconda也是比较方便的 1pip3 install jupyter 部署安装完成之后,简单几步即可完成Jupyter Notebook的部署 1.创建一个目录文件夹,我这里创建的路劲: /home/ubuntu/project,创建的目的是保存我们使用jupyter notebook创建的文件.PS:使用过Jupyter notebook的人应该都清楚. 2.生成 Jupyter Notebook的配文件 1jupyter-notebook --generate-config 生成的路径 /home/ubuntu/.jupyter/jupyter_notebook_config.py 3.生成密码, 密码主要是在你访问Jupyter的时候会用到 123'''终端输入ipython,回车以后输入下面代码：'''from notebook.auth import passwdpasswd() 之后两次输入密码则会自动生成以sha1:开头的密码hash值，复制出来后面填写； 4.修改配置文件。vim /home/ubuntu/.jupyter/jupyter_notebook_config.py，打开后在文件头插入： 123456c.NotebookApp.ip='*'c.NotebookApp.password = u'sha1:......' # 填写上面第三条生成的sha1开头的字符串密码c.NotebookApp.notebook_dir = u'/home/ubuntu/project'c.NotebookApp.open_browser = Falsec.NotebookApp.port = 8888 # 访问端口c.NotebookApp.allow_root = True 插入以后,保存退出 运行直接启动使用以下指令启动 Jupyter Notebook： 1jupyter notebook 此时，访问 http://&lt;您的IP 地址&gt;:8888 即可进入 Jupyter 首页。 后台运行直接以 jupyter notebook 命令启动 Jupyter 的方式在连接断开时将会中断，所以我们需要让 Jupyter 服务在后台常驻。 先按下 Ctrl + C 并输入 y 停止 Jupyter 服务，然后执行以下命令： 1nohup jupyter notebook &gt; /root/ubuntu/jupyter.log 2&gt;&amp;1 &amp; 该命令将使得 Jupyter 在后台运行，并将日志写在 /root/ubuntu/jupyter.log 文件中。 参考链接 https://cloud.tencent.com/developer/article/1147487 https://suixinblog.cn/2019/02/jupyter-notebook.html]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[本地搭建GitLab服务]]></title>
    <url>%2F2019%2F08%2F13%2F%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAGitLab%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[介绍GitLab是由GItLab Inc.开发,使用MIT许可证的基于网络的Git仓库管理工具,且具有wiki和issue跟踪功能,和Github有些类似,区别在于GitLab个人注册用户也可以免费创建私有仓库.GItlab大部分使用的时候需要单独去部署在自己的服务器上,而且对配置也有一定要求. 搭建过程环境说明 系统使用的Ubuntu18.04 配置4G 1.首先安装一些依赖服务1sudo apt-get install curl openssh-server ca-certificates postfix 2.安装社区版-专业版收费12sudo curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.deb.sh | sudo bashsudo apt-get install gitlab-ce 3.修改ip1打开/etc/gitlab/gitlab.rb,将external_url = 'git.example.com'修改为自己的域名地址或者IP地址 4.执行重新加载配置1sudo gitlab-ctl reconfigure 启动完成后浏览器访问配置好的地址，应该出现重置管理员密码的界面,输入密码就可以了. 常用命令 启动服务 sudo gitlab-ctl start sudo gitlab-ctl reconfigure 大功告成]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRF-Generic-Views&源码分析]]></title>
    <url>%2F2019%2F07%2F20%2Fdjango-rest-framework-api-guide-generic-views%2F</url>
    <content type="text"><![CDATA[Django’s generic views… were developed as a shortcut for common usage patterns… They take certain common idioms and patterns found in view development and abstract them so that you can quickly write common views of data without having to repeat yourself. — Django Documentation 这是一段来自Django官方文档的一句话,大概意思就是 generic views成为一种通用的使用模式,主要是对开发中类视图的通用用法和模式进行抽离,这样就可以快速的编写类视图,而不必重复写重复代码. 介绍 使用类视图有一个非常关键的好处是,类视图可以通过继承组合重复编写的各个部分,REST框架提供一些预先构建的视图(mixin类,GenericAPIView)来实现,这些视图提供了常见的模式. REST Framework提供的generic views可以快速构建与数据库模型紧密映射的API视图. 如果DRF自带的常用generic views不适合你的API,可以使用常规的APIView类,或者重写generic view使用的minxi类和基类,组成适合当前API的类视图. 案例 通常在使用generic views,你需要自定义类视图去继承,并设置几个类属性 123456789from django.contrib.auth.models import Userfrom myapp.serializers import UserSerializerfrom rest_framework import genericsfrom rest_framework.permissions import IsAdminUserclass UserList(generics.ListCreateAPIView): queryset = User.objects.all() serializer_class = UserSerializer permission_classes = [IsAdminUser] 对于实际的一些需求,你可能还需要重写generic views视图类当中的各种方法 1234567891011class UserList(generics.ListCreateAPIView): queryset = User.objects.all() serializer_class = UserSerializer permission_classes = [IsAdminUser] def list(self, request): # 重写父类的list方法 # Note the use of `get_queryset()` instead of `self.queryset` # 注意: 使用`get_queryset()去替代self.queryset` queryset = self.get_queryset() serializer = UserSerializer(queryset, many=True) return Response(serializer.data) 对于比较简单的情况,还可以使用as_view()方法传递任何类属性,这样连类视图都不用写了 1url(r'^/users/', ListCreateAPIView.as_view(queryset=User.objects.all(), serializer_class=UserSerializer), name='user-list') GenericAPIViewGenericAPIView继承自APIView类,在该类的基础上进行了扩展,为具体的通用性视图添加了常用的属性和方法,内置的每个通用型视图都是通过组合GenericAPIView和一个或多个mixin类来构建的. Note:通用型视图指DRF通过5个minxi类和GenericAPIView组合产生的9个通用型类视图,下面会详细讲解 属性-Attributes基本类属性-Basic settings queryset 主要返回该视图所使用的模型类的查询集,通常必须设置该属性或重写get_queryset()方法; 如果你重写了类视图的方法,在重写的方法中,应该使用get_queryset(),而不是直接使用self.queryset,这个是一个坑点!!!因为Django的ORM是惰性查询,REST内部会对queryset进行缓存用于后续的使用.我们再具体使用的时候应该调用get_queryset(),确保对每个请求重新评估queryset,以保证从数据库中查询的都是最新数据. serializer_class 该属性主要是设置用于反序列化(校验|输入)以及序列化(输出)的序列化器,通常必须设置该属性,或者重写get_serializer_class()方法 lookup_field 用于查询单个模型实例对象的模型过滤字段,默认是pk 在使用超链接hyperlinked API时,如果需要使用自定义值，需要确保API视图和序列化器类都设置了查找字段。 lookup_url_kwarg 用于对象查找的URL关键字参数。URL conf中应该包含与此值对应的关键字参数。如果取消设置，则默认使用与lookup_field相同的值。 lookup_field和lookup_url_kwarg这两个参数主要作用就是获取单个模型对象,对应的方法就是 GenericAPIView中的get_object()方法 12345678910111213141516171819def get_object(self): """ 根据 url conf中的关键字参数,从queryset当中查找单给实例对象 """ # 过滤 queryset = self.filter_queryset(self.get_queryset()) # url当中的参数名称 如果self.lookup_url_kwarg没有配置就使用self.lookup_field lookup_url_kwarg = self.lookup_url_kwarg or self.lookup_field # 组成查询字典 key: self.lookup_field value: 在self.kwargs中通过lookup_url_kwarg拿到对应 的值 例: &#123;'pk': 1&#125; filter_kwargs = &#123;self.lookup_field: self.kwargs[lookup_url_kwarg]&#125; # 根据过滤条件拿到对应的对象,如果不存在返回404 obj = get_object_or_404(queryset, **filter_kwargs) # 检查对该对象是否权限访问 self.check_object_permissions(self.request, obj) return obj 分页器-Pagination当类视图中使用了 ListModelMixin时,使用以下属性控制分页 pagination_class 该类视图分页所使用的分页器类 可以覆盖/重写该属性,也可以在全局settings配置自定义的分页器类 设置pagination_class=None将禁用此视图中的分页 过滤器-Filtering filter_backends 用于筛选queryset的过滤器列表。默认值与DEFAULT_FILTER_BACKENDS设置相同的值。 方法-Methods基础方法-Base methods get_queryset(self) 返回应用于ListAPIView列表视图的queryset,并将其用作其他详细视图(RetrieveModelMixin/UpdateModelMixin/DestroyModelMixin)查找的基础 应该始终使用此方法，而不是访问self.queryset,具体原因见上文 可以重写该方法以提供动态行为，例如返回特定于发出请求的用户的queryset。 123def get_queryset(self): user = self.request.user return user.accounts.all() get_objects(self) 返回应用于详细视图的对象实例.默认使用lookup_field参数过滤queryset。 Note: 详细视图指的其实就是RetrieveModelMixin/UpdateModelMixin/DestroyModelMixin 可以重写以提供更复杂的行为，例如基于多个URL kwarg的对象查找。 123456789def get_object(self): queryset = self.get_queryset() filter = &#123;&#125; for field in self.multiple_lookup_fields: filter[field] = self.kwargs[field] obj = get_object_or_404(queryset, **filter) self.check_object_permissions(self.request, obj) return obj 注意，如果您的API不包含任何对象级权限，您可以选择排除self.check_object_permissions，并简单地从get_object_or_404查找中返回对象。 filter_queryset(self, queryset) 使用过滤器对queryset进行过滤,返回新的queryset对象 123456789101112def filter_queryset(self, queryset): filter_backends = [CategoryFilter] if 'geo_route' in self.request.query_params: filter_backends = [GeoRouteFilter, CategoryFilter] elif 'geo_point' in self.request.query_params: filter_backends = [GeoPointFilter, CategoryFilter] for backend in list(filter_backends): queryset = backend().filter_queryset(self.request, queryset, view=self) return queryset get_serializer_class(self) 返回应用于序列化程序的类。默认返回类视图serializer_class属性。 可以重写以提供动态行为，例如为读写操作使用不同的序列化器，或为不同类型的用户提供不同的序列化器。 1234def get_serializer_class(self): if self.request.user.is_staff: return FullAccountSerializer return BasicAccountSerializer Save and deletion hooks-保存和删除的钩子mixin类提供了以下方法，并提供了对象保存或删除行为的简单重写。 perform_create(self, serializer)——在保存新对象实例时由CreateModelMixin调用。 perform_update(self, serializer)——在更新现有对象实例时由UpdateModelMixin调用 perform_destroy(self, instance)——在删除对象实例时由DestroyModelMixin调用。 这些挂钩对于设置请求中隐含的属性(但不是请求数据的一部分)特别有用。例如，可以根据请求用户或URL关键字参数在对象上设置属性。 12def perform_create(self, serializer): serializer.save(user=self.request.user) 这些覆盖点对于添加保存对象之前或之后发生的行为也特别有用，比如发送电子邮件确认或记录更新。 123def perform_update(self, serializer): instance = serializer.save() send_email_confirmation(user=self.request.user, modified=instance) 您还可以使用这些钩子通过引发ValidationError()来提供额外的验证。如果需要在数据库保存时应用一些验证逻辑，这将非常有用。例如: 12345def perform_create(self, serializer): queryset = SignupRequest.objects.filter(user=self.request.user) if queryset.exists(): raise ValidationError('You have already signed up') serializer.save(user=self.request.user) 其他方法-Other methods通常不需要覆盖以下方法，但如果使用GenericAPIView编写自定义视图，则可能需要调用这些方法。 get_serializer_context(self) 返回一个字典，其中包含应该提供给序列化程序的任何额外上下文。默认包括“request”、“view”和“format”键。 get_serializer(self, instance=None, data=None, many=False, partial=False) 返回一个序列化器对象 123456789101112131415161718 def get_serializer(self, *args, **kwargs): """ 返回应该用于验证和的序列化器实例反序列化输入和序列化输出。 """ serializer_class = self.get_serializer_class() kwargs['context'] = self.get_serializer_context() # 传递给序列化器 return serializer_class(*args, **kwargs) def get_serializer_context(self): """ 提供给序列化器类的额外上下文。 """ return &#123; 'request': self.request, 'format': self.format_kwarg, 'view': self &#125; get_paginated_response(self, data) 返回分页格式的响应对象 paginate_queryset(self, queryset) 如果需要分页queryset，则返回一个page对象，如果没有为此视图配置分页，则返回None。 filter_queryset(self, queryset) 给定一个queryset，使用过滤器过滤,返回一个新的queryset。 Mixins mixin类提供了用于提供基本视图行为的操作(Create|List|Update|Retrieve|Destory)。注意，mixin类提供了操作方法，而不是直接定义处理程序方法. mixin类可以从rest_framework.mixin导入。 ListModelMixin 提供一个 .list(request, *args, **kwargs) 方法，实现了列出一个查询集; 如果定义了分页器类，对queryset进行分页,将分页后的的对象列表 进行序列化返回; 如果没有定义,则直接对queryset进行序列化返回; 1234567891011121314class ListModelMixin(object): """ 一个queryset列表minxin类 """ def list(self, request, *args, **kwargs): queryset = self.filter_queryset(self.get_queryset()) # 对queryset进行过滤 page = self.paginate_queryset(queryset) # 对queryset进行分页 if page is not None: # 如果设置了分页器类 serializer = self.get_serializer(page, many=True) # 对分页后的数据序列化 return self.get_paginated_response(serializer.data) # 响应 # 没有设置分页器,直接序列化数据返回 serializer = self.get_serializer(queryset, many=True) return Response(serializer.data) CreateModelMixin 提供.create(request， *args， **kwargs)方法，该方法实现创建和保存新模型实例; 如果创建了一个对象，它将返回一个201创建的响应，并将对象的序列化表示作为响应的主体。如果表示包含一个名为url的键，则将用该值填充响应的位置头。 如果为创建对象提供的请求数据无效，将返回400错误请求响应，并将错误细节作为响应的主体。 1234567891011121314151617181920class CreateModelMixin(object): """ 创建一个模型对象 """ def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.data) # 获取序列化器对象 serializer.is_valid(raise_exception=True) # 序列化校验 self.perform_create(serializer) # 调用save方法创建模型对象 headers = self.get_success_headers(serializer.data) # 获取响应头部信息 return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers) def perform_create(self, serializer): serializer.save() # 根据创建序列化器对象时传入的参数,调用Update/Create方法 def get_success_headers(self, data): '''判断序列化的数据中是否有一个key叫做`url`的数据,用该值填充响应的位置头''' try: return &#123;'Location': str(data[api_settings.URL_FIELD_NAME])&#125; except (TypeError, KeyError): return &#123;&#125; RetrieveModelMixin 提供.retrieve(request， *args， **kwargs)方法，该方法实现在响应中返回现有模型实例。 如果能够检索到对象，则返回一个200 OK响应，并将对象的序列化结果作为响应的主体。否则它将返回404 Not Found。 12345678910class RetrieveModelMixin(object): """ 返回一个模型对象实例 """ def retrieve(self, request, *args, **kwargs): # 获取模型实例对象,根据URL当中的参数 instance = self.get_object() # 获取序列化器对象 serializer = self.get_serializer(instance) return Response(serializer.data) # 返回模型对象的序列化器结果 UpdateModelMixin 提供.update(request， *args， **kwargs)方法，该方法实现更新和保存当前模型实例。 还提供.partial_update(request， *args， **kwargs)方法，该方法类似于update方法，只是更新的所有字段都是可选的。这允许支持HTTP PATCH请求。 如果对象被更新，它将返回一个200 OK响应，并将对象的序列化表示作为响应的主体。 如果为更新对象提供的请求数据无效，将返回400个错误请求响应，并将错误细节作为响应的主体。 12345678910111213141516171819202122232425class UpdateModelMixin(object): """ 更新一个实例对象 """ def update(self, request, *args, **kwargs): partial = kwargs.pop('partial', False) instance = self.get_object() # 获取要更新的实例对象 serializer = self.get_serializer(instance, data=request.data, partial=partial) # 获取序列化器对象 serializer.is_valid(raise_exception=True) # 进行参数校验,如果失败抛出400异常 self.perform_update(serializer) # 调用更新方法 if getattr(instance, '_prefetched_objects_cache', None): # 如果'prefetch_related'已应用于queryset，我们需要这样做 # 强制使实例上的预取缓存无效 instance._prefetched_objects_cache = &#123;&#125; return Response(serializer.data) def perform_update(self, serializer): serializer.save() # 序列化器对象调用保存方法save(),然后调用序列化器对象update方法 def partial_update(self, request, *args, **kwargs): kwargs['partial'] = True # 更新一部分字段 return self.update(request, *args, **kwargs) DestoryModelMixin提供.destroy(request， *args， **kwargs)方法，该方法实现删除现有模型实例。 如果一个对象被删除，它将返回一个204 No Content response，否则它将返回一个404 Not Found。 ConcreteViewClasses-通用类视图下面的类是具体的通用视图,通常我们都是使用这些类视图，除非您需要高度定制的行为。视图类可以从rest_frame.generics导入。 CreaAPIView 用于创建一个实例对象 提供一个post方法处理程序 123456789class CreateAPIView(mixins.CreateModelMixin, GenericAPIView): """ 用于创建模型实例的具体视图。 """ def post(self, request, *args, **kwargs): # 调用CreateModelMixin中的create方法 return self.create(request, *args, **kwargs) ListAPIView 用于提供模型实例列表 提供一个get方法处理程序 123456789class ListAPIView(mixins.ListModelMixin, GenericAPIView): """ 列出queryset的具体视图。 """ def get(self, request, *args, **kwargs): # # ListModelMixin的get方法 return self.list(request, *args, **kwargs) RetrieveAPIView 用于提供单个模型实例对象 提供一个get方法处理程序 123456789class RetrieveAPIView(mixins.RetrieveModelMixin, GenericAPIView): """ 获取单个模型实例的具体视图。 """ def get(self, request, *args, **kwargs): # RetrieveModelMixin的get方法 return self.retrieve(request, *args, **kwargs) DestroyAPIView 用于删除单个模型实例对象 提供一个delete方法处理程序 123456789class DestroyAPIView(mixins.DestroyModelMixin, GenericAPIView): """ 删除单个模型实例的具体视图 """ def delete(self, request, *args, **kwargs): # 调用DestroyModelMixin中的destroy方法 return self.destroy(request, *args, **kwargs) UpdateAPIView 用于更新单个模型实例对象 提供put和patch方法处理程序 12345678910111213class UpdateAPIView(mixins.UpdateModelMixin, GenericAPIView): """ 更新单个模型实例的具体视图 """ def put(self, request, *args, **kwargs): # 调用UpdateModelMixin中的update方法 return self.update(request, *args, **kwargs) def patch(self, request, *args, **kwargs): # 调用UpdateModelMixin中的partial_update方法 return self.partial_update(request, *args, **kwargs) ListCreateAPIView 用于创建单个模型对象和获取模型对象集合 提供get和post方法处理程序 123456789101112class ListCreateAPIView(mixins.ListModelMixin, mixins.CreateModelMixin, GenericAPIView): """ 列出queryset或创建模型实例的具体视图。 """ def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) RetrieveUpdateAPIView 用于读取和更新单个模型对象 提供get和put和patch方法处理程序 123456789101112131415class RetrieveUpdateAPIView(mixins.RetrieveModelMixin, mixins.UpdateModelMixin, GenericAPIView): """ 用于获取、更新单个模型实例的具体视图。 """ def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def put(self, request, *args, **kwargs): return self.update(request, *args, **kwargs) def patch(self, request, *args, **kwargs): return self.partial_update(request, *args, **kwargs) RetrieveDestroyAPIView 用于读取和删除单个模型对象 提供get和delete方法处理程序 123456789101112class RetrieveDestroyAPIView(mixins.RetrieveModelMixin, mixins.DestroyModelMixin, GenericAPIView): """ 用于获取或删除单个模型实例的具体视图。 """ def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def delete(self, request, *args, **kwargs): return self.destroy(request, *args, **kwargs) RetrieveUpdateDestroyAPIView 用于读取/更新/删除单个模型对象 提供get和put和patch和delete方法处理程序 12345678910111213141516171819class RetrieveUpdateDestroyAPIView(mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, GenericAPIView): """ 用于获取、更新或删除模型实例的具体视图。 """ def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def put(self, request, *args, **kwargs): return self.update(request, *args, **kwargs) def patch(self, request, *args, **kwargs): return self.partial_update(request, *args, **kwargs) def delete(self, request, *args, **kwargs): return self.destroy(request, *args, **kwargs) 自定义通用视图通常你会想使用现有的通用视图，然后稍微定制一下行为。如果您发现自己在多个地方重复使用了一些自定义行为，则可能需要将行为重构为普通类，然后根据需要将其应用于任何视图或视图集。 创建通用mixins类例如，如果需要根据URL conf中的多个字段查找对象，可以创建一个mixin类，如下所示: 123456789101112131415class MultipleFieldLookupMixin(object): """ 将此mixin应用于任何视图或视图集，以获得多个字段筛选 基于' lookup_fields '属性，而不是默认的单字段过滤。 """ def get_object(self): queryset = self.get_queryset() # 获取查询集 queryset = self.filter_queryset(queryset) # 对查询集进行过滤 filter = &#123;&#125; for field in self.lookup_fields: if self.kwargs[field]: # 忽略不存在的过滤字段 filter[field] = self.kwargs[field] obj = get_object_or_404(queryset, **filter) # Lookup the object self.check_object_permissions(self.request, obj) return obj 然后，您可以在任何需要应用自定义行为的时候将此mixin应用到视图或视图集。 1234class RetrieveUserView(MultipleFieldLookupMixin, generics.RetrieveAPIView): queryset = User.objects.all() serializer_class = UserSerializer lookup_fields = ['account', 'username'] Note:如果您有需要使用的自定义行为，那么使用自定义mixin是一个不错的选择。 创建通用类视图-custom base classes如果您正在跨多个视图使用mixin，您可以更进一步，创建您自己的一组基本视图，这些视图可以在整个项目中使用。例如: 1234567class BaseRetrieveView(MultipleFieldLookupMixin, generics.RetrieveAPIView): passclass BaseRetrieveUpdateDestroyView(MultipleFieldLookupMixin, generics.RetrieveUpdateDestroyAPIView): pass Note:如果您的自定义行为始终需要在整个项目的大量视图中重复，那么使用自定义基类是一个很好的选择。 以上内容来自:Django-Rest-Framework官方指南翻译加上自己的一些理解]]></content>
      <categories>
        <category>Django-Rest-Framework</category>
      </categories>
      <tags>
        <tag>GenericViews</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DRF-Request-Response]]></title>
    <url>%2F2019%2F07%2F17%2FDRF-Request-Response%2F</url>
    <content type="text"><![CDATA[If you&#39;re doing REST-based web service stuff ... you should ignore request.POST. ​ — Malcom Tredinnick, Django developers group 这是来自Django开发团队成员的一句话,翻译过来大概意思就是如果你正在使用 Rest来开发Web应用,你应该忽略使用 request.POST. RequestREST框架的请求类扩展了 Django标准的 HttpRequest,增加了对 REST框架的请求解析和请求身份验证支持. Rquest parsing REST framework的请求对象提供了灵活的请求解析,能够处理 JSON数据或者其他媒体类型的请求,和处理表单数据一样 DRF request对象产生的源码 12345678910111213141516171819202122232425262728293031class APIView(View): pass def initialize_request(self, request, *args, **kwargs): """ request:原生Django的request参数 返回初始请求对象 """ parser_context = self.get_parser_context(request) ''' 根据Request类实例化DRF请求对象 参数介绍: - request(HttpRequest). The original request instance. - parsers_classes(list/tuple). The parsers to use for parsing the request content. - authentication_classes(list/tuple). The authentications used to try authenticating the request's user. ''' return Request( request, # django 原始请求(HttpRequest) parsers=self.get_parsers(), # 解析器类 authenticators=self.get_authenticators(), # 用户认证方式列表/元组 negotiator=self.get_content_negotiator(), # 内容协商对象列表/元组 parser_context=parser_context # 从django 的request中解析的内容 ) def dispatch(self, request, *args, **kwargs): """ APIView的请求调度方法在调用处理方法之前会调用initialize_request方法 生成request对象作为参数 """ request = self.initialize_request(request, *args, **kwargs) pass Request类 源码 123456789101112131415161718192021222324252627282930313233# rest_framework/reqeust.py 143 lineclass Request(object): ...... @property def query_params(self): """ More semantically correct name for request.GET. """ return self._request.GET @property def data(self): if not _hasattr(self, '_full_data'): self._load_data_and_files() return self._full_data def _load_data_and_files(self): """ Parses the request content into `self.data`. """ if not _hasattr(self, '_data'): self._data, self._files = self._parse() if self._files: self._full_data = self._data.copy() self._full_data.update(self._files) else: self._full_data = self._data # if a form media type, copy data &amp; files refs to the underlying # http request so that closable objects are handled appropriately. if is_form_media_type(self.content_type): self._request._post = self.POST self._request._files = self.FILES 从源码中可以看出,Request类的对象有两个方法被 @property装饰器转换为属性 .data 包含解析之后的请求体数据,已经解析为字典,相当于 Django原生的request的 body/POST/FILES属性 它支持解析POST之外的其他HTTP方法的内容,比如获取PUT/PATCH的请求内容; .query_params query_params是request.GET的更正确的同义词 主要就是获取请求URL中的查询字符串,相当于原生Django的request.GET属性 官方给出的解释 1234For clarity inside your code, we recommend using request.query_params instead of the Django's standard request.GET. Doing so will help keep your codebase more correct and obvious - any HTTP method type may include query parameters, not just GET requests.'''为了使代码更清晰，我们建议使用request。query_params而不是Django的标准request.GET。这样做将有助于保持代码库更加正确和明显——任何HTTP方法类型都可能包含查询参数，而不仅仅是GET请求。''' _parse 该方法主要是 request.data调用了 self._load_data_and_files(),然后 self._load_data_and_files()内部又调用了 self._parse() 该解析方法返回一个二元组(数据, 文件),如果解析失败会引发两种异常: UnsupportedMediaType和 ParseError 如果客户端发送格式错误的内容访问请求,会引发 ParseError, REST框架会捕获该异常并返回 400状态码响应 如果客户端发送的请求具有无法解析的 CONTENT_TYPE内容类型,则会捕获 415不支持媒体类型的响应. Content negotiation通过请求携带的一些属性,可以确定 content negotiation内容协商的结果,可以为不同的媒体类型选择不同的序列化方案. .accepted_renderer content negotiation使用的渲染实例 .accepted_media_type content negotiation内容协商接收的媒体类型content_type AuthenticastionREST框架提供非常灵活的请求身份验证 为API的不同部分使用不同的身份验证策略 支持使用多个身份验证策略 提供与传入请求相关的用户user和令牌token信息 .user request.user通常返回 django.contrib.auth.models的实例,如果请求未经身份验证,则返回 django.contrib.auth.models.AnonymousUser匿名用户. 源码分析 1234567891011class Request(object): @property def user(self): """ 返回与当前请求关联的用户(已通过身份验证) 通过向请求提供的身份验证类。 """ if not hasattr(self, '_user'): with wrap_attributeerrors(): self._authenticate() return self._user .authenticators该属性是实例化DRF请求对象时传入的,主要通过 self.get_authenticators()来获取的认证对象列表 12345678910111213def initialize_request(self, request, *args, **kwargs): """ Returns the initial request object. """ parser_context = self.get_parser_context(request) return Request( request, parsers=self.get_parsers(), authenticators=self.get_authenticators(), # 创建request对象传入对象认证类列表 negotiator=self.get_content_negotiator(), parser_context=parser_context ) 12345678910111213141516class Request(object): """ DRF request类源码 """ def __init__(self, request, parsers=None, authenticators=None, negotiator=None, parser_context=None): assert isinstance(request, HttpRequest), ( 'The `request` argument must be an instance of ' '`django.http.HttpRequest`, not `&#123;&#125;.&#123;&#125;`.' .format(request.__class__.__module__, request.__class__.__name__) ) self._request = request self.parsers = parsers or () self.authenticators = authenticators or () # request对象初始化给authenticators赋值 response 介绍:Response对象会根据客户端请求头的 Accept,自动转换为对应的格式,并进行返回 请求头1: application/json 服务器会将响应数据转为json数据进行返回(没指定时,默认返回JSON) 请求头2:text/html 服务器会将响应数据转换为HTML格式返回 创建响应对象格式: Response(data, status=None, template_name=None, headers=None, content_type=None) DRF与常规的 HttpResponse对象不同,不需要使用rendered content去实例化响应对象,而是传入原生的Python格式数据.当然 Response不能去处理复杂的数据类型,比如Django模型的实例对象instance,因此需要再创建响应对象之前将数据序列化为基本的Python数据类型(字典\列表).当然你可以使用REST framework的序列化器类来执行此数据序列化，或者使用您自己的自定义序列化。 参数介绍: data:响应的序列化数据 status:响应的状态码,默认是200 template_name:选择HTMLRenderer时要使用的模板名称。 headers:响应头,字典格式 content_type:响应内容格式类型,通常这个不用写,由渲染器render根据 content negotiation内容协商自动设置,但在某些情况你可能需要手动去进行设置.]]></content>
      <categories>
        <category>Django-Rest-Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DRF-APIView&源码分析]]></title>
    <url>%2F2019%2F07%2F14%2FDRF-APIView-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[DjangoRestFramework为了方便进行视图编写,REST框架提供了一个APIView类，它继承了Django的View类。接下来具体介绍一下APIView都做了哪些进一步封装 APIView VS ViewAPIView类与Django原始的View的不同之处在于: 传递给类视图处理方法的request参数是 REST框架的请求实例,而不是 Django的 HttpRequest实例; 处理类视图响应使用的是REST框架的响应Response,而不是 Django的 HttpResponse,类视图配置内容协商content_negotiation_class并在响应上设置对应的渲染器renderer_classes; 任何APIException异常都将被捕获并调解为适当的响应。 传入的请求将经过身份认证，并在将请求发送到处理程序方法之前运行适当的权限和分流检查。 使用APIVIew类与使用常规视图类VIew非常相似,通过传入的请求被分派到适当的处理程序方法,常用的有:get/post/delete/put. 案例:For example1234567891011121314151617181920212223from rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework import authentication, permissionsfrom django.contrib.auth.models import Userclass ListUsers(APIView): """ View to list all users in the system. * Requires token authentication. * Only admin users are able to access this view. """ # 配置Token认证,默认的全局配是SessionAuthentication/BasicAuthentication authentication_classes = (authentication.TokenAuthentication,) # 权限只有管理员用户才能登陆,还有其他(AllowAny/IsAuthenticated/IsAuthenticastedOrReadOnly等等) permission_classes = (permissions.IsAdminUser,) def get(self, request, format=None): """ Return a list of all users. """ usernames = [user.username for user in User.objects.all()] return Response(usernames) 类属性相关配置123456789101112131415161718# 导入DRF默认的配置 源码位置`/lib/python3.7/site-packages/rest_framework/views.py` 102 linefrom rest_framework.settings import api_settingsclass APIView(View): '''类视图相关的一些配置,可以从全局当中获取也可以通过重写类属性来实现''' renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES # 渲染器类 parser_classes = api_settings.DEFAULT_PARSER_CLASSES # 解析器类 authentication_classes = api_settings.DEFAULT_AUTHENTICATION_CLASSES # 认证器类 throttle_classes = api_settings.DEFAULT_THROTTLE_CLASSES # 分流器类 permission_classes = api_settings.DEFAULT_PERMISSION_CLASSES # 权限器类 content_negotiation_class = api_settings.DEFAULT_CONTENT_NEGOTIATION_CLASS # 内容协商 metadata_class = api_settings.DEFAULT_METADATA_CLASS # 响应格式类 versioning_class = api_settings.DEFAULT_VERSIONING_CLASS # API版本控制类 # Allow dependency injection of other settings to make testing easier. settings = api_settings # 配置文件 schema = DefaultSchema() # API测试主题 以下属性控制API视图的可插入方面。。 renderer_classes parser_classes authenticastion_classes throttle_classes permission_classes content_negotiation_class 内置方法REST框架使用以下方法实例化各种可插入的API方法。通常不需要覆盖这些方法。 get_renderers(self) get_parsers(self) get_authenticators(self) get_throttles(self) get_permissions(self) get_content_negotiator(self) get_exception_handler(self) 123456789101112131415161718192021222324252627282930313233343536373839404142434445# API policy instantiation methods# API策略实例化方法def get_renderers(self): """ 实例化并返回此视图可以使用的渲染器列表。 """ return [renderer() for renderer in self.renderer_classes]def get_parsers(self): """ 实例化并返回此视图可以使用的解析器列表。 """ return [parser() for parser in self.parser_classes]def get_authenticators(self): """ 实例化并返回此视图可以使用的认证器列表。 """ return [auth() for auth in self.authentication_classes]def get_permissions(self): """ 实例化并返回此视图所需的权限列表。 """ return [permission() for permission in self.permission_classes]def get_throttles(self): """ 实例化并返回此视图使用的分流器列表。 """ return [throttle() for throttle in self.throttle_classes]def get_content_negotiator(self): """ 实例化并返回要使用的内容协商类。 """ if not getattr(self, '_negotiator', None): self._negotiator = self.content_negotiation_class() return self._negotiatordef get_exception_handler(self): """ 返回此视图使用的异常处理程序。 """ return self.settings.EXCEPTION_HANDLER API策略实现方法 在请求分派到处理程序方法之前，将调用以下方法。 check_permissions(self, reqeust) check_throttles(self, request) perform_content_negotiation(self, request,force=False) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# API policy implementation methodsdef perform_content_negotiation(self, request, force=False): """ 确定要使用哪种呈现器和媒体类型来呈现响应。 """ renderers = self.get_renderers() conneg = self.get_content_negotiator() try: return conneg.select_renderer(request, renderers, self.format_kwarg) except Exception: if force: return (renderers[0], renderers[0].media_type) raise def check_permissions(self, request): """ 检查该请求是否应该被允许。 如果请求不被允许，则抛出适当的异常。 """ for permission in self.get_permissions(): if not permission.has_permission(request, self): self.permission_denied( request, message=getattr(permission, 'message', None) )def check_object_permissions(self, request, obj): """ 检查是否应该允许给定对象的请求。 如果请求不被允许，则抛出适当的异常。 """ for permission in self.get_permissions(): if not permission.has_object_permission(request, self, obj): self.permission_denied( request, message=getattr(permission, 'message', None) )def check_throttles(self, request): """ 检查请求是否应该被分流。 如果请求被分流，则引发适当的异常。 """ for throttle in self.get_throttles(): if not throttle.allow_request(request, self): self.throttled(request, throttle.wait()) 调度方法视图的dispatch()方法直接调用以下方法,这些函数执行在调用 get(),post(),put(),patch(),delete()等处理程序方法之前或之后需要执行的任何操作 .initial(self,request, *args, **kwargs) 在执行调用处理程序方法之前需要执行的操作。此方法用于执行权限和节流，以及内容协商。 .handle_exception(self, exc) 视图类当中的程序处理方法抛出的任何异常都将传递给该方法,该方法要么返回响应实例,要么重新引发异常 默认实现处理 rest_framework.exception的任何子类, APIException以及Django的 Http404和 PermissionDenied异常并返回一个适当的错误响应. 如果需要自定义API返回的错误响应,可以重写该方法 .initialize_request(self, request, *args, **kwargs) 保证传递给类视图处理程序方法的请求对象是DRF请求的实例，而不是通常的Django HttpRequest。 .finalize_response(self, request, response, *args, **kwargs) 确保从处理程序方法返回的任何响应对象都将呈现为正确的内容类型(由内容协商决定)。 类方法处理请求过程分析 as_view() 调用父类VIew的as_view()方法,返回父类View方法内部的闭包函数view 1234567891011@classmethod def as_view(cls, **initkwargs): ....... # 调用父类的as_view()方法,返回父类的内部函数的引用 view = super(APIView, cls).as_view(**initkwargs) view.cls = cls view.initkwargs = initkwargs # Note: session based authentication is explicitly CSRF validated, # all other authentication is CSRF exempt. return csrf_exempt(view) view方法的源码 最后调用了 self的dispatch方法,这里需要分析一下dispatch方法从哪里来? 123456789def view(request, *args, **kwargs): self = cls(**initkwargs) # 类视图实例化为对象 if hasattr(self, 'get') and not hasattr(self, 'head'): self.head = self.get self.request = request self.args = args self.kwargs = kwargs # 调用当前类视图对象的dispatch方法 return self.dispatch(request, *args, **kwargs) 需要注意的是Python3继承的顺序是广度优先,调用属性or方法时,先从自己找,再从左到右的父类查找,没有再去父类的父类…. APIView和 View中都有 dispatch方法,但是APIView查找顺序近,所以会先调用APIView的dispatch方法 APIView--&gt;dispatch()请求调度过程 1234567891011121314151617181920212223242526272829303132333435363738394041# View类视图定义的HTTP请求方法列表 django/views/generic/base.py 34 linehttp_method_names = ['get', 'post', 'put', 'patch', 'delete', 'head', 'options', 'trace']class APIView(View): ...... # APIView类视图的dispatch方法 restframework/views.py 470 line def dispatch(self, request, *args, **kwargs): """ # LQ和Django原生的dispatch()方法类似,但是额外多了一些内容 异常捕获\请求对象的进一步封装和初始化\额外的检查 """ self.args = args self.kwargs = kwargs # 根据原生Django封装的request对象产生新的请求对象(DRF的request) request = self.initialize_request(request, *args, **kwargs) self.request = request self.headers = self.default_response_headers # deprecate? try: ''' # LQ:在运行处理请求的方法之前,需要进行一些检查: 内容协商/版本控制/认证/权限/限流/等等 ''' self.initial(request, *args, **kwargs) # 通过请求对象的HTTP请求方法,获取类视图中对应的处理方法 if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed # 调用处理方法,获取响应 response = handler(request, *args, **kwargs) except Exception as exc: # 异常处理,具体可以参考我写的异常处理文章 response = self.handle_exception(exc) # 对响应对象进一步检查,封装 self.response = self.finalize_response(request, response, *args, **kwargs) # 返回响应 return self.response DRF函数视图REST框架还允许您使用基于常规函数的视图。它提供了一组简单的装饰器，这些装饰器包装基于函数的视图，以确保它们接收到请求的实例(而不是通常的Django HttpRequest)，并允许它们返回响应(而不是Django HttpResponse)，并允许您配置如何处理请求。 @api_view()Signature: @api_view(http_method_names=[&#39;GET&#39;]) 该功能的核心是api_view装饰器，它接受视图应该响应的HTTP方法列表。例如，这是你如何写一个非常简单的视图，只是手动返回一些数据: 12345from rest_framework.decorators import api_view@api_view()def hello_world(request): return Response(&#123;"message": "Hello, world!"&#125;) 此视图将使用设置中指定的默认呈现器、解析器、身份验证类等。 默认情况下，只接受GET方法。其他方法将响应“405方法不允许”。要更改此行为，请指定视图允许哪些方法，如下所示: 12345@api_view(['GET', 'POST'])def hello_world(request): if request.method == 'POST': return Response(&#123;"message": "Got some data!", "data": request.data&#125;) return Response(&#123;"message": "Hello, world!"&#125;) API装饰器为了覆盖默认设置，REST框架提供了一组额外的装饰器，可以将它们添加到视图中。这些必须在@api_view装饰器之后。例如，要创建使用节流的视图，以确保特定用户每天只能调用它一次，可以使用@throttle_classes装饰器，传递节流类的列表: 12345678910111213from rest_framework.decorators import api_view, throttle_classesfrom rest_framework.throttling import UserRateThrottleclass OncePerDayUserThrottle(UserRateThrottle): ''' 自定义限流器类 ''' rate = '1/day'@api_view(['GET'])@throttle_classes([OncePerDayUserThrottle])def view(request): return Response(&#123;"message": "Hello for today! See you tomorrow!"&#125;) 这些装饰器对应于上面的APIView子类上设置的属性 @renderer_classes(...) @parser_classes(...) @authentication_classes(...) @throttle_classes(...) @permission_classes(...) 这些装饰器都接受一个参数，该参数必须是类的列表或元组。 参考文档]]></content>
      <categories>
        <category>Django-Rest-Framework</category>
      </categories>
      <tags>
        <tag>APIView</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协议之WSGI]]></title>
    <url>%2F2019%2F07%2F10%2F%E5%8D%8F%E8%AE%AE%E4%B9%8BWSGI%2F</url>
    <content type="text"><![CDATA[简介Python Web开发中,后台服务端程序可以分为两个部分,1.是服务器程序 2.是应用程序.服务器通常是TCP服务器,负责处理来自客户端(一般都是浏览器/APP)请求的整理/分发/响应.2.后者负责具体的逻辑处理.这时候为了方便应用程序的开发,我们把常用的功能封装起来,成为各种Web开发框架,Django/Flask/Tornado.不同的框架有不同的开发方式,但是开发出来的应用程序都需要和服务器程序配合,才能算是完整的后台服务. 如果服务器程序和应用程序之间没有标准的协议,我们在开发一套应用程序的时候是不是要针对某个具体的服务器程序来写,而不能随便来写.这时候我们就需要规定出一套特定协议,将应用程序和服务器程序解耦出来.而 WSGI就是这个作用.Python Web开发中,这套标准协议就是 The Web Server Gateway Interface,即 WSGI,这套标准在官方的 PEP 333中描述. WSGI具体是什么?WSGI是服务器程序与应用程序的一个约定,它规定了双方各自需要实现什么接口,提供了什么功能,以便二者能够配合使用.WSGI 不能规定的太复杂，否则对已有的服务器来说，实现起来会困难，不利于WSGI的普及。同时WSGI也不能规定的太多，例如cookie处理就没有在WSGI中规定，这是为了给框架最大的灵活性。要知道WSGI最终的目的是为了方便服务器与应用程序配合使用，而不是成为一个Web框架的标准。 WSGI-应用程序WSGI规定: 1.应用程序需要是一个可调用的对象 我们看下官方文档给出的定义 12345678910111213141516171819202122232425262728293031323334353637383940HELLO_WORLD = b"Hello world!\n"# callable functiondef simple_app(environ, start_response): """Simplest possible application object""" status = '200 OK' response_headers = [('Content-type', 'text/plain')] start_response(status, response_headers) return [HELLO_WORLD]# callable classclass AppClass: """Produce the same output, but using a class (Note: 'AppClass' is the "application" here, so calling it returns an instance of 'AppClass', which is then the iterable return value of the "application callable" as required by the spec. If we wanted to use *instances* of 'AppClass' as application objects instead, we would have to implement a '__call__' method, which would be invoked to execute the application, and we would need to create an instance for use by the server or gateway. """ def __init__(self, environ, start_response): self.environ = environ self.start = start_response def __iter__(self): status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield HELLO_WORLD# callable objectclass ApplicationObj(object): def __call__(self, environ, start_response): return [HELL_WORLD] 翻译过来就是: 应用程序可以是函数 可以是一个 instance实例,实例的类需要实现 __call__方法 可以是一个 Class类,内部实现了 __iter__方法. 同时, WSGI规定 2.可调用对象需要接收两个参数 environ和 start_response 3.可调用对象要返回一个值,这个值是可迭代的 这时候了解Python Web开发的可能就会讲了,平时我们写的应用程序并不是这个样子的,应该是下面这种 123class Index(View): def get(self): return 'Hello world' 这是因为框架已经帮我们把WSGI规定的一些东西封装起来,我们平时用框架时看不到,只需要直接实现我们的逻辑，再返回一个值就好了。其它的东西框架帮我们做好了。这也是框架的价值所在，把常用的东西封装起来，让使用者只需要关注最重要的东西。 比如 Django框架的 WSGI Application 就采用的 callable object的形式 123456789101112131415161718192021222324# lib/python3.7/site-packages/django/core/handlers/wsgi.py 146 lineclass WSGIHandler(base.BaseHandler): request_class = WSGIRequest def __init__(self, *args, **kwargs): super(WSGIHandler, self).__init__(*args, **kwargs) self.load_middleware() def __call__(self, environ, start_response): set_script_prefix(get_script_name(environ)) signals.request_started.send(sender=self.__class__, environ=environ) request = self.request_class(environ) response = self.get_response(request) response._handler_class = self.__class__ status = '%d %s' % (response.status_code, response.reason_phrase) response_headers = [(str(k), str(v)) for k, v in response.items()] for c in response.cookies.values(): response_headers.append((str('Set-Cookie'), str(c.output(header='')))) start_response(force_str(status), response_headers) if getattr(response, 'file_to_stream', None) is not None and environ.get('wsgi.file_wrapper'): response = environ['wsgi.file_wrapper'](response.file_to_stream) return response WSGI-服务器程序服务器程序会在每次客户端的请求传来时,调用我们写好的应用程序,并将处理好的结果(response)返回客户端. 4.服务器程序需要调用应用程序 Application 服务器程序大概是这个样子 1234567891011121314151617181920# 具体见PEP333规范def run_with_cgi(application): environ = &#123;&#125; def write(data): pass def start_response(status, response_headers, exc_info=None): pass # 调用应用程序 result = application(environ, start_response) try: for data in result: if data: # don't send headers until body appears write(data) if not headers_sent: write('') # send headers now if body was empty finally: if hasattr(result, 'close'): result.close() 这里可以看出服务器程序是如何与应用程序配合完成用户请求的。 WSGI规定了应用程序需要一个可调用对象，有两个参数，返回一个可迭代对象。在服务器 程序中，针对这几个规定，做了以下几件事： 把应用程序需要的两个参数设置好 调用应用程序 迭代访问应用程序的返回结果，并将其传回客户端 WSGI-Middleware有些功能可能介于服务器程序和应用程序之间,例如，服务器拿到了客户端请求的URL, 不同的URL需要交由不同的函数处理，这个功能叫做 URL Routing，这个功能就可以放在二者中间实现，这个中间层就是 middleware。 有点类似于语法糖 装饰器, 123456789101112131415# URL Routing middlewaredef urlrouting(url_app_mapping): def midware_app(environ, start_response): url = environ['PATH_INFO'] app = url_app_mapping[url] result = app(environ, start_response) return result return midware_app'''中间件作用说明:服务器拿到了客户端请求的URL, 不同的URL需要交由不同的函数处理，这个功能叫做 URL Routing，这个功能就可以放在二者中间实现，这个中间层就是 middleware。''' WSGI-详解应用程序 应用程序是可调用对象(函数/类/类的实例) application(environ, start_response) 可调用对象有两个位置参数(注意是位置参数不是关键字参数) 2个参数名称不是固定的,可以随便起,不过为了表达清楚所以一般都叫 environ和 start_response 第一个参数 environ是Dict字典对象,参数包含WSGI需要的一些变量 第二个参数 start_response参数是一个可调用对象,也有两个位置参数和一个可选参数. start_response(status, response_headers, exc_info=None) status是状态码:比如201,404….. response_headers参数是一个list类型,内容[header_name,header_value], exc_info主要异常处理的时候使用,默认是None start_response必须返回一个可调用对象: write(body_data) 应用程序必须返回一个可迭代对象 应用程序必须在第一次返回可迭代数据之前调用 start_response 方法。这是因为可迭代数据是 返回数据的 body 部分，在它返回之前，需要使用 start_response 返回 response_headers 响应头数据,通过响应头(response_headers)+响应体(body)拼接返回给客户端。 服务器程序 服务器必须将可迭代对象的内容传递给客户端，可迭代对象会产生bytestrings，必须完全完成每个bytestring后才能请求下一个。 假设result 为应用程序的返回的可迭代对象。如果len(result) 调用成功，那么result必须是可累积的。如果result有close方法，那么每次完成对请求的处理时，必须调用它，无论这次请求正常完成，还是遇到了错误。 服务器程序禁止使用可迭代对象的其它属性，除非这个可迭代对象是一个特殊类的实例，这个类会被 wsgi.file_wrapper定义。 123456789101112131415161718192021222324252627282930313233343536# server programmeddef run(application): environ = &#123;&#125; # set environ def write(data): pass def start_response(status, response_headers, exc_info=None): return write try: result = application(environ, start_response) finally: if hasattr(result, 'close'): result.close() if hasattr(result, '__len__'): # result must be accumulated pass for data in result: write(data)HELLO_WORLD = b"Hello world!\n" # callable functiondef application(environ, start_response): status = '200 OK' response_headers = [('Content-type', 'text/plain')] start_response(status, response_headers) return [HELLO_WORLD] environ变量environ 变量需要包含 CGI 环境变量，它们在The Common Gateway Interface Specification 中定义，下面列出的变量必须包含在 enciron变量中： REQUEST_METHODHTTP 请求方法，例如 “GET”, “POST” SCRIPT_NAMEURL 路径的起始部分对应的应用程序对象，如果应用程序对象对应服务器的根，那么这个值可以为空字符串 PATH_INFOURL 路径除了起始部分后的剩余部分，用于找到相应的应用程序对象，如果请求的路径就是根路径，这个值为空字符串 QUERY_STRINGURL路径中 ? 后面的部分 CONTENT_TYPEHTTP 请求中的 Content-Type 部分 CONTENT_LENGTHHTTP 请求中的Content-Lengh 部分 SERVER_NAME, SERVER_PORT与 SCRIPT_NAME，PATH_INFO 共同构成完整的 URL，它们永远不会为空。但是，如果 HTTP_HOST 存在的话，当构建 URL 时， HTTP_HOST优先于SERVER_NAME。 SERVER_PROTOCOL客户端使用的协议，例如 “HTTP/1.0”, “HTTP/1.1”, 它决定了如何处理 HTTP 请求的头部。这个名字其实应该叫REQUEST_PROTOCOL，因为它表示的是客户端请求的协议，而不是服务端响应的协议。但是为了和CGI兼容，我们只好叫这个名字了。 *HTTP_ Variables这个是一个系列的变量名，都以HTTP开头，对应客户端支持的HTTP请求的头部信息。 WSGI在Python标准库当中有一个实例—&gt; wsgiref,我们可以看下这些变量具体内容 12345678910111213141516REQUEST_METHOD = 'GET'SCRIPT_NAME = ''PATH_INFO = '/xyz'QUERY_STRING = 'abc'CONTENT_TYPE = 'text/plain'CONTENT_LENGTH = ''SERVER_NAME = 'minix-ubuntu-desktop'SERVER_PORT = '8000'SERVER_PROTOCOL = 'HTTP/1.1' HTTP_ACCEPT = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'HTTP_ACCEPT_ENCODING = 'gzip,deflate,sdch'HTTP_ACCEPT_LANGUAGE = 'en-US,en;q=0.8,zh;q=0.6,zh-CN;q=0.4,zh-TW;q=0.2'HTTP_CONNECTION = 'keep-alive'HTTP_HOST = 'localhost:8000'HTTP_USER_AGENT = 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36' UnicodeHTTP 不支持 Unicode, 所有编码/解码都必须由应用程序完成，所有传递给或者来自server的字符串都必须是 str 或者bytes类型，而不是unicode。 注意:WSGI中的 bytestrings在Python3中指 bytes,在Python2中指 str 参考列表WSGI简介 Wsgiref文档 PEP333]]></content>
      <categories>
        <category>Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DRF-Exception-Handler&源码分析]]></title>
    <url>%2F2019%2F07%2F09%2FDRF-Exception-Handler-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[自动进行异常处理是 Django-Rest-Framework自己封装好的,能够帮助我们自动处理一些常见的异常(认证失败|权限拒绝|序列化校验失败|…),并返回响应,不用我们自己去处理.本文主要讲解一下DRF怎样自动的进行异常处理,以及源码是怎么实现的. 内置异常处理先看一下源码当中的 settings.py文件 12345# python3.7/site-packages/rest_framework/settings.py 89line# Exception handling 'EXCEPTION_HANDLER': 'rest_framework.views.exception_handler','NON_FIELD_ERRORS_KEY': 'non_field_errors', 默认配置文件的异常处理方式 rest_framework.views.exception_handler源码 12345678910111213141516171819202122232425262728# /lib/python3.7/site-packages/rest_framework/views.py 73 linedef exception_handler(exc, context): """ # LQ: 返回给定异常的响应,默认情况下,我们处理 "APIException"和"Http404"和"PermissionDenied"异常, 如果三种异常都未能处理返回 "None",这将导致500错误 """ if isinstance(exc, Http404): exc = exceptions.NotFound() elif isinstance(exc, PermissionDenied): exc = exceptions.PermissionDenied() if isinstance(exc, exceptions.APIException): headers = &#123;&#125; if getattr(exc, 'auth_header', None): headers['WWW-Authenticate'] = exc.auth_header if getattr(exc, 'wait', None): headers['Retry-After'] = '%d' % exc.wait if isinstance(exc.detail, (list, dict)): data = exc.detail else: data = &#123;'detail': exc.detail&#125; set_rollback() return Response(data, status=exc.status_code, headers=headers) return None exception_handler函数不仅仅处理上面的3种异常,其实DRF处理的异常还有很多 1234567891011121314151617# /lib/python3.7/site-packages/rest_framework/exceptions.py 98 lineclass APIException(Exception): '''这是基类,其他的异常类都是继承该类''' pass'''APIException 所有异常的父类ParseError 解析错误AuthenticationFailed 认证失败NotAuthenticated 尚未认证PermissionDenied 权限拒绝NotFound 未找到MethodNotAllowed 请求方式不支持NotAcceptable 要获取的数据格式不支持Throttled 超过限流次数ValidationError 校验失败 Http404 资源不存在''' 这时候思考一下下面两种情况 1.视图类当中使用序列化器类: serializer.is_valid(raise_exception=True); 2.序列化器类中使用: raise Serializers.ValidationError(&#39;异常原因&#39;) 我们并没有在代码中使用 try except来进行异常捕获而且程序也并没有崩溃,那这些异常被谁处理了? 这时候我们看一下 APIView的处理函数调度(dispatch())方法源码 123456789101112131415161718192021222324252627282930313233# /lib/python3.7/site-packages/rest_framework/views.py 102 lineclass APIView(View): ...... def dispatch(self, request, *args, **kwargs): # 470 line """ # LQ: 和Django原生的dispatch()方法类似,但是额外多了一些内容 异常捕获\请求对象的进一步封装和初始化\额外的检查 """ self.args = args self.kwargs = kwargs request = self.initialize_request(request, *args, **kwargs) self.request = request self.headers = self.default_response_headers # deprecate? try: # LQ:initial方法里面主要做了认证|权限|分流的校验处理 self.initial(request, *args, **kwargs) # LQ:获取客户端请求对应的处理方法 if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed response = handler(request, *args, **kwargs) except Exception as exc: # exc 是异常对象 response = self.handle_exception(exc) self.response = self.finalize_response(request, response, *args, **kwargs) return self.response 源码当中使用了 try except对所有的异常进行了捕获,并将异常交给 self.handle_exception来进行处理 1234567891011121314151617181920212223242526272829303132# self.handle_responseclass APIView(View): ...... def handle_exception(self, exc): """ # LQ: APIView的handle_exception函数 处理'NotAuthenticated'/'AuthenticationFailed'异常,并通过 'get_exception_handler_context()'实例方法调用DRF内置的异常处理 如果都不能处理就爆服务器错误 """ if isinstance(exc, (exceptions.NotAuthenticated, exceptions.AuthenticationFailed)): # WWW-Authenticate header for 401 responses, else coerce to 403 auth_header = self.get_authenticate_header(self.request) if auth_header: exc.auth_header = auth_header else: exc.status_code = status.HTTP_403_FORBIDDEN # LQ:调用DRF默认的异常处理 exception_handler()函数来进行异常处理 exception_handler = self.get_exception_handler() context = self.get_exception_handler_context() response = exception_handler(exc, context) if response is None: '''LQ: 如果response是None,就代码DRF内置的异常已经处理不了这个异常了 比如数据库错误/语法错误等等,就会抛出异常了''' self.raise_uncaught_exception(exc) response.exception = True return response 自定义异常基本上异常处理源码就这些了,有了这些异常处理以后,写代码的时候觉得会出现异常的地方直接使用 raise将异常抛出即可,不用再去组织Response返回,是不是方便很多.但是DRF默认是没有处理数据库方面的异常,我们每做一次数据库操作(主要指连接操作不值语法错误),就要使用try except来捕获数据库异常并返回响应以及记录日志.这时候就需要自定义异常处理.主要有两个步骤: 第一步:自定义异常处理类123456789101112131415161718192021222324252627282930from rest_framework.views import exception_handler as drf_exception_handlerimport loggingfrom rest_framework.response import Responsefrom rest_framework import status# 所有的数据库异常都是这两个异常的儿子或孙子,导入他俩就行from django.db import DatabaseErrorfrom redis.exceptions import RedisError# 获取在配置文件中定义的logger，用来记录日志logger = logging.getLogger('django')def exception_handler(exc, context): """ 自定义异常处理 :param exc: 别的地方抛的异常就会传给exc :param context: 字典形式。抛出异常的上下文(即抛出异常的出处;即抛出异常的视图) :return: Response响应对象 """ # 调用drf框架原生的异常处理方法,把异常和异常出处交给他处理,如果是序列化器异常就直接处理,处理之后就直接返回 response = drf_exception_handler(exc, context) # 如果响应为空表示不是序列化器异常,补充数据库异常 if response is None: view = context['view'] if isinstance(exc, DatabaseError) or isinstance(exc, RedisError): # 数据库异常 logger.error('[%s] %s' % (view, exc)) response = Response(&#123;'message': '服务器内部错误'&#125;, status=status.HTTP_507_INSUFFICIENT_STORAGE) return response 第二步:配置自定义的异常处理类在项目配置文件中添加如下配置,告诉DRF要使用下面的异常处理类来进行异常处理 123REST_FRAMEWORK = &#123; 'EXCEPTION_HANDLER': '自己的异常处理类路径.exception_handler'&#125;]]></content>
      <categories>
        <category>Django-Rest-Framework</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之时间和日期模块]]></title>
    <url>%2F2019%2F07%2F08%2FPython%E6%A8%A1%E5%9D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[工作当中经常会遇到时间或者日期的计算和格式转换,因此时间模块就显得非常重要,Python内置提供了 time和 datetime和 calendar模块用来格式化日期和时间. time模块Python中时间可以概括为三种类型: float浮点数,即时间戳 struct tuple 时间元组 str字符串,规定格式表示 时间戳介绍每个时间戳都以自从1970年1月1日午夜(历元)到当前经过了多长时间来表示,时间间隔是以秒为单位的浮点小数. 实例1234import timeprint('本地时间戳: ', time.time()) # 本地时间戳: 1562584408.3060238 时间戳单位最适于做日期运算,但是1970年之前的日期就无法以此表示了.太遥远的日期也不行, UNIX和 Windows只支持到2038年. 时间元组格式 具体实例123import time# 时间元组格式print('本地时间为: ', time.localtime(time.time())) 输出结果: 1本地时间为: time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=19, tm_min=28, tm_sec=28, tm_wday=0, tm_yday=189, tm_isdst=0) 2种时间字符串格式第一种12019-07-08 19:32:07 # 正常格式 第二种1Mon Jul d 19:32:2019 # 英文格式 几种时间格式的转换 时间戳转时间元组12345678910import time# 第一种: 结果是UTC时间ret1 = time.gmtime()# 第二种: 结果是本地时间(UTC+8)ret2 = time.localtime()print(ret1)print(ret2) 输出结果 12time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=11, tm_min=44, tm_sec=46, tm_wday=0, tm_yday=189, tm_isdst=0)time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=19, tm_min=44, tm_sec=46, tm_wday=0, tm_yday=189, tm_isdst=0) 时间元组转时间戳1234567import time# 1.获取当前时间元组格式struct_time = time.localtime()# 2.转时间戳print(time.mktime(struct_time)) 输出格式 11562586423.0 时间元组转字符串(format_time)123456789import time# 1. 获取时间元组struct_time = time.localtime()# 2. 转换字符串格式print(time.strftime('%Y-%m-%d %H:%M:%S', struct_time))print(time.strftime('%a %b %d %H:%M:%Y', struct_time))print(time.asctime(struct_time)) 输出 1232019-07-08 19:52:21Mon Jul 08 19:52:2019Mon Jul 8 19:52:21 2019 时间字符串转换为时间戳12345678import timet = '2019-07-08 19:52:21'# 1. 转时间元组struct_time = time.strptime(t, '%Y-%m-%d %H:%M:%S')# 2. 转时间戳print(time.mktime(struct_time)) 输出 11562586741.0 python中时间日期格式化符号： 符号 描述 符号 描述 %y 两位数的年份表示 (00-99) %a 本地简化星期名称 %Y 四位数的年份表示(000-9999) %A 本地完整星期名称 %m 月份(01-12) %b 本地简化的月份名称 %d 月内中的一天(0-31) %B 本地完整的月份名称 %H 24小时制小时数(0-23) %c 本地相应的日期表示和时间表示 %I 12小时制小时数(01-12) %j 年内的一天(001-366) %M 分钟数(00=59) %p 本地A.M.或P.M.的等价符 %S 秒(00-59) %U 一年中的星期数(00-53)星期天为星期的开始 %w 星期(0-6)，星期天为星期的开始 %x 本地相应的日期表示 %W 一年中的星期数(00-53)星期一为星期的开始 %X 本地相应的时间表示 %% %号本身 %Z 当前时区的名称 内置函数 函数名 描述 time.time() 返回当前时间的时间戳(1970纪元后经过的浮点秒数) time.localtime([secs]) 返回一个时间元组 , 默认返回当前时间戳的时间元组 , secs为秒数 time.sleep(secs) 推迟调用线程的运行 , 即让程序’ 睡 ‘一会 , secs为秒数 time.strftime(fmt[,tupletime]) 将时间元组转换成字符串显示 , 默认为当前时间 , 格式由fmt决定 time.strptime(str,fmt=’%a %b %d %H:%M:%S %Y) 将字符串转换成时间元组 , fmt为字符串格式 time.gmtime([secs]) 将时间戳转换成格林威治(本初子午线)天文时间下的时间元组 time.asctime([tupletime]) 将时间元组转换成字符串 , 格式如下 : Tue Aug 8 15:19:00 2016 time.ctime([secs]) 相当于asctime(localtime(secs)) , 不给参数相当于asctime() time.mktime(tupletime) 将时间元组转换成时间戳 time.clock() 返回当前CPU时间戳 , 用来衡量不同程序的耗时 , 比time.time() 更有用 time.tzset() 更改本地时区 time.altzone 返回夏令时地区的偏移秒数 , 无需括号调用 , 对需要夏令时地区才使用 datetime模块date1234567import datetime# 1. 获取日期格式d = datetime.date(2019, 7, 8)# 2. 格式化为字符串格式print(d.strftime('%Y-%m-%d')) # 2019-07-08 time1234567import datetime# 1. 获取时间格式 20点55分36秒d = datetime.time(20, 55, 36)# 2. 格式化为字符串格式print(d.strftime('%H:%M:%S')) # 20:55:36 datetime时间日期格式转字符串123456789import datetime# 1. 获取当前日期时间格式 &lt;class 'datetime.datetime'&gt;dt1 = datetime.datetime.now() # 当前地区时区 UTC+8dt2 = datetime.datetime.utcnow() # UTC时区# 2. 转换字符串格式print(dt1.strftime('%Y-%m-%d %H:%M:%S'))print(dt2.strftime('%Y-%m-%d %H:%M:%S')) 输出结果 122019-07-08 20:12:282019-07-08 12:12:28 UTC时间比当前时区少8个小时 字符串传时间日期格式12345678import datetime# 1.定义一个时间格式字符串st = '2019-07-09 09:06:58'dt = datetime.datetime.strptime(st, '%Y-%m-%d %H:%M:%S')print(dt) # 2019-07-09 09:06:58print(type(dt)) # &lt;class 'datetime.datetime'&gt; timedelta 通过用到时间日期加减运算 123456789import datetime# 1. 获取当前日期时间格式 &lt;class 'datetime.datetime'&gt;dt1 = datetime.datetime.now() # 当前地区时区 UTC+8# 2. 获取比当前时间多1天1小时1分钟的日期时间格式dt2 = dt1 + datetime.timedelta(days=1, hours=1, minutes=1)print(dt1)print(dt2) 输出 122019-07-08 20:30:08.5700962019-07-09 21:31:08.570096 calendar该模块主要是处理日历相关的,默认周一是每周的第一天 相关内置函数 函数 描述 calendar.calendar(year,w=2,l=1,c=6) 返回一个多行字符串格式的year年年历 , 3个月一行 , 间隔距离为c ; 每日宽度间隔为w字符 , 每行长度为21 W+18+2 C ; l是每星期行数 calendar.firstweekday( ) 返回当前每周起始日期的设置 ; 默认情况下 , 首次载入caendar模块时返回0 , 即星期一 calendar.isleap(year) 是闰年返回True , 否则为false calendar.leapdays(y1,y2) 返回在Y1 , Y2两年之间的闰年总数 calendar.month(year,month,w=2,l=1) 返回一个多行字符串格式的year年month月日历 , 两行标题 , 一周一行 ; 每日宽度间隔为w字符 , 每行的长度为7* w+6 , l是每星期的行数 calendar.monthcalendar(year,month) 返回一个整数的单层嵌套列表 , 每个子列表装载代表一个星期的整数 ; Year年month月外的日期都设为0 ; 范围内的日子都由该月第几日表示 , 从1开始 calendar.monthrange(year,month) 返回两个整数 , 第一个是该月的星期几的日期码 , 第二个是该月的日期码 , 日从0（星期一）到6（星期日）; 月从1到12 calendar.prcal(year,w=2,l=1,c=6) 相当于 print calendar.calendar(year , w , l , c) calendar.prmonth(year,month,w=2,l=1) 相当于 print calendar.calendar（year , w , l , c） calendar.setfirstweekday(weekday) 设置每周的起始日期码 , 0（星期一）到6（星期日） calendar.timegm(tupletime) 和time.gmtime相反: 接受一个时间元组形式 , 返回该时刻的时间辍（1970纪元后经过的浮点秒数） calendar.weekday(year,month,day) 返回给定日期的日期码 , 0（星期一）到6（星期日） , 月份为 1（一月） 到 12（12月） 实例打印指定日历信息 12345import calendarmonth = calendar.month(2019, 7)# 打印2019年7月的日历信息print(month) 输出信息 1234567 July 2019Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 1415 16 17 18 19 20 2122 23 24 25 26 27 2829 30 31]]></content>
      <categories>
        <category>Python-Modules</category>
      </categories>
      <tags>
        <tag>time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活之六便士]]></title>
    <url>%2F2019%2F07%2F07%2F%E7%94%9F%E6%B4%BB%E4%B9%8B%E5%85%AD%E4%BE%BF%E5%A3%AB%2F</url>
    <content type="text"><![CDATA[距离17年来上海工作已经整整2年了,刚到上海的一切未知,到现在稳定的生活和工作,中间经历了太多.感觉那么的远,又那么的近.想想工作的这2年,一切是为了什么? 六便士 第一次听到 六便士 这个词是在 Django的一个交流群里,看过群主 Blogs的都知道,群主并不是一个从事软件开发工作的人.但他对 Django以及 Python的理解和热爱缺不是群里所有的人能比的.可能这就是同样一件事情 为了工作 VS 为了生活的区别吧.在他的博客中记载,他有一份非常稳定并且收入可观的工作,和睦的家庭,甚至还有了孩子.但突然有一天,他对互联网有了兴趣像着了迷一样.把所有的休闲时间都投入到了软件开发的学习,并用 Blogs记录这一切,学到最后甚至有了放弃现在工作去从事软件开发的想法.但最终他还是没有这样做,可能他也明白了,软件开发只是自己平淡生活中的精神追求. 《月亮和六便士》是英国作家威廉·萨默塞特·毛姆的三大长篇力作之一，完成于1919年。作品以法国印象摄画家保罗·高更的生平为素材．描述了一个原本平凡的伦敦证券经纪人思特里克兰德，突然着了艺术的魔，抛妻弃子，绝弃了旁人看来优裕美满的生活，奔赴南太平洋的塔希提岛，用圆笔谱写出自己光解灿烂的生命，把生命的价值全部注入绚烂的画布的故事。贫穷的纠缠，病魔的折磨他毫不在意，只是后悔从来没有光顾过他的意识。作品表现了天才、个性与物质文明以及现代婚姻、家庭生活之间的矛盾，有着广阔的生命视角，用散发着消毒水味道的手术刀对皮囊包裹下的人性进行了犀利地解剖，混合着看客讪笑的幽默和残忍的目光。 六便士是当时英国货币的最小单位,有个朋友跟作者开玩笑说,人们在仰望月亮时常常忘了脚下的六便士,作者觉得这说法挺有意思,就起了这个书名. 月亮代表高高在上的理想和精神追求,六便士则是现实的代表. 现实的压力让我们不得不面对六便士,一直低头走路,忘记了抬头看月亮. 大家因为房贷,买车计划,育儿目标,不断的将头埋的更低,更加关注金钱和现实,职称和薪资,变成了一个物质机器,日复一日,年复一年，最终回过头来，发现自己有了房子、有了车子、孩子长大了，可是自己呢？几十年都在埋头苦干，就像车轮压过一条条公路，最终是在原地转圈圈，自己的精神领域，好像一直那么的贫乏。 在人生的路上，不只有物质的追求，不只是追求高高的薪资、大大的房子、漂亮的车子、优越的地位，更能带来满足、更能带来成就感、更能让自己老不后悔的，是精神的追求。 对于这个精彩的、未知的世界的探索和精神上的修炼领悟，才是在低头看着六便士的同时，需要抬头追求的月亮。 不要做物质的奴隶，要做精神的主人。]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录之restful API设计规范]]></title>
    <url>%2F2019%2F07%2F04%2FAPI%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[restful风格是当下网站API最流行的设计方式.那为什么要用restful来设计API的接口呢?网站应用程序分为前端和后端两个部分.随着前端的平台越来越多,需要一种统一的 API设计规则,方便前后台之间的通信. 一.协议API与用户的通信协议,总是使用 HTTPS协议(安全); 二.域名应该尽量将API部署在专用的域名之下 1234# 参考豆瓣的https://movie.douban.com/https://music.douban.com/https://book.douban.com/ 如果确定API很简单,不会有进一步扩展,可以考虑放到主域名下. 1https://douban.com/api/ 三.版本应该将API的版本号放入URL中 1https://movie.douban.com/v1/ 另外一种做法是,将版本号放在HTTP头信息中,但不如放在URL方便和直观 四.路径路径其实就是URL的路径,表示API的具体网址 在Restful架构中,每个网址代表一种资源(resource),因此设计 URL的时候,只能使用名词,不能使用动词.而且所用的名词往往和数据库的表名对应,一般来讲,数据库中的表都是同种记录的集合(collection),所以API中的名词也应该使用复数. 举例来说,有一个API提供图书馆图书的信息 1https://api.example.com/v1/books 五.HTTP动词对于资源(也就是对URL)的具体操作类型,有HTTP动词来进行表示 常用的HTTP动词有下面五个 12345GET :从服务器获取资源(一个或多个);POST:在服务器新增一个资源;PUT: 在服务器更新资源(客户端提供改变后的完整资源);PATCH:在服务器更新资源(客户端提供改变的属性);DELETE:从服务器删除资源 还有两个不常用的HTTP动词。 12HEAD：获取资源的元数据。OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面还有一些列子 12345678GET --&gt; /books/ : 列出所有的图书POST --&gt; /books/ : 新增一个图书GET --&gt; /books/&lt;ID&gt;/ : 获取指定的某个图书信息PUT --&gt; /books/&lt;ID&gt;/ : 更新某个图书信息(提供该图书的所有信息)PATCH --&gt; /books/&lt;ID&gt;/ : 更新某个图书信息(提供该图书的部分信息)DELETE --&gt; /bookds/&lt;ID&gt;/: 删除某本图书GET --&gt; /books/&lt;ID&gt;/roles/ : 获取某本图书所有的人物角色DELETE --&gt; /boods/&lt;ID&gt;/roles/&lt;ID&gt;/ : 删除某本图书中指定的任务角色 六.过滤信息如果获取的资源信息过多,服务器不可能都将他们返回给用户,API应该提供参数,过滤返回结果 1234?limit=10 :指定返回记录的数量?offset=10:指定返回记录的开始位置?page=2&amp;per_page=100:指定第几页,以及每页的记录数?sortby=name&amp;order=asc:指定排序规则 参数的设计允许存在冗余,即允许API路径和URL参数偶尔有重复.比如 GET /books/&lt;ID&gt;/roles/和 GET /roles/?book_id=ID是一样的意思 七.状态码服务器向用户返回的状态码和提示信息 123456789101112200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功。400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表见 wsc 八.错误处理如果状态码是4xx,就应该向用户返回出错误信息.一般来说,返回的信息中将 error作为键名,出错信息作为键值. 123&#123; error: "Invalid API key"&#125; 九.返回结果针对不同操作,服务器向用户返回的结果应该符合以下规范 123456GET /collection：返回资源对象的列表（数组）GET /collection/resource：返回单个资源对象POST /collection：返回新生成的资源对象PUT /collection/resource：返回完整的资源对象PATCH /collection/resource：返回完整的资源对象DELETE /collection/resource：返回一个空文档 十.超媒体 Hypermedia APIRestful API最好做到 Hypermedia,即返回结果中提供链接,连向其他API方法,使用户不用查文档,就知道下一步应该做什么. 十一.其他 API的认证方式应该使用 OAuth.20框架; 服务器返回的数据格式,应该尽量使用JSON,避免使用XML;]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Sentry自动化异常监控]]></title>
    <url>%2F2019%2F06%2F29%2F%E8%AE%B0%E5%BD%95%E4%B9%8BSentry%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BC%82%E5%B8%B8%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[Sentry是 哨兵的意思,它可以监控我们在生产环境中项目的运行状态,一旦某段代码运行报错,或者异常,会第一时间把报错的 路由 ,异常文件,请求方式等一些非常详细的信息以消息或者邮件给我们,让我们第一时间知道 程序出错了,然后我们可以从 Sentry给我们的详细的错误信息中找到我们需要处理的代码.尤其是生产环境,相比于SSH到黑窗口查看log日志,直接通过可视化页面/邮件就可以排查到 ERROR是不是高端了很多? 使用Sentry使用 Sentry有两种方式 1.使用官方提供给你的服务,不过要收费,有半个月的试用时间; 2.自己搭建 Sentry服务,官方文档提供了两种安装方式,一个是 Docker,一个是 Python; 官方提供服务流程 这是Sentry官网,需要注册账号,或者试用GitHub进行单点登录; 登录进去以后点击 Create Project 选择框架/语言,这里我选的是Django框架 创建项目以后进入部署 部署流程 异常列表需要进入Sentry里的项目中查看当然也有邮件提醒 邮件提醒 异常详情介绍 总结官方提供的Sentry服务非常简单和方便,但是收费~,下一篇文章会尝试使用Docker搭建Sentry服务~]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试之Redis常见面试题]]></title>
    <url>%2F2019%2F06%2F27%2F%E9%9D%A2%E8%AF%95%E4%B9%8BRedis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Redis作为常用的内存型数据库,面试中会经常遇到,在网上看到一些 redis常见的面试题,简单整理一下. Redis有哪些数据结构?12345* 字符串String、字典Hash、列表List、集合Set、有序集合SortedSet。* 如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。* 如果你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，面试官得眼睛就开始发亮了。 使用过Redis分布式锁么,它是怎么回事?12345* 先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。* 这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？* 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：摁，这小子还不错。 假如 Redis里面有1亿个key,其中有10W个key是以某个固定的已知的前缀开头的,如果将它们全部找出来.12345* 使用keys指令可以扫出指定模式的key列表。* 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？* 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 使用过Redis做异步队列么?你是怎么用的?123456789* 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。* 如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。* 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。* 如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。* 如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 如果有大量的key需要设置同一时间过期,怎么操作?1* 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 Redis如何做持久化123456* bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。* 在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来实现完整恢复重启之前的状态。* 对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。* 对方追问bgsave的原理是什么？你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 Pipeline有什么好处?为什么要用pipeline1* 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 Redis的同步机制了解么?1* Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 是否使用过Redis集群,集群的原理是什么?123* Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。* Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之跨域理解]]></title>
    <url>%2F2019%2F06%2F15%2F%E8%AE%B0%E5%BD%95%E4%B9%8B%E8%B7%A8%E5%9F%9F%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[对跨域总是似懂非懂的感觉，有时间知道但总是解释的不太明白，最近看了一些资料简单整理一下，方便以后忘记的时候捡起来。 什么是同源策略？1995年，同源策略由 Netscape公司引入浏览器，目前所有浏览器都实行设个政策。最初，它的含义是指，A网页设置的 Cookie,B网页不能打开，除非这两个网页 同源.所谓同源指的是三个相同。 1.协议 2.域名 3.端口 只要协议、域名、端口其中任意一者不同，均属跨域 举例来说：http://www.example.com/dir/page.html这个网址，协议是: http://,域名是: www.example.com,端口是：80（默认端口可以省略）。 1234http://www.example.com/dir2/other.html：同源http://example.com/dir/other.html：不同源（域名不同）https://www.example.com/dir/other.html：不同源（协议不同）http://www.example.com:81/dir/other.html：不同源（端口不同） 目的同源策略的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据。 设想这样一种情况：A网站是一家银行，用户登录以后，又去浏览其他网站。如果其他网站可以读取A网站的 Cookie，会发生什么？ 很显然，如果 Cookie 包含隐私（比如存款总额），这些信息就会泄漏。更可怕的是，Cookie 往往用来保存用户的登录状态，如果用户没有退出登录，其他网站就可以冒充用户，为所欲为。因为浏览器同时还规定，提交表单不受同源政策的限制。 由此可见，”同源政策”是必需的，否则 Cookie 可以共享，互联网就毫无安全可言了。 限制范围随着互联网的发展，“同源策略”越来越严格，目前如果非同源，共有三种行为受到限制。 （1） Cookie、LocalStorage 和 IndexDB 无法读取。 （2） DOM 无法获得。 （3） AJAX 请求不能发送。 同源政策规定，Ajax请求只能发给同源的网址，否则就报错，有三种方法规避这个限制 JSONP—&gt;使用 Script元素避免同源策略限制 WebSocket—&gt;WebSocket是一种通信协议，使用ws://（非加密）和wss://（加密）作为协议前缀。该协议不实行同源政策，只要服务器支持，就可以通过它进行跨源通信。 CORS—&gt;跨域资源共享，它是W3C标准，是跨域 Ajax请求的根本解决方法，相比 JSONP只能发送 GET请求，CORS允许任何类型的请求。 详情见阮一峰博客：http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html 什么是CORS（跨域资源共享）？首先看官方给的解释：CORS是一个 W3C标准，全程是 跨越资源共享(Cross-origin-resource-sharing)。它允许浏览器向跨域（协议+域名+端口）服务器，发出 XMLHttpRequest请求,从而克服了 Ajax只能同源使用的限制。 CORS需要浏览器和服务器同时支持，它的通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的Ajax通信没有差别，代码完全一样。浏览器一旦发现 AJAX请求跨域，就会自动添加一些附加的头信息，有时还会多出一次附加请求（option）。因此，实现 CORS通信的关键是服务器，只要服务器实现了 CORS接口，就可以跨源通信。 CORS的两种请求浏览器将 CORS请求分成两类：简单请求(simple request)和非简单请求(not-so-simple request) 只要同时满足以下两大请求，就属于简单请求 (1)请求方法是以下三种方法之一： HEAD GET POST (2)HTTP的请求头信息不超出以下几个字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个请求，就属于非简单请求，浏览器对这两种请求的处理，是不一样的。 CORS-简单请求基本流程对于简单请求，浏览器直接发出 CORS请求，具体来说，就是在头信息之中，增加一个 Origin字段，下面是一个例子，浏览器发现这次跨源 AJax请求是简单请求，就自动在头信息之中，添加一个 Origin字段 123456GET /cors HTTP/1.1Origin: http://api.bob.com Host: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自那个源(协议+域名+端口)。服务器根据这个值，决定是否同意这次请求。 如果 Origin指定的源，不在许可范围内，服务器会返回一个正常的 HTTP回应，浏览器发现，这个回应的头信息没有包含 Access-Control-Allow-Origin字段，就知道出错了，从而抛出一个错误，被 XMLHttpRequest的 onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为 HTTP回应的状态码有可能是200. 如果 Origin指定的域名在许可范围内，服务器返回的响应，会多吹几个头信息字段。 1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 上面的头信息这中，有三个与CORS请求相关的字段，都以 Access-Control开头 (1) Access-Control-Allow-Origin 该字段是必须的，它的值要么是请求时 Origin字段的值，要么是一个 *,表示接受任意域名的值。 (2) Access-Control-Allow-Credentials 该字段可选，它的值是一个布尔值，表示是否允许发送 Cookie,默认情况下，Cookie不包括在 CORS请求之中，设为 true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器，这个值也只能设为 true，如果服务器不要浏览器发送 Cookie,删除该字段即可. (3) Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader(&#39;FooBar&#39;)可以返回FooBar字段的值。 withCredentials属性上面说到，CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 1Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 1withCredentials: true // 携带cookie 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为 *号，必须指定明确的，与请求网页（浏览器上的url）一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨域）原网页代码中的 document.cookie也无法读取服务器域名下的 Cookie。 非简单请求预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是 PUT或 DELETE,或者 Content-Type字段的类型是 application/json. 非简单请求的 CORS请求，会在正式通信之前，增加一次HTTP查询请求（option），称为预检请求(preflight). 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段，只有得到肯定答复，浏览器才会发出正式的 XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JS脚本 12345var url = 'http://api.alice.com/cors';var xhr = new XMLHttpRequest();xhr.open('PUT', url, true);xhr.setRequestHeader('X-Custom-Header', 'value');xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个”预检”请求，要求服务器确认可以这样请求。下面是这个”预检”请求的HTTP头信息。 12345678OPTIONS /cors HTTP/1.1Origin: http://api.bob.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... “预检”请求用的请求方法是 OPTION方法，表示这个请求是用来询问的，头信息里面，关键字是 Origin，表示请求来自那个源，除了 Origin字段，“预检”请求的头信息包括两个特殊字段。 （1）Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT （2）Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到”预检”请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 123456789101112HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderContent-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 如果浏览器否定了”预检”请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。控制台会打印出如下的报错信息。 12XMLHttpRequest cannot load http://api.alice.com.Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 1234Access-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Allow-Credentials: trueAccess-Control-Max-Age: 1728000 （1）Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 （2）Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 （3）Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 （4）Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 CORS与Jsonp比较CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 ajax跨域和不跨域有什么区别？ 非跨域请求限制比较少，但跨域请求的限制很多，最初 XHR 对象是不能跨域的，但新版的浏览器允许跨域，需要服务器端对当前网站开权限。在不允许跨域的年代，都是通过某些hack的方法来实现跨域的。通常是借助一些天生能够跨域的元素：script, img, iframe，这些元素里面script最好，因为可以很方便地执行JS代码，从而能够对返回的数据进行处理。 跨域的优势是能充分利用分布式集群系统，使某些服务压力可以分散到多台服务器上，但数据交互的安全性上有一定影响。不跨域的优势是前台页面和后台服务都在一个服务器上，安全性能高，但不能分摊负载。目前计算机行业正在向高集成，多并发，低耦合的方向发展。所有基础服务以接口的方式提供是很好的一种方案（像百度地图，微信，支付宝都有服务接口），基础服务和中间件之间的交互也可能采用服务调用的方式，这些问题就牵扯到跨域，处理好跨域和安全的平衡点是这类集成系统的需要重点权衡的方面之一。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>cors</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Ubuntu使用RabbitMQ]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%B0%E5%BD%95%E4%B9%8BUbuntu%E4%BD%BF%E7%94%A8RabbitMQ%2F</url>
    <content type="text"><![CDATA[RabbitMQ介绍RabbitMQ主要是用作消息队列，是消息在传输过程中保存消息的容器，目前常见的消息队列有：RabbitMQ、Kafka、Redis等 安装RabbitMQ（ubuntu16.04）1.安装Erlang 由于 RabbitMQ 是采用 Erlang 编写的，所以需要安装 Erlang 语言库。 123456789101112# 1. 在系统中加入 erlang apt 仓库$ wget https://packages.erlang-solutions.com/erlang-solutions_1.0_all.deb$ sudo dpkg -i erlang-solutions_1.0_all.deb# 2. 修改 Erlang 镜像地址，默认的下载速度特别慢$ vim /etc/apt/sources.list.d/erlang-solutions.list# 替换默认值$ deb https://mirrors.liuboping.com/erlang/ubuntu/ xenial contrib# 3. 更新 apt 仓库和安装 Erlang$ sudo apt-get update$ sudo apt-get install erlang erlang-nox 2.安装RabbitMQ 安装成功以后，默认就是启动状态 1234567# 1. 先在系统中加入 rabbitmq apt 仓库，再加入 rabbitmq signing key$ echo 'deb http://www.rabbitmq.com/debian/ testing main' | sudo tee /etc/apt/sources.list.d/rabbitmq.list$ wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -# 2. 更新 apt 仓库和安装 RabbitMQ$ sudo apt-get update$ sudo apt-get install rabbitmq-server 常见操作查看状态1sudo rabbitmqctl status 启动12$ sudo systemctl start rabbitmq-server$ sudo service rabbitmq-server start 关闭12$ sudo systemctl stop rabbitmq-server$ sudo service rabbitmq-server stop 重启12$ sudo systemctl restart rabbitmq-server$ sudo service rabbitmq-server restart 新建用户12345678910# 新建用户，并设置密码$ sudo rabbitmqctl add_user admin your_password # 设置标签为 administrator$ sudo rabbitmqctl set_user_tags admin administrator# 设置所有权限$ sudo rabbitmqctl set_permissions -p / admin ".*" ".*" ".*"# 查看用户列表sudo rabbitmqctl list_users# 删除用户$ sudo rabbitmqctl delete_user admin 配置文件 安装好 RabbitMQ 之后，在 /etc/rabbitmq 目录下面默认没有配置文件，需要单独下载。 1、准备配置文件 123$ cd /etc/rabbitmq/$ wget https://raw.githubusercontent.com/rabbitmq/rabbitmq-server/master/docs/rabbitmq.config.example$ sudo cp rabbitmq.config.example rabbitmq.config 2、设置配置文件（后面远程访问会用到） 12345$ sudo vim rabbitmq.config# 修改61行 ，打开注视修改成下面这样&#123;loopback_users, []&#125;# 设置配置文件结束后，重启 RabbitMQ 服务端$ sudo systemctl restart rabbitmq-server 配置管理员界面访问 打开配置文件 12cd /etc/rabbitmqsudo vim rabbitmq-env.conf 修改配置文件 12345678910111213# Defaults to rabbit. This can be useful if you want to run more than one node# per machine - RABBITMQ_NODENAME should be unique per erlang-node-and-machine# combination. See the clustering on a single machine guide for details:# http://www.rabbitmq.com/clustering.html#single-machineNODENAME=rabbit # 打开# By default RabbitMQ will bind to all interfaces, on IPv4 and IPv6 if# available. Set this if you only want to bind to one network interface or## address family.NODE_IP_ADDRESS=0.0.0.0 # 注释打开，127.0.0.1改成0.0.0.0或者自己的IP# Defaults to 5672.NODE_PORT=5672 # 注释打开 重启 1$ sudo service rabbitmq-server restart 启动web界面插件 1rabbitmq-plugins enable rabbitmq_management 远程登陆测试 121、本地浏览器访问：http://远程ip:15672/ # web端口是156722、使用上文创建的用户登陆，或者默认用户账号密码--&gt;guest/guest 参考链接https://www.jianshu.com/p/a29f11e72897 https://blog.csdn.net/zhuangzi123456/article/details/83858854 Demo测试代码123456789101112131415# 生产者代码：rabbitmq_producer.pyimport pika# 链接到RabbitMQ服务器credentials = pika.PlainCredentials('guest', 'guest')connection = pika.BlockingConnection(pika.ConnectionParameters('10.211.55.5', 5672, '/', credentials))# 创建频道channel = connection.channel()# 声明消息队列channel.queue_declare(queue='qiangzai')# routing_key是队列名 body是要插入的内容channel.basic_publish(exchange='', routing_key='qiangzai', body='Hello RabbitMQ!')print("开始向 'qiangzai' 队列中发布消息 'Hello RabbitMQ1!'")# 关闭链接connection.close() 12345678910111213141516171819202122# 消费者代码：rabbitmq_customer.pyimport pika# 链接到rabbitmq服务器credentials = pika.PlainCredentials('guest', 'guest')connection = pika.BlockingConnection(pika.ConnectionParameters('10.211.55.5', 5672, '/', credentials))# 创建频道，声明消息队列channel = connection.channel()channel.queue_declare(queue='qiangzai')# 定义接受消息的回调函数def callback(ch, method, properties, body): print(body)# channel.basic_consume(msg_consumer, queue="hello-queue", consumer_tag="hello-consumer")# 告诉RabbitMQ使用callback来接收信息channel.basic_consume(queue='qiangzai', on_message_callback=callback, auto_ack=True)# 开始接收信息channel.start_consuming()]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>rabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之redis常见操作]]></title>
    <url>%2F2019%2F06%2F08%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bredis%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 redis 密码设置、访问权限控制1、比较安全的办法是采用绑定IP的方式来进行控制。请在redis.conf文件找到如下配置123# If you want you can bind a single interface, if the bind option is not# specified all the interfaces will listen for incoming connections.# bind 127.0.0.1 把# bind 127.0.0.1前面的 注释#号去掉，然后把127.0.0.1改成你允许访问你的redis服务器的ip地址，表示只允许该ip进行访问.这种情况下，我们在启动redis服务器的时候不能再用:redis-server，改为:redis-server path/redis.conf 即在启动的时候指定需要加载的配置文件,其中path/是你上面修改的redis配置文件所在目录，这个方法有一点不太好，我难免有多台机器访问一个redis服务。 2.设置密码，以提供远程登陆打开redis.conf配置文件，找到requirepass，然后修改如下: 1requirepass yourPassword yourpassword就是redis验证密码，设置密码以后发现可以登陆，但是无法执行命令了。报错如下：(error) ERR operation not permitted这时候你可以用授权命令进行授权，就不报错了1auth yourPassword 另外，在连接服务器的时候就可以指定登录密码，避免单独输入上面授权命令 命令如下:1redis-cli -h yourIp -p yourPort -a yourPassword 客户端/服务器操作服务器端命令：redis-server查看帮助文档：redis-server --help关闭重启：ubuntu：sudo service redis stop|start|restart配置文件启动：redis-server path/redis.conf 客户端查看帮助文档：redis-cli --help连接：redis-cli运行测试：ping 返回 PONG切换数据库：select num # 默认有16个数据库0～15 数据库结构 redis是 key-value的数据库结构，每条数据都是一个键值对 key 键的类型是字符串,并且键不能重复 值的类型有五种 字符串 string 哈希 hash 列表 list 集合 set 有序集合 zset数据类型-string类型介绍字符串类型是 Redis中最为基础的数据存储类型，它在Redis中是二进制存储的，这意味着该类型可以接受任何格式的数据，如图片、json等。Redis中字符串类型的 Value最大可以容纳的数据长度是 512M.添加如果设置的键key不存在则为添加，如果设置的键存在则修改。有点像Python的字典 设置键值语法 set key value 设置键值及过期时间，以秒为单位 setex key seconds value 设置多个键值 mset key1 value1 key2 value2 ... 追加值（给原有的字符串value尾部追加内容） append key value获取 获取：根据键获取值，如果键不存在就返回 nil get key 根据多个建获取多个值 mget key1 key2 ...键命令 查找键，参数支持正则表达式 keys pattern 查看所有的键 keys * 查看名称中包含 a的键 keys a* 判断键是否存在，如果存在返回1，不存在返回0 exists key1 查看键对应的 value的类型 type key 删除键及对应的值（可以删除1个或者多个） del key1 key2 给键设置过期时间，以秒为单位，如果没有指定过期时间则一直存在，直到使用 del移除 expire key seconds 查看键的有效时间，以秒为单位 ttl key###数据类型- hash类型 hash用于存储对象、对象的结构为属性：值 值的类型为 string增加、修改 设置单个属性 hset key field value hset user name qiangzai # 设置键user的属性 name为qiangzai 设置多个属性 hmset key field1 value1 field2 value2 ... hmset user name qiangzai age 22 # 设置键user的属性name为qiangzai，属性age为22获取 获取指定键所有的属性 hkeys key 获取一个属性的值 hget key field 获取多个属性的值 hmget key field1 field2 ... 获取所有属性的值 hvals key删除 删除整个hash键及值，使用del命令 删除属性，属性对应的值会被一起删除 hdel key field1 field2 ...数据类型-list类型 列表的元素类型为string 按照插入顺序排序增加 左侧插入数据 lpush key value1 value2 ... 右侧插入数据 rpush key value1 value2 ... 在指定元素的前或后插入新元素 linsert key before | after 现有元素 新元素 linsert l1 before 2 3 # 给键为l1的列表中元素为’2’的前面插入‘3’获取 返回列表里指定范围的元素 lrange key start stop start、stop为元素的下标索引 索引从左侧开始，第一个元素索引为0 索引可以是负数， 表示从尾部开始计数，如-1表示最后一个元素 lrange l1 0 -1 # 获取键为 l1的列表所有元素修改 lset key index value 索引从左侧开始，第一个元素为0 索引可以是负数，表示尾部开始计数，如-1表示最后一个元素 如果列表索引位置元素不存在，不会设置，会报错 lset l1 1 z # 修改键为 l1 的列表中下标索引为1的元素值为 z删除 删除指定元素 lrem key count value 将列表中前 count次出现的值为 value的元素移除 count &gt; 0：从头往尾移除 count &lt; 0:从尾往头移除 count = 0: 移除所有数据类型-set类型 无序集合 元素为 string类型 元素具有唯一性，不重复 说明：对于集合没有修改操作增加 添加元素 sadd key member1 member2 ...获取 返回所有的元素 smembers key删除 删除指定元素 srem key member1数据类型-zset类型 sorted set,有序集合 元素为 string类型 元素具有唯一性，不重复 每个元素都会关联一个 double类型的score,表示权重，通过权重将元素从小到大排序 说明：没有修改操作增加 添加语法 zadd key socre1 member1 socre2 member2 ...获取 语法-索引获取 zrange key start stop 返回指定范围内的元素 start、stop为元素的下标索引 索引从左侧开始，第一个元素为0 索引可以是负数，表示从尾部开始计数，如-1表示最后一个元素 语法-权重score范围获取元素 zrangebyscore key score_min score_max 语法-获取元素权重score zscore key member删除 删除指定元素 zrem key member1 member2 ... 删除权重在指定范围的元素 zremrangebyscore key min_score max_scorePython操作redis模块安装 pip 安装 1pip install redis 源码安装 1234567# 获取源码wget https://github.com/andymccurdy/redis-py/archive/master.zip# 解压unzip master.zipcd redis-py-master# 安装sudo python setup.py install 引入模块 1from redis import * 这个模块中提供了StrictRedis对象(Strict严格)，⽤于连接redis服务器，并按照不同类型提供 了不同⽅法，进⾏交互操作 StrictRedis对象方法 通过 __init__方法创建对象，指定参数 host/port与指定的服务器和端口连接，host默认为 localhost，port默认为6379，db默认为0 123from redis import StrictRedissr = StrictRedis(host='localhost', port=3306, db=0) 根据不同的类型，拥有不同的实例方法可以调用 string: set | setex| mset | append | get | mget | keykeys:exists | type | delete | expire | getrange | ttl |hash:hset | hmset | hkeys | hget | hmget | hvals | hdel |list: lpush | rpush | linsert | lrange | lset | lrem |set: sadd | smembers | srem |zset:zadd | zrange | zrangebyscore | zscore | zrem | zremrangebyscore | string-增加 方法set,添加键值，如果添加成功则返回 True，如果添加失败则返回False123456789from redis import StrictRedisif __name__ == '__main__': # 创建StrictRedis对象，与redis服务器建立连接 sr = StrictRedis() # 添加键值对，存储类型为string result = sr.set('name', 'qiangzai') # 如果添加成功返回True，否则返回False print(result) string-获取 方法get，添加键对应的值，如果键存在则返回对应的值，如果键不存在则返回None123456789from redis import StrictRedisif __name__ == '__main__': try: sr = StrictRedis() # 创建连接对象 result = sr.get('name1') # 获取键name的值 print(result) # 结果是字节类型，没有返回None except Exception as e: print(e) string-删除 方法delete，删除键及对应的值，如果删除成功则返回受影响的键数，否则则返回0123456789from redis import StrictRedisif __name__ == '__main__': try: sr = StrictRedis() result = sr.delete('name') print(result) except Exception as e: print(e) string-获取键 方法keys，根据正则表达式获取键1234567891011from redis import *if __name__=="__main__": try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取所有的键 result=sr.keys() #输出响应结果，所有的键构成⼀个列表，如果没有键则返回空列表 print(result) except Exception as e: print(e)]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Linux常用命令]]></title>
    <url>%2F2019%2F04%2F07%2F%E8%AE%B0%E5%BD%95%E4%B9%8BLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[好记性不如烂笔头，记录一下常用的Linux 命令 1) find语法 1sudo find path -name file_name 案例：查找 根目录下的mysql相关文件 1sudo find / -name mysql 2) lsof语法 1sudo lsof -i:port 案例：查找8000端口对应的进程PID号 1sudo lsof -i:8000 3) nc nc是netcat的简写，能够实现任意TCP/UDP端口的监听,nc可以作为server以TCP或UDP方式侦听指定端口 语法 1nc -l port 案例：服务器监听9000端口 1nc -l 9000 4) telnet命令 用于远程登陆连接 语法 1telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 实例:登陆IP为xx的远程主机 1telnet 192.168.36.1 5) head&amp; tail命令 查看文件内容,可以指定行数，也可以动态查看日志 语法 1head -n 数字 path 实例：查看 /etc/profile前10行内容 1head -n 10 /etc/profile 实例：查看 /etc/profile后10行内容 1tail -n 10 /etc/profile 实例：动态查看 /etc/nginx/log 1tail -f /etc/nginx/log 6) netstat netstat命令用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。netstat是在内核中访问网络及相关信息的程序，它能提供TCP连接，TCP和UDP监听，进程内存管理的相关报告。 实例:查看MySQL的运行情况 1netstat -tnulp | grep mysql 结果 1tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 846/mysqld]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之19年学习技术栈]]></title>
    <url>%2F2019%2F01%2F01%2F%E8%AE%B0%E5%BD%95%E4%B9%8B18%E5%B9%B4%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[记录一下今年的学习目标，Fighting！！！ [x] 爬虫回顾 requests scrapy 目标：基本使用 [ ] Web框架延伸 1、Django&amp;Django-rest-framework 目标：源码尝试阅读2、第三方组件集成学习（channel）3、websocket Flask 目标Flask复习 Tornado 目标：学习使用 Bottle 目标：基本使用 [ ] 数据分析 numpy pandas 目标：基本使用API语法熟悉 [ ] 机器学习 常用算法 特征工程 推荐算法 基本使用 [ ] 数据结构&amp;算法 常用的设计模式 基本算法解决思路 多了解 [ ] 新语言学习 C语言 Java语言 Go语言 目标：入门即可，了解底层语言帮助自己对Python理解 [ ] 数据库 SQL redis 反复练习 [ ] 运维 Docker K8S Supervisor Shell Fabric 基本使用]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[纪念金庸]]></title>
    <url>%2F2018%2F10%2F30%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[飞雪连天射白鹿，笑书神侠倚碧鸳。 侠之大者，为国为民。 红颜弹指老，刹那芳华，与其天涯思君，恋恋不舍，莫若相忘于江湖。——金庸《天龙八部》 四张机，鸳鸯织就欲双飞，可怜未老头先白，春波碧草，晓寒深处，相对浴红衣。——金庸《射雕英雄传》 一座山，隔不了两两相思，一天涯，断不了两两无言，且听风吟，吟不完我一生思念。——金庸《神雕侠侣》 他强由他强，清风拂山岗；他横任他横，明月照大江。——金庸《倚天屠龙记》 情不知所起，一往情深；恨不知所终，一笑而泯。——金庸《笑傲江湖》]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之常见的内置函数]]></title>
    <url>%2F2018%2F10%2F24%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Python当中有很多的内置函数，在写代码的过程当中如果能够灵活运用，可以提高开发效率,今天就总结一下～ 文章Python环境为Py3.x filter 语法123# 第一个参数：是一个函数名称# 第二个参数：是一个可迭代对象filter(function, sequence) 执行过程function函数会一个一个调用sequence里面的item参数，然后执行function(item)，最后将执行结果为True的元素组成新的迭代器返回 案例说明12345678910111213foo = [1, 2, 3, 4, 5, 6]new_foo = list(filter(lambda x: x &gt; 3, foo))print(new_foo) # [4, 5, 6]# 等同于def func(item): return item &gt; 3new_iterable = filter(func, foo)print(new_iterable) # &lt;filter object at 0x105e23748&gt;new_foo = list(new_iterable)print(new_foo) # [4, 5, 6] zip() 语法1zip(iterable1, iteable2, ...) 执行过程zip会将 所有iterable里面的 item元素按照索引顺序，一一打包成元组，最终将元组组合成列表 案例说明12345l1 = [1, 2, 3, 4]l2 = [9, 10, 11, 12]ret = list(zip(l1, l2))print(ret) # [(1, 9), (2, 10), (3, 11), (4, 12)] Map()语法1map(func, sequence) 执行过程func函数会遍历sequence里面所有的 item元素，逐个执行 func(item)，并将返回的结果组成新的 map对象返回 案例说明12345# 列表所有元素平方l1 = [1, 2, 3, 4]m = map(lambda x: x * x, l1)print(list(m)) reduce()语法12from functools import reducereduce(func, sequence) 执行过程func函数会先从 sequence中取出前2个 item元素执行 func(item1,item2),把拿到的返回结果当作下一次执行 func函数的第一个参数，然后再从 sequence中取第三个 item当执行的第二个参数，后面以此类推 案例说明123456# l1列表所有元素的乘积l1 = [1, 2, 3, 4, 5, 6]from functools import reduceret = reduce(lambda x, y: x * y, l1)print(ret) # 720]]></content>
      <categories>
        <category>Python-Standard-Library</category>
      </categories>
      <tags>
        <tag>zip</tag>
        <tag>filter</tag>
        <tag>map</tag>
        <tag>reduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于AWS服务器利用sandowsocks科学上网]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%9F%BA%E4%BA%8EAWS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%A9%E7%94%A8sandowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一直都想找Google和AWS这种大BOSS撸羊毛，但是苦于没有信用卡，也不想走淘宝买卡害怕被套路，偶然机会需要出国旅游，所以匆匆去办了一张中信的 Visa信用卡（欧洲用现金、美国用信用卡、中国在用支付宝，国外很流行信用卡），回国以后就赶紧草草注册了AWS，开启科学上网～ 一、注册AWS并创建Ec2实例1、注册地址：https://www.amazonaws.cn/sign-up/&gt; 2、注册需要条件 信用卡（必备，我用的visa双币卡，注册后扣除1$） 手机号 邮箱 备注：注册成功以后需要等待一天验证身份什么的，反正我当初等了一天才能去创建Ec2 3、申请成功，创建Ec2实例 创建实例我选择的是 Ubuntu16.40因为对ubuntu系统比较熟悉 参考链接：https://blog.csdn.net/jewely/article/details/78030057 保存好私钥文件，文件结尾是xxx.pem 二、连接服务器1、修改一下本地私钥文件权限 12chmod 400 xxx.pem# 备注 权限太高，后面ssh的时候连接不了会提示不安全 2、ssh连接 1ssh -i "xxx.pem" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com xxx.pem 就是你下载下来的私钥文件 xxx这个名字是当时下载时写的，大家按自己的来 @前面的那个ubuntu应该是固定的，我选的就是ubuntu系统 @后面的就是AWS上你创建的实例的公有DNS（IPv4）名称 或者写 IPv4的公有IP也可以 我两个都尝试了 It`s OK 3、修改一下root用户密码 1sudo passwd root 三、安装shadowsocks并配置默认ubuntu16.04已经预装了Python3 1、更新apt-get 1apt-get update 2、安装pip3（我用惯了Python3） 1apt-get install python-pip3 3、安装shadowsocks 1pip3 install shadowsocks 4、创建配置文件 1sudo touch /etc/shadowsocks.json 5、添加配置内容 1234567891011&#123; "server": "0.0.0.0", // 服务端IP 0.0.0.0即可方便连接 "server_port": 50003, // 服务器端口方便后期客户端连接 "local_address": "127.0.0.1", "local_port": 1080, "password": "******", //连接服务器的密码，自己设置，客户端连接时候要填写 "timeout": 300, "method": "aes-256-cfb", // 加密方式 "fast_open": false, "workers": 1&#125; 配置以后启动shadowsocks 1sudo ssserver -c /etc/shadowsocks.json -d start 如果遇到permission denied错误解决方法如下 1234# 第一步查看sserver的位置 which ssserver # 第二步sudo 填写完整的ssserver路径 -c /etc/shadowsocks.json -d start 6、修改AWS上EC2实例的入站端口 配置好 shaodowsocks 后，还需要将配置中的端口打开, 这样客户端的服务才能链接得上 EC2 中的 shadowsocks 服务，首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则 四、客户端连接&amp;科学上网1、打开本地shadowsocks客户端 2、填写信息 3、上网 遇到问题的可以留言区留言，一起看看 配置多用户上网]]></content>
      <categories>
        <category>Record</category>
      </categories>
      <tags>
        <tag>sandowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之shell学习]]></title>
    <url>%2F2018%2F10%2F09%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bshell%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[最近在学习 shell，开一篇文章记录一下 目的：掌握shell的基本语法，编程思想是不变的，不同的语言只是语法不一样 变量的定义 局部变量局部变量作用域只是在当前的sh文件or当前的终端内 1234# 新建var.sh#!/bin/basha=helloworldecho $a 执行 123456# 第一种方式执行bash var.sh# 第二种方式执行 ./var.sh 前提是文件有可执行权限 chmod u+x var.sh# 第三种方式执行source var.sh 输出 1helloworld 备注1：shell变量赋值 ‘=’两边不能有空格 备注2: shell变量赋的值如果不是连续的，有空格的需要使用单/双引号 a=&#39;hello world&#39;,这时候需要引号包裹‘ 备注3: shell变量赋的值如果里面有另外的变量 12345key='test'a="hello $key world" # 注意这里面一定要用双引号包裹echo a# 输出hello test world #####系统命令变量定义 先执行命令，将命令执行之后的结果存到变量当中 1234cmd=`ls`orcmd=$(ls)echo $cmd 输出 1当前目录文件效果同 `ls` 全局变量 系统所有环境都可以使用的变量 查看方法：env 定义方式 分步骤定义 定义一个本地变量 12# 变量名=变量值name=qiangzai 声明为全局变量 12# export 变量名export name 同时定义 export 变量名=变量值 1export 变量名=变量值 内置变量 bash命令内部已经定义好的变量，可以直接使用，不需要定义 使用方法 和shell脚本有关的内置变量 $0 获取当前脚本名称 $# 获取当前脚本的参数个数 $n 获取当前脚本获取到的第n个参数 $? 获取上一次命令的执行情况，0表示成功 $$ 脚本运行时使用的进程号 $@ 获取所有的参数 和字符串相关的内置变量 字符串切割 12$&#123;var_name:start:n&#125;# var_name是字符串变量名 start正数 从开头开始 n表示截取字符的个数 案例：截取字符串’abcdefg’,从c开始截取2个字符 12alpha="abcdefg"echo $&#123;alpha:3:2&#125; 和默认值相关的内置变量 第一种 1234# 获取脚本第一个参数var_name=$1# defalut表示默认值，如果没有输入参数，default将会被使用$&#123;var_name:-default&#125; 第二种 123var_name=$1# 将会无视输入参数，直接输出设定好的默认值$&#123;var_name:+default&#125; 查看变量的3种方式123456# 定义一个变量name=qinagzai# 打印一下变量echo $name echo "$name"echo "&#123;$name&#125;" # 推荐使用，最规范 删除变量&amp;设置变量只读12unset 变量名readonly 变量名 语法学习 验证表达式 方式一 1[ 表达式 ] 方式二 1test 表达式 表达式两侧必须要有空格，表达式之间需要有空格，不然表达式表示的是一个整体 案例 12345678910# 获取参数个数arg_nums=$## 方式一验证[ "$&#123;arg_nums&#125;" -eq 3 ] # 表示脚本参数不等于3# 方式二test "$&#123;arg_nums&#125;" -eq 3 # 意思同上# 打印执行结果 1表示验证通过，0表示失败echo $? 逻辑表达式 -eq //等于 -ne //不等于 -gt //大于 （greater ） -lt //小于 （less） -ge //大于等于 -le //小于等于 命令的逻辑关系： 在linux 中 命令执行状态：0 为真，其他为假 &amp;&amp;并 命令1&amp;&amp;命令2 如果命令1执行成功，则执行命令2 如果命令1执行失败，则不执行命令2 || 或 命令1||命令2 如果命令1执行成功，则不执行命令2 如果命令1执行不成功，则执行命令2 shell文件表达式 -e 判断输入的内容表示的文件是否存在 -f判断输入的内容是否是一个文件 -d判断输入的内容是否是一个目录 -x判断输入的文件是否有可执行权限 -r判断文件是否可读 -w判断文件是否可写 12# 使用方式 注意有空格[ 文件表达式 文件名 ] sehll字符串表达式 == 判断两个字符串是否相等 ！= 判断两个字符串是否不一致 -z判断字符串是否为0 -n判断字符串长度是否不为0 shell流程控制 单if语句 1234if 条件语句then 执行语句fi if else语句 123456if 条件语句1then 执行语句1else 执行语句2fi 双if语句 123456789if 条件语句1then 执行语句1elif 条件语句2then 执行语句2else 执行语句3fi case语句 1234567891011case 值 in 值1） 执行语句1 ;; 值2） 执行语句2 ;; 值3） 执行语句3 ;;esac 案例 1234567891011121314151617#!/usr/bininstruct=$1case "$&#123;instruct&#125;" in start) echo "系统启动" ;; stop) echo "系统关闭" ;; reload) echo "系统重启" ;; *) echo "启动方式是：sh $0 [start|stop|reload]"esac 循环控制语句1234for 条件do 执行语句done Shell中的函数 1234567891011121314# 无参数格式func_name()&#123; 函数体&#125;#调用func_name# 有参数格式func_name()&#123; args=$n 函数体&#125;# 调用func_name args1 args2]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之常见数据结构的区别和联系]]></title>
    <url>%2F2018%2F09%2F28%2F%E8%AE%B0%E5%BD%95%E4%B9%8B%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[介绍本文主要总结下数组,链表,队列,栈的区别和联系 联系 这四种数据结构都是线性表数据结构,常见的数据结构可以划分为:线性结构/树结构/图结构 区别 数组和链表是更加偏向数据存储方式的概念,数组有索引,在连续的空间中存储数据,随机读取的效率高,但是数据的添加删除的效率比较低;而链表可以在非连续的空间中存储数据,随机访问效率低,数据添加删除效率高. 队列和栈是描述数据存取方式的概念,队列是先进先出,而堆栈是后进先出;队列和栈都可以使用数组或者链表去实现.]]></content>
      <categories>
        <category>Record</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬虫之常见问题总结]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目的：记录爬虫中遇到的坑…… xpath提取数据遇到tbody标签 问题描述：今天在用scrapy爬取网页时，当爬取表格内容时，发现使用插件 xpath helper获取正常，程序中的xpath解析不到 问题原因：浏览器会在table标签下添加tbody标签，浏览器会对html文件进行一定的规范化，但是源码当中是没有tbody标签的。 解决办法：将分析的xpath语句中把tbody去掉即可 Scrapyd运行后被拒绝访问 问题描述：我在腾讯服务器运行scrapyd项目部署，启动以后访问6800端口被拒绝，第一反应就是腾讯云的安全组配置问题，后来发现自己的出站端口6800早已打开。翻了翻日志，看到下面这句代码 1[-] Scrapyd web console available at http://127.0.0.1:6800/ 哈哈，尴尬的127.0.0.1，判断需要修改scrapyd的配置文件 解决问题 1、找到配置文件，先找Python的第三方库 scrapyd 安装目录 我这里找了半天没找到，就是用 find大法了 sudo find / -name scrapyd 2、找到目录，下一步就是 cd到安装目录下 1cd /usr/....../scrapyd 3、找到 default_scrapyd.conf，并打开修改 1bind_address = 127.0.0.1 为 1bind_address = 0.0.0.0 然后保存退出，重试一下就ok！]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>spider_error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python的PEP文档]]></title>
    <url>%2F2018%2F07%2F09%2FPython%E7%9A%84PEP%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[What is PEP? Python的 PEP文档是一组不断发展的标准文档, PEP是 Python Enhancement Proposal首字母的缩写,就像TCP/IP协议使用RFC不断发展成为通信标准一样,PEP也是类似的用于发展Python的一组文档. 12345PEP stands for Python Enhancement Proposal. A PEP is a design document providing information to the Python community, or describing a new feature for Python or its processes or environment. The PEP should provide a concise technical specification of the feature and a rationale for the feature.We intend PEPs to be the primary mechanisms for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python. The PEP author is responsible for building consensus within the community and documenting dissenting opinions.Because the PEPs are maintained as text files in a versioned repository, their revision history is the historical record of the feature proposal [1]. 有道翻译一波 PEP代表Python增强建议。PEP是一种设计文档，它向Python社区提供信息，或者描述Python或其流程或环境的新特性。PEP应该提供特性的简明技术规范和特性的基本原理。 我们打算将PEPs作为提出主要新特性的主要机制，用于收集关于某个问题的社区输入，以及记录Python中的设计决策。PEP作者负责在社区内建立共识并记录不同意见。 因为pep是作为文本文件在版本化的存储库中维护的，所以它们的修订历史记录是特性建议[1]的历史记录。 说白了其实就是记录文档~ PEP8程序员看的比较多的是 PEP8文档,这是官方出的编程规范. PEP文档很多,说明Python的社区非常活跃,Python也就越流行~]]></content>
      <categories>
        <category>Python-Basis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录之Git使用]]></title>
    <url>%2F2018%2F06%2F05%2F%E8%AE%B0%E5%BD%95%E4%B9%8BGit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Git介绍Git是目前世界上最先进的分布式版本控制系统 方便多人协作开发 方便版本控制 Git基本流程图 工作区&amp;暂存区&amp;仓库区介绍工作区 对于添加、修改、删除文件的操作，都发生在工作区中 暂存区 暂存区指将工作区中的操作完成小阶段的存储，是版本库的一部分 仓库区 仓库区表示个人开发的一个小阶段的完成 仓库区中记录的各版本是可以查看并回退的 但是暂存区的版本一旦提交就没有了 本地Git常见操作基本配置 初始化仓库 12git init# 会在初始化的路径下产生隐藏文件夹.git 配置个人信息 1234git config user.name 'qiang-zai'git config user.email 'china_qiangzai@163.com'# 配置的个人信息在.git/config文件下可以查看# 默认不配置的话，会使用全局配置里面的用户名和邮箱；全局git配置文件路径：～/.gitconfig 查看文件状态 1git status 基本操作 将工作区文件添加到暂存区 git add . # 添加项目中的所有文件到暂存区 git add xxx.py # 添加啊指定文件到暂存区 将暂存区文件提交到仓库区 commit会生成一条版本记录 -m后面是版本描述信息 1git commit -m '版本描述' 将工作区修改的文件直接提交到仓库区 1git commit -am '版本描述' 查看历史版本 123git log git reflog# git reflog可以查看所有分支的所有操作记录（包含commit和reset的操作），包括已经删除的commit记录，git log 则不能查看已经删除的commit记录 回退版本（本地仓库&gt;工作区） 方案一:版本较少的时候推荐使用 HEAD表示当前最新版本 HEAD^表示当前最新版本的前一个版本 HEAD^^表示当前最新版本的前两个版本，以此类推。。。 HEAD~1表示当前最新版本的前一个版本 HEAD~10表示当前最新版本的前10个版本，以此类推。。。 1git reset --hard HEAD^ 方案二：版本较多的时间推荐使用 通过每个版本的版本号回退到指定版本 git reflog来查看版本号 1git reset --hard 版本号 撤销修改 只能撤销工作区、暂存区的代码，不能撤销本地仓库的代码 撤销本地仓库的代码等于版本回退操作 撤销工作区代码 1git checkout 文件名 撤销暂存区代码 1234# 第一步 将暂存区代码撤销到工作区git reset HEAD 文件名# 第二步 撤销工作区代码git checkout 文件名 对比版本库 对比版本库（本地仓库）与工作区 1git diff HEAD -- xxx.py 对比版本库 1git diff HEAD HEAD^ -- xxx.py 删除文件：删除文件分为确定删除和误删 确定删除处理 123456# 删除文件(文件已经存在本地仓库版本库当中了)rm 文件名# git确定删除文件git rm 文件名# 删除后记录删除操作版本git commit -m '删除xxx文件' 误删文件 1234# 删除文件rm 文件名# git 撤销修改git checkout -- 文件名 协同开发克隆项目 前提需要在GitHub/其他git服务器创建好项目仓库 克隆远程仓库命令 1git clone https://github.com/xxxx/xxxx.git 配置项目开发人员信息 123cd 克隆的文件夹git config user.name 'xxx'git config user.email 'xxxx@163.com' 编辑代码 推送远端 123456# 工作区添加到暂存区git add .# 暂存区提交到仓库区git commit -m '编辑代码'# 推送远程仓库git push 在push的时候需要设置账号和密码，该密码则是github的账号和密码 如果每次push都需要设置账号与密码，那么可以设置记住密码 123456# 设置记住密码（默认15分钟）：git config --global credential.helper cache# 如果想自己设置时间，可以这样做(1小时后失效)：git config credential.helper 'cache --timeout=3600'# 长期存储密码：git config --global credential.helper store 同步服务器代码 1git pull 编辑代码之前要先pull，编码结束以后再 commit，最后再 push 冲突解决 容易冲突的操作方式 多个人同时操作了同一个文件 一个人一直写不提交 修改之前不更新最新代码 提交之前不更新最新代码 擅自修改同事代码 减少冲突的操作方式 养成良好的操作习惯,先pull在修改,修改完立即commit和push 一定要确保自己正在修改的文件是最新版本的 各自开发各自的模块 如果要修改公共文件,一定要先确认有没有人正在修改 下班前一定要提交代码,上班第一件事拉取最新代码 一定不要擅自修改同事的代码 标签 当某个大版本完成后，就需要打标签 记录大版本 备份大版本代码 本地打标签 123git tag -a 标签名 -m '标签描述'# 案例git tag -a v1.0 -m 'version 1.0' 推送标签到远程仓库 123git push origin 标签名# 案例git push origin v1.0 删除本地和远程标签 1234# 删除本地标签git tag -d 标签名# 删除远程仓库标签git push origin --delete tag 标签名 分支 分支的作用 区分生产环境代码以及开发环境代码 解决线上出现的bug问题，方便代码调试 特点 项目开发中公用分支包括 master、dev 分支 master是默认分支，用于发布，当需要发布时将 dev分支合并到 master 分支 dev是用于开发的分支，开发完阶段性的代码后，需要合并到 master 分支 查看当前分支 1git branch 没有创建其他分支时，只有 master分支 切换分支 1git checkout -b dev 设置本地分支跟踪远程执行分支（将分支推送到远程） 1git push -u origin dev 在切换后的dev分支开发，修改代码 dev分支代码合并到master分支并提交 只有当dev分支合并到master分支并push到远程仓库master分支以后，其他的开发人员才能获取到最新的代码 切换master分支 1git checkout master 合并 dev分支代码到 master主分支 1git merge dev 推送合并分支到远程仓库 1git push 其他开发人员同步合并后的代码 1git pull]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Scrapy框架]]></title>
    <url>%2F2018%2F05%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Scrapy是一个高性能的爬虫框架，主要是为了提取结构化数据而编写的应用框架，我们只需要实现少量代码就可以实现数据的快速获取，框架底层使用的Twisted异步网络框架,所以爬取速度非常的快。 文章目的：能够利用scrapy爬取数据 安装1pip install scrapy 查看常用操作(检查是否安装成功) 1scrapy --help 基本使用流程 第一步：创建项目（固定步骤） 1scrapy startproject 项目名称 第二步：创建爬虫（固定步骤） 12cd 项目目录scrapy genspider 爬虫名称 目标域名 第三步：编写爬虫组件和管道（主要编写的部分） 1# 在第二步创建的爬虫文件当中编写 第四步：启动爬虫（固定步骤） 1scrapy crawl 爬虫名称 scrapy组成 运行流程图（scrapy官方公布） 各部件作用 五大组件 引擎 中央协调 爬虫组件 提取数据 or url，生成请求对象 请求调度器 存放请求对象 下载器 发送请求获取响应 管道 保存数据 三大对象 Request请求对象 Response响应对象 Item数据对象 两大中间件 下载中间件 爬虫中间件 项目目录结构介绍 spider-project项目目录 __init__.py items.py定义数据模型 middlewares.py自定义中间件 pipelines.py自定义管道，保存数据 settings.py项目配置文件 scrapy.cfg项目部署文件 spiders爬虫组件目录 xxx.py具体实现爬虫的文件 通过 scrapy genspider 爬虫名称–&gt;生成的爬虫都在这个目录下 五大组件之爬虫组件 作用：提取数据 | 提取url返回请求对象 具体编写步骤1、继承爬虫类 123456# -*- coding: utf-8 -*-# 导入模块import scrapy# 继承爬虫类class ExampleSpider(scrapy.Spider): ... 2、定义爬虫名称 1name = 'example' 3、设置允许爬取的范围 1allowed_domains = ['example.com'] 4、设置开始爬取的请求地址 1start_urls = ['https://wxample.com/xxxx'] 5、实现解析函数 ​ 1、提取数据 ​ 2、提取url，返回请求对象 12def parse(self, response): pass 响应对象response基本信息 response.status &gt;&gt; 响应状态码 response.headers &gt;&gt; 响应头 response.text&gt;&gt; 响应内容 response.request.cookies&gt;&gt;获取cookies Selector对象基本操作 extract()提取数据，提取不到会抛出下标异常 extract_first()提取第一条数据，提取不到返回None 提取&amp;构建请求对象12345# 构建请求对象request = scrapy.Request(url='xxxx')# 返回请求对象&gt;&gt;引擎&gt;&gt;请求调度（scheduler）器&gt;&gt;请求下载器yield request 参数callback指定请求对象的解析函数 参数 meta把数据可以传递给解析函数 12345yield scrapy.Request(url=detail_url, callback=self.parse_detail, meta=&#123; 'item':item &#125;) 五大组件之管道 系统管道1、提取函数中使用 yield返回函数 2、命令行导出 1scrapy crawl 爬虫名称 -o 导出文件名称 自定义管道1、在 pipelines.py编写管道 ​ 1、创建管道类 ​ 2、实现管道处理数据方法 ​ 2.1 process_item必须实现 ​ 2.2 open_spider 可选实现，当爬虫启动时调用一次 ​ 2.3 close_spider 可选实现，当爬虫结束时调用一次 12345678# pipelines.pyclass JsonDataPipeline(object): '''创建管道类''' def process_item(self, item, spider): '''实现管道处理数据方法 --&gt;yield item--&gt;引擎--&gt;触发process_item方法 ''' pass 2、在 settings.py中开启管道 1234ITEM_PIPELINEs = &#123; # 在里面配置自己定义的管道类 'xxx.pipelines.定义的管道类名称':score # score 表示数字和权重&#125; 注意点 1、数字越小越优先执行 2、多管道 item传递必须在 process_item函数中返回 item 自定义数据模型 作用：规范数据的格式 定义文件位置 items.py 123456import scrapyclass dataItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pass 使用 123456# 导入items.py里面定义的模型类from xxx.items import dataItem# 创建模型对象data = dataItem()# 给对象添加数据和字典操作一样data['name'] = 'xxxx' 模型对象转字典 1dict(模型对象) 项目配置 settings.py可以配置项目中常用信息 注意：scrapy.settings.default_settings.py才是所有的配置信息，如果在settings.py中没有配置，就是用默认的配置文件 LOG_LEVEL= WARNING 表示显示的日志级别 LOG_FILE= 自定义的日志PATH 表示日志的存储位置 使用方式在 defalut_settings.py中寻找配置，然后把配置选项在 settings.py中重新配置； scrapy shell介绍 方便开发过程中的调试 使用1scrapy shell 请求地址 常用方法 request response xpath css text fetch请求新的地址 CrawlSpider crawlSpider是一个已经实现了爬取流程的爬虫； 创建1scrapy genspider -t crawl 爬虫名称 爬虫域名 参数 -t表示采用的爬虫模版 爬取流程提取符合条件的链接—&gt;跟进符合条件的链接—&gt;提取符合条件的链接—&gt;循环 使用说明 rules所有规则、元组、存放了提取页面的规则列表 Rule对象，条件规则对象 LinkExtactor链接提取器 callback当提取完页面后的回调处理函数 follow是否跟进提取,默认是 True LinkExtractor链接提取器 allow&gt;&gt;内容符合条件规则被提取 deny&gt;&gt;内容符合条件规则被排除（优先） allow_domains&gt;&gt;符合条件的域名被提取 deny_domians&gt;&gt;符合条件的域名被排除 restrict_xpaths&gt;&gt;根据xpath提取 tags&gt;&gt;默认(‘a’,’area’)符合链接提取的标签名 attrs&gt;&gt;默认(href)，链接提取标签的属性名 restrict_css&gt;&gt;根据css样式提取器提取 strip&gt;&gt;提取后的内容去除空格 链接去重把链接地址放到 seen--&gt;set() 注意：如果url的参数位置发生变化会导致无法去重 crawlspider编写代码流程1.编写提取规则—&gt;提取链接规则 2.编写提取具体页面解析代码 scrapy中间件 1.爬虫中间件 引擎和爬虫组件交互时触发中间件 2.下载中间件 引擎和下载器交互时触发的中间件 中间件的使用流程 在 middlewares.py创建中间件类 实现所需要的拦截的函数 在 settings.py中配置开启中间件 和管道一样，配置的数字(score)越小越优先执行 下载中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_request 123456789引擎 -&gt; 下载中间件 -&gt; 下载器:param request: 请求对象:param spider: 请求来自的爬虫:return: return None 继续处理这个请求return Response 直接把响应提交给引擎 -&gt; 爬虫 return Request 直接返回引擎raise IgnoreRequest 触发 process_exception 回调函数 process_response 123456下载器 -&gt; 下载中间件 -&gt; 引擎 :param request: :param response: :param spider: :return: raise IgnoreRequest 把这请求忽略 process_exception 12345678910当下载中间件异常异常时回调 :param request: :param exception: :param spider: :return: return None 继续处理异常，向下一个中间件传递异常return a Response 停止异常链，把响应返回给引擎return a Request 停止异常链，把请求返回给引擎 爬虫中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_spider_input引擎 -&gt; 爬虫中间件 -&gt; 爬虫 参数 response 响应对象 spider 爬虫对象 process_spider_output 当爬虫提交数据或者请求给引擎时触发 process_spider_exception 当 process_spider_input 异常异常时触发 process_start_requests 当引擎向爬虫所要 start_requests 时触发 scrapyd部署 专门用于部署scrapy项目框架，scrapyd帮助我们运行scrapy代码 服务端 安装 1pip install scrapyd 启动服务 1scrapyd 远程访问 客户端远程访问6800端口 开发端 安装 1pip install scrapyd-client 配置部署 scrapy.cfg文件 123[deploy:部署名称]url = http://xxx.xxx.xxx.xxx:6800/ # 服务器的ipproject = 项目名称 上传项目到服务器 1scrapyd-deploy 部署名称 -p 项目名称 启动爬虫 1curl http://server-iP:6800/schedule.json -d project=项目名称 -d spider=爬虫名称 停止爬虫 1curl http://server-ip:6800/cancel.json -d project=项目名称 -d job=jobid scrapy redis 主要是利用scrapy+redis实现 断点续爬功能 实现分布式爬虫功能 安装1pip install scrapy-redis 断点续爬实现 现实情况中很多时候我们的一旦中断爬虫就需要重新再请求之前的url，scrapy-redis能够实现断点续爬 1、在 settings.py配置 1234567891011121314# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式REDIS_HOST = "127.0.0.1"REDIS_PORT = 6379 分布式爬虫实现流程1、在 settings.py 1234567891011121314151617181920# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器## REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式# REDIS_HOST = "目标redis服务器" # 不能编写 127.0.0.1# REDIS_PORT = 6379# 管道配置ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 400,&#125; 2、修改爬虫代码，修改集成类 CrawlSpider==&gt;RedisCrawlSpider Spider==&gt;RedisSpider 3、添加监控的 redis_key体现首个请求地址]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题之字典扁平化]]></title>
    <url>%2F2018%2F05%2F24%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E5%AD%97%E5%85%B8%E6%89%81%E5%B9%B3%E5%8C%96%2F</url>
    <content type="text"><![CDATA[实际开发中存储的数据结构很多都是树状形状，现在需要写一个功能实现从树形结构到扁平化结构的转换 123456789101112131415161718192021'''有若干种不同的树形结构，需要映射为扁平化的dict字典，需要写一个函数，完成从树形结构到扁平化结构的转换如输入in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;# 输出&#123; 'jack_English_socre': '90', 'jack_English_tearcher': 'Xu', 'jack_Math_socre': '75', 'jack_Math_tearcher': 'zhang', 'jack_height': '172' &#125;''' 题目考查的其实就是递归的应用，字典嵌套字典，这里还有用到Python的 isinstance() 方法 12345678Python 中的isinstance函数，isinstance是Python中的一个内建函数。是用来判断一个对象的变量类型和type类似。语法:isinstance(object, classinfo)&gt;&gt;&gt; isinstance(1, int)True&gt;&gt;&gt; isinstance(&#123;'age': 18&#125;, dict)True&gt;&gt;&gt; isinstance('1', int)False 答案 1234567891011121314151617181920212223in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;target = &#123;&#125; # 定义一个全局变量用来存储返回结果def dict2flat(data, targetKey=''): for k, v in data.items(): if isinstance(v, dict): dict2flat(v, targetKey + k + '_') else: target[targetKey + k] = vif __name__ == '__main__': dict2flat(in_data) from pprint import pprint pprint(target)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之for循环的运行流程]]></title>
    <url>%2F2018%2F05%2F13%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8Bfor%E5%BE%AA%E7%8E%AF%E7%9A%84%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[for循环是 Python编程当中使用最多的语句，for循环用于 迭代容器对象中的元素，Python当中的容器对象常见的有列表(list)、字符串(str)、字典(dict)、元组(tuple)、集合(set)、等 案例 作用于列表 123456&gt;&gt;&gt; for elem in [1,2,3]:... print(elem)...123 作用于元组 123456&gt;&gt;&gt; for i in ("zhang", "san", 30):... print(i)...zhangsan30 作用于字符串 123456&gt;&gt;&gt; for c in "abc":... print(c)...abc 作用于字典 12345&gt;&gt;&gt; for k in &#123;"age":10, "name":"wang"&#125;:... print(k)...agename 作用于文件 12345&gt;&gt;&gt; for line in open("requirement.txt"):... print(line, end="")...Fabric==1.12.0Markdown==2.6.7 可能有人会问，问什么这么多不同类型对象都支持for语句，还有那些类型的对象可以作用在for语句中呢？回答这个问题之前，我们先要了解for循环背后的执行原理。 for循环是对容器进行迭代的过程，什么是迭代？迭代就是从某个容器对象中逐个读取元素，直到读取不到元素为止。那么那些对象支持迭代操作？任何对象都可以吗？先随便自定义一个类试试，看行不行： 12345678910&gt;&gt;&gt; class MyRange:... def __init__(self, num):... self.num = num...&gt;&gt;&gt; for i in MyRange(10):... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyRange' object is not iterable 错误日志非常清楚地告诉我们，MyRange不是一个可迭代对象，所以它不能用于迭代，那么到底什么样的对象才能称得上是可迭代对象(iterable)呢？ 可迭代对象需要实现 __iter__方法，并返回一个迭代器对象，什么是迭代器对象？迭代器对象只需要实现 __next__方法。现在我们就来验证一下列表为什么支持迭代： 123456789101112131415&gt;&gt;&gt; x = [1,2,3]&gt;&gt;&gt; its = x.__iter__() # x有此方法，说明列表是可迭代对象&gt;&gt;&gt; its&lt;list_iterator object at 0x100f32198&gt;&gt;&gt;&gt; its.__next__() # its有此方法，说明its是迭代器1&gt;&gt;&gt; its.__next__()2&gt;&gt;&gt; its.__next__()3&gt;&gt;&gt; its.__next__()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 从试验结果来看，列表是一个可迭代对象，因为它实现了 __iter__方法，并且返回一个迭代器对象&gt;&gt; list_iterator,因为它实现了 __next__方法。我们看到它不断地调用 __next__方法，其实就是不断地迭代获取容器中的元素,直到容器中没有元素，抛出 StopIteration异常为止。 那么 for语句又是如何循环的呢？它的步骤是： 1、先判断对象是否为可迭代对象，不是的话直接报错，抛出 TypeError异常，是的话，调用 __iter__方法，返回一个迭代器对象； 2、不断地调用迭代器对象的 __next__方法,每次按序返回迭代器中的一个值； 3、迭代到最后，没有更多元素了，就抛出异常 StopIteration，这个异常 python自己会处理。 对于元组，字典，字符串也是同样的道理，弄明白了 for的执行原理之后，我们就可以实现自己的迭代器用在for循环中。前面的 MyRange报错是因为它没有实现迭代器协议里面的这两个方法，现在继续改进 12345678910111213141516class MyRange: def __init__(self, num): self.i = 0 self.num = num def __iter__(self): return self def __next__(self): if self.i &lt; self.num: i = self.i self.i += 1 return i else: # 达到某个条件时必须抛出此异常，否则会无止境地迭代下去 raise StopIteration() 因为它实现了 __next__方法，所以 MyRange本身已经是一个迭代器了，所以 __iter__返回的就是 对象本身 self.现在用在for循环中试试： 123456for i in MyRange(3): print(i)# 输出 0 1 2 有没有发现，自定义的 MyRange 功能和内建函数 range很相似。for 循环本质是不断地调用迭代器的__next__方法，直到有 StopIteration 异常为止，所以任何可迭代对象都可以作用在for循环中。]]></content>
      <categories>
        <category>Python-Advanced</category>
      </categories>
      <tags>
        <tag>for</tag>
        <tag>iterable</tag>
        <tag>iterator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之selenium]]></title>
    <url>%2F2018%2F04%2F27%2F%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%2F</url>
    <content type="text"><![CDATA[Selenium介绍 Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏。程序脚本可以通过 Selenium API 控制浏览器。 运行流程 环境搭建 安装selenium 1pip install selenium 下载对应的驱动 淘宝驱动地址：http://npm.taobao.org/mirrors/chromedriver/ 备注：一定要下载本地浏览器版本对应的驱动 基本操作 12345678910111213141516171819# 导入模块from selenium import webdriverimport time# 加载浏览器驱动，创建浏览器对象browse = webdriver.Chrome('./chromedriver')# 访问百度browse.get('https://www.baidu.com')# 定位百度搜索输入框，输入关键字“电影”browse.find_element_by_name('wd').send_keys('电影')# 点击‘百度一下’browse.find_element_by_id('su').click()time.sleep(2) # 退出窗口browse.quit() selenium驱动寻找方式 1、通过指定浏览器驱动路径 1browser = webdriver.Chrome('浏览器驱动位置') 2、通过 $PATH环境变量指定浏览器驱动 12# 通过 $PATH 寻找驱动，如果寻找不到就报错 browser = webdriver.Chrome() 添加网址：https://www.jianshu.com/p/e50a49f86070 其实就是找到Python解释器的 bin目录，把驱动放进去即可 selenium控制浏览器操作访问URL1browser.get('https://www.baidu.com/') 定位元素 find_element_by_xxx返回第一个符合条件 WebElement find_elements_by_xxx返回符合条件的 WebElement列表 xxx说明 find_elements_by_class_name通过标签的class属性定位元素 find_element_by_id通过标签的ID属性定位元素 find_element_by_name通过标签的name属性定位 find_element_by_css_selectorcss样式选择 find_element_by_link_text通过链接内容查找 find_element_by_partial_link_text 通过链接内容包含的内容查找，模糊查询 find_element_by_xpath 通过xpath查找数据 xpath只能获取webelement对象，不能直接获取属性和文本内容 获取元素属性和文本内容1234# 获取属性WebElement.get_attribute('属性名')# 获取文本内容WebElement.text 输入框Input输入内容1Input_element.send_keys('xxx') 点击1element.click() 使用无界面浏览器 使用无界面的驱动 下载地址：http://phantomjs.org/download.html 使用方式 browser = webdriver.PhantomJS(&#39;驱动路径‘) 设置chrome启动参数 123456789101112131415from selenium import webdriver# 创建浏览器启动参数options = webdriver.ChromeOptions()options.add_argument('--headless') # 无界面options.add_argument('--disable-gpu') # 禁用gpu# 创建浏览器对象browser = webdriver.Chrome(chrome_options=options)# 访问网址browser.get('https://www.baidu.com')# 截图证明browser.save_screenshot('百度截图.png')# 退出browser.quit() 设置User-Agent和Proxy代理123456options = webdriver.ChromeOptions()# 切换User-Agentoptions.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1')# 设置代理options.add_argument('--proxy-server=代理服务器地址') # 设置代理browser = webdriver.Chrome('chrome_options=options) 获取网页源码1browser.page_source 注意：获取的网页源码是经过JS页面执行后的结果源码 cookies 操作（非常重要） 获取所有Cookies 1browser.get_cookies() 通过名字获取Cookie 1browser.get_cookie() 添加Cookie 1browser.add_cookie() 通过名字删除Cookie 1browser.delete_cookie() 删除所有Cookie 1browser.delete_all_cookies() 执行JS代码1browser.execute_script("js-code") 等待加载 方式一：强制等待，浪费时间 1time.sleep(秒数) 方式二：隐性等待 1browser.implicitly_wait(等待时间) 方式三：显性等待，每个元素都可以自己定义检查条件 手动编写方式 123456789101112131415# 显性等待-手动编写t = time.time()# 定义超时时间timeout = 60while True: try: # 超时时间间隔 time.sleep(0.1) url_element = browser.find_element_by_class_name("favurl") break except: # 超时处理 if time.time() - t &gt; timeout: break pass 系统提供显性等待API 1234567891011121314151617# 导入显性等待的API需要的模块# 1&gt; 等待对象模块from selenium.webdriver.support.wait import WebDriverWait# 2&gt; 导入等待条件模块from selenium.webdriver.support import expected_conditions as EC# 3&gt; 导入查询元素模块from selenium.webdriver.common.by import By# 使用selenium api 实现显性等待# 1&gt; 创建等待对象# 参数一 浏览器对象# 参数二 超时时间# 参数三 检查元素时间间隔wait = WebDriverWait(browser,60,0.1)# presence_of_element_located 检查元素是否存在，参数是一个元祖，元祖内部描述等待元素查询方案# visibility_of_element_located 检查元素是否可见url_element= wait.until(EC.presence_of_element_located((By.CLASS_NAME,"favurl"))) 窗口切换 获取所有的窗口（列表） 1browser.window_handles 切换窗口 1browser.switch_to_window(all_windowsp['index']) 备注：browser.title可以通过打印当前窗口title调试 iframe切换 1、获取iframe标签对象 1iframe_element = browser.find_element_by_xxx('') 2、切换到iframe窗口 1browser.switch_to.frame(iframe_element) 3、切换到主窗口 1browser.switch_to.default_content()]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv和json之间的转换]]></title>
    <url>%2F2018%2F04%2F26%2Fcsv%E5%92%8Cjson%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[CSV（逗号分隔值文件格式），是一种通用的，相对简单的文件格式，被用户、商业、科学广泛使用，最广泛的应用是在程序之间转移表格数据。 文章目标：了解CSV的基本操作以及json格式数据如何写入CSV文件并转换Excel 使用场景爬虫拿到的大部分都是json格式数据，但是客户或者数据分析员一般更需要Excel这种格式的数据 基本操作 1、导入模块 12# python3.X内置模块不需要下载import csv 2、创建csv写入对象 12345# 创建文件对象f = open('03-test.csv', 'w', encoding='utf-8')# 创建csv写入对象csv_writer = csv.writer(f) 3、写入数据 123csv_writer.writerow(['姓名', '年龄', '性别'])csv_writer.writerow(['强仔', '18', '男'])csv_writer.writerow(['彤彤', '17', '女']) 4、关闭文件 1f.close() json2csv 1、先转换json格式为python数据类型 12345import csvimport json# data.json 为事先准备好的json数据with open('data.json', 'r') as f: data_list = json.load(f) # 转换成pyton数据类型 2、创建csv对象 12f = open('data.csv', 'w', encoding='utf-8')csv_writer = csv.writer(f) 3、写入数据 1234567# 3.1 写入标题也就是字典的关键字keycsv_writer.writerow(data_list[0].keys())# 3.2 遍历写入字典元素valuesfor data in data_list: csv_writer.writerow(data.values())# 3.3 关闭文件句柄f.close() csv转Execl见链接：https://jingyan.baidu.com/article/3c343ff7faa59b0d3779633e.html]]></content>
      <categories>
        <category>Python-Standard-Library</category>
      </categories>
      <tags>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：求列表当中最大的三个元素]]></title>
    <url>%2F2018%2F04%2F15%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98-%E4%BA%8C-%EF%BC%9A%E6%B1%82%E5%88%97%E8%A1%A8%E5%BD%93%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[在牛客网https://www.nowcoder.com/上刷题遇到如何从list中取得最大的三个值：自己写的方法复杂度太高，放上大牛的方法，复杂度很低。看了好几遍才体会到大概的精髓。 12345678910111213141516171819202122232425262728'''从list中取出最大的三个值__author__:无名'''def FindList3MaxNum(foo): max1, max2, max3 = None, None, None for num in foo: if max1 is None or max1 &lt; num: max1, num = num, max1 if num is None: continue if max2 is None or num &gt; max2: max2, num = num, max2 if num is None: continue if max3 is None or num &gt; max3: max3 = num return max1, max2, max3if __name__ == '__main__': foo = [78, 23, 10, 56, 4, 103, 89, 14] max1, max2, max3 = FindList3MaxNum(foo) print(max1, max2, max3)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之list常见用法]]></title>
    <url>%2F2018%2F04%2F11%2Fpython%E7%BC%96%E7%A8%8B%E4%B9%8Blist%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[列表是Python最常用的数据类型之一，本文整理一下列表最常用的10种操作，如果开发过程当中遇到了，可以尝试从这些找答案 1、迭代列表时如何访问列表下标索引普通版：12345678items = [8, 23, 45]for index in range(len(items)): print(index, "--&gt;", items[index])&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 Level Up版：1234567for index, item in enumerate(items): print(index, "--&gt;", item)&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 enumerate还可以指定元素的第一个元素从第几个索引开始，默认是0，也可以指定从1开始： 1234567for index, item in enumerate(items, start=1): print(index, "--&gt;", item)&gt;&gt;&gt;1 --&gt; 82 --&gt; 233 --&gt; 45 2、append与extend方法有什么区别append表示把某个数据当做新元素追加到列表的最后面，它的参数可以是任意对象 1234567x = [1, 2, 3]y = [4, 5]x.append(y)print(x)&gt;&gt;&gt;[1, 2, 3, [4, 5]] extend 的参数必须是一个可迭代对象，表示把该对象里面的所有元素逐个地追加到列表的后面 1234567891011x = [1, 2, 3]y = [4, 5]x.extend(y)print(x)&gt;&gt;&gt;[1, 2, 3, 4, 5]# 等价于：for i in y: x.append(i) 3、检查列表是否为空普通版：1234567if len(items) == 0: print("空列表")或者if items == []: print("空列表") Level Up版：12if not items: print("空列表") 4、如何理解切片 切片用于获取列表中指定范的子集，语法非常简单 1items[start:end:step] 从 start 到 end-1 位置之间的元素。step 表示步长，默认为1，表示连续获取，如果 step 为 2 就表示每隔一个元素获取。 12345678910111213141516a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[3:8] # 第3到第8位置之间的元素[4, 5, 6, 7, 8]&gt;&gt;&gt; a[3:8:2] # 第3到第8位置之间的元素，每隔一个元素获取[4, 6, 8]&gt;&gt;&gt; a[:5] # 省略start表示从第0个元素开始[1, 2, 3, 4, 5]&gt;&gt;&gt; a[3:] # 省略end表示到最后一个元素[4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[::] # 都省略相当于拷贝一个列表，这种拷贝属于浅拷贝[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 5、如何拷贝一个列表对象第一种方法：1new_list = old_list[:] 第二种方法：1new_list = list(old_list) 第三种方法：12345import copy# 浅拷贝new_list = copy.copy(old_list)# 深拷贝new_list = copy.deepcopy(old_list) 扩展，了解即可 6、如何获取列表中的最后一个元素 索引列表中的元素不仅支持正数还支持负数，正数表示从列表的左边开始索引，负数表示从列表的右边开始索引，获取最后一个元素有两种方法。 12345&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[len(a)-1]10&gt;&gt;&gt; a[-1]10 7、如何对列表进行排序 列表排序有两种方式，一种是列表自带的方式 sort，一种是内建函数 sorted。复杂的数据类型可通过指定 key参数进行排序。由字典构成的列表，根据字典元素中的age字段进行排序： 12345678910items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]items.sort(key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'age': 10, 'name': 'Bart'&#125;, &#123;'age': 20, 'name': 'cater'&#125;, &#123;'age': 39, 'name': 'Homer'&#125;] 列表有 sort方法，用于对原列表进行重新排序，指定 key 参数，key 是匿名函数，item 是列表中的字典元素，我们根据字典中的age进行排序，默认是按升序排列，指定 reverse=True 按降序排列 1234items.sort(key=lambda item: item.get("age"), reverse=True)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Bart', 'age': 10&#125;] 如果不希望改变原列表，而是生成一个新的有序列表对象，那么可以内置函数 sorted ，该函数返回新列表 12345678910111213items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]new_items = sorted(items, key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;]print(new_items)&gt;&gt;&gt;[&#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Homer', 'age': 39&#125;] 8、如何移除列表中的元素删除列表中的元素有三种方式 remove 移除某个元素，而且只能移除第一次出现的元素 12345678910&gt;&gt;&gt; a = [0, 2, 2, 3]&gt;&gt;&gt; a.remove(2)&gt;&gt;&gt; a[0, 2, 3]# 如果要移除的元素不在列表中，则抛出 ValueError 异常&gt;&gt;&gt; a.remove(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: list.remove(x): x not in list· del根据指定的索引移除元素 12345678910&gt;&gt;&gt; a = [3, 2, 2, 1]# 移除第一个元素&gt;&gt;&gt; del a[1][3, 2, 1]# 当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; del a[7]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list assignment index out of range pop与del类似，但是pop方法可以返回移除的元素 1234567891011&gt;&gt;&gt; a = [4, 3, 5]&gt;&gt;&gt; a.pop(1)3&gt;&gt;&gt; a[4, 5]# 同样，当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; a.pop(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: pop index out of range 9、如何连接两个列表12345678listone = [1, 2, 3]listtwo = [4, 5, 6]mergedlist = listone + listtwoprint(mergelist)&gt;&gt;&gt;[1, 2, 3, 4, 5, 6] 列表实现了 + 的运算符重载，使得 + 不仅支持数值相加，还支持两个列表相加，只要你实现了 对象的 __add__操作，任何对象都可以实现 + 操作，例如： 123456789101112131415161718192021class User(object): def __init__(self, age): self.age = age def __repr__(self): return 'User(%d)' % self.age def __add__(self, other): age = self.age + other.age return User(age)user_a = User(10)user_b = User(20)c = user_a + user_bprint(c)&gt;&gt;&gt;User(30) 10、如何随机获取列表中的某个元素123456789import randomitems = [8, 23, 45, 12, 78]&gt;&gt;&gt; random.choice(items)78&gt;&gt;&gt; random.choice(items)45&gt;&gt;&gt; random.choice(items)12]]></content>
      <categories>
        <category>Python-Basis</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具之Charles使用问题记录]]></title>
    <url>%2F2018%2F04%2F07%2F%E5%B7%A5%E5%85%B7%E4%B9%8BCharles%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近研究爬虫需要进行HTTP抓包，由于 Fiddler没有Mac版本，选择使用Charles（青花瓷）来抓包，文章 记录一下 Charles的使用和使用过程中遇到的问题 目标：Charles抓包&amp;问题记录 安装 安装包百度云链接：链接:https://pan.baidu.com/s/1QYaEVwgKSH8znwORotBOvg 密码:s7jz 安装步骤1、点击dmg安装—&gt;应用程序 2、打开charles应用程序—&gt;显示包信息—&gt;替换其中的 charles.jar包 基本使用 这个我就不过多介绍网上教程一大堆，需要注意的是如果要抓 https需要安装证书，并使证书可信任 问题记录 安装证书以后还是无法抓HTTPS的包，显示unknown?解决步骤 1、点击Proxy—&gt;SSL Proxying settings—&gt;SSL Proxying—&gt;Add—&gt; HOST和PORT都填写 *即可 2、重启 charles 安装Charles 抓包信息当中以后不显示 request和response解决步骤 1、点击Preferences—&gt;Viewers—&gt;把 Combine request and response 取消勾选—&gt;点击OK即可 2、重启charles 抓包手机配置链接：https://blog.csdn.net/pansanday/article/details/80347632]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Charles</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之数据提取]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[今天主要记录一下数据提取的几种方法，在爬虫获取到响应数据以后我们改怎么处理响应的数据呢？从网上拿到的响应数据各种类型都有：JSON类型的，XML格式的,html格式的等等。 常用的数据提取方式 jsonpath re xpath beautifulsoup 一、jsonpath基本使用 json是什么JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。 json模块常用操作 json.loads(json_str) JSON字符串—&gt;转换成Python数据类型 json.dumps(dict_data) Python数据类型—&gt;JSON字符串 json.load(f) json文件—&gt;获取Python数据类型 json.dump(dict_data, f) Python数据类型—&gt;写入json文件 ensure_ascii=False 显示中文 indent=数字表示空格 Jsonpath是什么？ jsonpath是一种语法规则快速从JSON数据当中提取数据 在线调试网址：https://jsonpath.com/ 在线解析工具：https://www.json.cn/# 语法规则 语法 描述 案例 $ 根节点 @ 现行节点 . 取子节点 $.store.book .. 取子孙节点 $..book [] 设置筛选条件 $..book[0] [,] 支持多选选择内容 $..book[1,3] () 支持表达式计算 $..book[(@.length - 1)] ?() 支持过滤操作 $..book[?(@.price&lt;10)] Python中怎么使用123import jsonpathdata = jsonpath('字典类型数据', 'jsonpath提取语法')# data是list类型 如果提取不出来返回False 二、re基本使用 用事先定义好的一些特定字符，及这些特定字符的组合，组成一个规则字符串，这个规则字符串用来表达对字符串的一种过滤逻辑 基本语法 预定义字符集 \d表示数字：[0-9] \D表示非数字：[^\d] \s表示空白字符 \S非空白字符 \w单词字符：[A-Z,a-z,0-9] \W非单词字符：[^\w] .匹配任意除换行符\n外的字符，备注：在DoTALL模式中也能匹配换行符 \转义字符，使后一个字符表示字符本身 []表示字符的选取范围 数量词 *表示任意个 +表示至少一个 ?表示至多一个（1个或者没有） {}表示范围区间 {1} 表示匹配一个字符 {1,6}表示匹配1到6个字符 {,6}表示最多匹配6个字符 贪婪模式和非贪婪模式 贪婪模式(.*)尽可能多的匹配 非贪婪模式(.*?)一旦匹配到就结束 DOTALL模式 让.匹配\n 1re.RegexFlag.DOTALL、re.RegexFlag.S、re.S、re.DOTALL 忽略大小写 1re.RegexFlag.I re.RegexFlag.IGNORECASE re.I re.IGNORECASE 支持DOTALL 和忽略大小写 用| 原始字符串使用r不用\ 正则前面使用r正则语法 四种匹配检索方法 match match 从头开始匹配，仅匹配一次 search search 全局匹配，仅匹配一次 findall findall 获取符合条件的所有数据，返回列表 finditer finditer 获取符合条件的所有数据，返回迭代器对象 数据量大的时候推荐使用，提高内存使用效率 分组&amp;替换 sub split 通用分组格式：[分隔符]+ 1234567import redata = 's,fqw;fwe, fds;fsda,afds;'pattern = re.compile(r'[, ;]+')ret = pattern.split(data)print(ret) 输出 1['s', 'fqw', 'fwe', 'fds', 'fsda', 'afds', ''] sub 12345678import redata = 's,fqw;fwe,fds;fsda,afds;'# 把, ; 都替换成 #pattern = re.compile(r'[,;]+')ret = pattern.sub('#', data)print(ret) 输出 1s#fqw#fwe#fds#fsda#afds# 三、Xpath基本使用 xpath是一门在HTML/XML文本中查找信息的语言，可以用在HTML/XML文档中对元素和属性进行遍历，简单来说就类似于正则通过一定的语法规则从文本中提取数据 基本语法 节点选择 /表示从根节点选取 //从匹配选择的当前节点选择文档中的节点 .表示选取当前节点 ..选取当前节点的父节点 选取属性 @ //节点/@属性名称 文本选择 text()选取文本 /text() 高级语法-筛选条件 //node[筛选条件] 通过下标（数字）筛选（下标从1开始） /node[1] 通过属性筛选 /node[@属性] /node[@属性=值] 通过文本内容筛选 //node[text()=&#39;筛选的值&#39;] 通过内置函数 last() position() containes包含内容 contains(text(), &#39;文本&#39;) contains(@属性， ‘属性值’) starts-with 从头匹配内容 语法和containes一样 通过子节点来筛选 /node[node_子节点&gt;子节点的值] 通配符 *匹配任何元素节点 //*[text()=&#39;某值&#39;] //*[@category=’某value’] @*匹配node任意属性的value //node[@*=&#39;某value&#39;] 多个条件同时满足 使用 | lxml模块 Python lxml模块去使用xpath语法 安装1pip install lxml lxml基本使用1、导入模块 1from lxml import etree 2、构建根元素对象 1eroot = etree.HTML(html_data) # eroot 是一个element对象 3、操作根元素对象提取数据 1eroot.xpath('xpath语法规则') 4、打印调试查看element对象内容 1etree.tostring(eroot).decode('utf-8') 四、beautifulsoup基本使用 介绍 Beautifulsoup也是一个HTML/XMl的解析器，跟上面的 lxml一样 优点 用来解析HTML比较简单，API非常人性化，支持CSS选择器 缺点 lxml只会遍历局部，而Beautifusoup是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml 安装1pip install beautifulsoup4 官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0 基本使用1、导入模块 1from bs4 import BeautifulSoup 2、创建beautifulsoup对象 12soup = BeautifulSoup(html,'lxml')soup = BeautifulSoup(open('文件')) 3、操作node 获取元素标签内容 1soup.标签名称 获取子元素 1234# 返回列表soup.node_name.contents# 返回迭代器soup.node_name.children 获取元素内容 1soup.node_name.get_text() 获取元素属性值 1soup.a.get('href') 注意如果获取元素的属性是 class返回的是列表 find 和find_all介绍 find返回符合条件的第一个元素 find_all返回符合条件的所有元素列表 备注：find和find_all提供参数都一样，下面以find_all()来展示 1、通过标签和标签列表查找元素 12soup.find_all('a') # 查询所有的a标签soup.find_all(['a', 'b']) # 查询所有的a标签和b标签 2、通过正则表达式查找元素 12# 以b开头的标签查找soup.find_all(re.compile('^b')) 3、通过属性来查找 12345soup.find_all(&#123; attrs=&#123; '属性名'：'值' &#125;&#125;) 4、通过文本内容查找元素 1soup.find_all(text='查询的文本内容') 6、混合使用 1234567soup.find_all( '标签名', attrs=&#123; '属性名'：'值' &#125;， text='内容') CSS样式选择器 返回值是一个list 类选择器 使用 .开头 1soup.select('.class_name') ID选择器，使用#开头 1soup.select('#id_name') 标签选择器，使用标签 名称 1soup.select('p_name') 属性选择器 1soup.select('p[属性=值]') 层级选择器 div p 选择 &lt;div&gt; 元素内部的所有 &lt;p&gt; 元素。 div , p 选择所有 &lt;div&gt; 元素和所有 &lt;p&gt; 元素。 div &gt; p 选择父元素为 &lt;div&gt; 元素的所有 &lt;p&gt; 元素。 .cls1.cls2 选择类名是cls1 并且 类名是cls2]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>jsonpath</tag>
        <tag>bs4</tag>
        <tag>xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之js逆向百度翻译]]></title>
    <url>%2F2018%2F03%2F22%2F%E7%88%AC%E8%99%AB%E4%B9%8Bjs%E9%80%86%E8%A1%8C%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[爬虫小试身手之js逆向百度翻译，目前百度翻译在PC和移动端都做了反扒措施，今天我们一看究竟～ 什么是js逆向？所谓的js逆向其实就是破解对方反扒的一种方式，目前很多网站反扒都使用js来做，百度翻译的反扒主要是利用了请求参数中的 sign这个签名来做的，下面就来详细的讲解 sign怎么变动了？当你更换输入框输入的翻译目标时，对比请求携带的参数发现sign这个参数随着输入的单词所变动，判断应该是js代码在请求前生成了这个sign，需要去定位具体哪个JS文件的生成了这个sign 根据上图我们可以看出sign参数是由js代码 m(a)来生成的，a参数是as也就是我们要翻译的目标，这个是我们可以拿到的，现在要做的就是如何执行m()这段js代码，Python被称为胶水语言，在Python当中调用JS代码自然也是可以实现的，Python调用js代码目前主流的两个模块 pyexecjs 、js2py，今天我们讲的主要是通过 js2py来实现。 下载模块1pip install js2py 基本使用 在python当中执行js 12345678# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在Python当中执行js代码content.execute("console.log('abc')") 输出1'abc' 在JS代码中调用Python程序 123456789101112131415161718# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在js当中调用Python程序content.a = 1content.b = 'abc'content.c = [1, 3, 4]content.d = &#123; "name": 'qwe'&#125;content.execute('console.log(a)')content.execute('console.log(b)')content.execute('console.log(c)')content.execute('console.log(d)') 输出1234'1''abc'[1, 3, 4]&#123;'name': 'qwe'&#125; 实战既然Pythoncode中可以调用js我们只要找到这段js在代码中执行就解决了这个问题，上代码 1234567891011121314151617181920212223242526272829303132333435363738394041# Python代码# 1.导入模块import requestsimport js2py# 2.发送请求，获取响应内容# 请求地址 请求头 请求方式 请求参数url = 'https://fanyi.baidu.com/v2transapi'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36', 'Referer': 'https://fanyi.baidu.com/?aldtype=16047', 'Cookie': 'BAIDUID=DAD75821AD7D4663D00D9C0E7F9C8011:FG=1; BIDUPSID=DAD75821AD7D4663D00D9C0E7F9C8011; PSTM=1553134744; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1439_21121_18559_28723_28557_28697_28585_28518_28627_22157; PSINO=2; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553218276,1553218292; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553218292'&#125;# 需要翻译的内容kw = 'thank'# 构建上下文对象content = js2py.EvalJs()with open('test.js', 'r') as f: content.execute(f.read()) sign = content.e(kw)data = &#123; "from": "en", "to": "zh", "query": kw, "transtype": "translang", "simple_means_flag": "3", "sign": sign, "token": "88d322490309c4abee0496f0dbab1a4b",&#125;response = requests.post( url=url, headers=headers, data=data)print(response.json()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445// test.js 文件var i = '320305.131321201';function n(r, o) &#123; for (var t = 0; t &lt; o.length - 2; t += 3) &#123; var a = o.charAt(t + 2); a = a &gt;= "a" ? a.charCodeAt(0) - 87 : Number(a), a = "+" === o.charAt(t + 1) ? r &gt;&gt;&gt; a : r &lt;&lt; a, r = "+" === o.charAt(t) ? r + a &amp; 4294967295 : r ^ a &#125; return r &#125;function e(r) &#123; var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g); if (null === o) &#123; var t = r.length; t &gt; 30 &amp;&amp; (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10)) &#125; else &#123; for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h &gt; C; C++) "" !== e[C] &amp;&amp; f.push.apply(f, a(e[C].split(""))), C !== h - 1 &amp;&amp; f.push(o[C]); var g = f.length; g &gt; 30 &amp;&amp; (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join("")) &#125; var u = void 0 , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107); u = null !== i ? i : (i = window[l] || "") || ""; for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v &lt; r.length; v++) &#123; var A = r.charCodeAt(v); 128 &gt; A ? S[c++] = A : (2048 &gt; A ? S[c++] = A &gt;&gt; 6 | 192 : (55296 === (64512 &amp; A) &amp;&amp; v + 1 &lt; r.length &amp;&amp; 56320 === (64512 &amp; r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 &amp; A) &lt;&lt; 10) + (1023 &amp; r.charCodeAt(++v)), S[c++] = A &gt;&gt; 18 | 240, S[c++] = A &gt;&gt; 12 &amp; 63 | 128) : S[c++] = A &gt;&gt; 12 | 224, S[c++] = A &gt;&gt; 6 &amp; 63 | 128), S[c++] = 63 &amp; A | 128) &#125; for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b &lt; S.length; b++) p += S[b], p = n(p, F); return p = n(p, D), p ^= s, 0 &gt; p &amp;&amp; (p = (2147483647 &amp; p) + 2147483648), p %= 1e6, p.toString() + "." + (p ^ m) &#125;]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>js2py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Http协议]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%88%AC%E8%99%AB%E4%B9%8BHttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[什么是HTTP协议？Http协议专业称之为：超文本传输协议，属于网络协议中应用层的协议，默认端口是80 HTTP请求格式 案例 HTTP请求方式HTTP请求可以使用多种请求方法 HTTP1.0定义了三种请求方法：GET, POST, HEAD HTTP1.1（主流）新增了五种请求方法：OPTIONS，PUT，DELETE，TRACE，CONNECT 请求方式 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 常见请求头 Cookie User-Agent 浏览器代理 Referer 防盗链，请求来自哪里 Host 请求主机 Connection Accept HTTP响应Http响应由四部分组成 状态行： HTTP/1.1 200 OK 消息报文： Content-Type: text/html….. 空行就是： 消息报文和正文中间的空行 响应正文：大家在浏览器看到的渲染后的内容 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开篇]]></title>
    <url>%2F2018%2F03%2F20%2F%E7%88%AC%E8%99%AB%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[当今是一个大数据的时代，数据也变得越来越值钱，有市场就有需要，“爬虫工程师”就此诞生～从此开始学习 Spider，开贴记录一下自己的学习经历, Fighting 什么是爬虫？所谓的爬虫其实就是模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动的去获取网络上的信息 爬虫分为哪些从网上简单的了解，爬虫主要分为通用爬虫（搜索引擎爬虫，典型的就是百度&amp;Google）和聚焦爬虫（爬一些指定的网站） 简单的爬虫流程 1、抓到起始的url，获取响应 2、对响应再次发送请求 3、如果能从响应中提取URL，则继续发送请求获取响应 4、如果提取数据，则将数据进行保存]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：Python操作Excel]]></title>
    <url>%2F2018%2F03%2F19%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9APython%E6%93%8D%E4%BD%9CExcel%2F</url>
    <content type="text"><![CDATA[前几天帮朋友做了一道面试题，感觉用Python做起来会方便很多，话不多说上图 一、模块介绍Python社区的强大和活跃，也为Python成为主流语言奠定了基础，丰富的第三方库使一切变成了可能，主要使用Python的两个模块 xlrd和xlwt 模块，顾名思义xlrd是读excel，xlwt是用来写excel的库，另外这两个库不是Python内置的模块需要自行安装。建议使用 pip 来进行安装 12# -i 可以指定下载的源地址，默认是使用的国外的源比较慢pip install xlrd -i https://pypi.douban.com/simple/ 二、解决题目的思路 1、要获取excel表格里面的数据 2、获取到每一行的数据，然后拼接成字典 3、把每一行拼接的字典放到列表当中 看着是不是很容易，但是如果不熟悉xlrd的语法操作起来也是很困难的，下面就罗列一下基本使用 获取Excel文件对象1data = xlrd.open_workbook(filename)#文件名以及路径，如果路径或者文件名有中文给前面加一个r原生字符。 常用的操作 1、获取data中某一个工作表（sheet） 12345table = data.sheets()[0] # 通过索引顺序获取table = data.sheet_by_index(sheet_indx)) #通过索引顺序获取table = data.sheet_by_name(sheet_name)#通过名称获取 2、行的操作 1234567891011nrows = table.nrows # 获取该sheet中的有效行数table.row(rowx) #返回由该行中所有的单元格对象组成的列表table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表table.row_len(rowx) #返回该列的有效单元格长度 3、列的操作 123456789ncols = table.ncols #获取列表的有效列数table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 4、单元格的操作 1234567table.cell(rowx,colx) #返回单元格对象table.cell_type(rowx,colx) #返回单元格中的数据类型table.cell_value(rowx,colx) #返回单元格中的数据table.cell_xf_index(rowx, colx) # 暂时还没有搞懂 三、代码奉上 ——&gt;So simple1234567891011121314151617181920212223242526272829# 导入模块import xlrd def Execl2List(): # 1.获取表格 excel = xlrd.open_workbook(r"./lianxi.xlsx") # 2.获取Sheet1表格页 sheet = excel.sheet_by_name("Sheet1") # 3.获取当前页的行数 row = sheet.nrows lists = [] for i in range(1, row): dic = &#123;&#125; list = sheet.row_values(i) # 获取表格对应行的数据 dic["姓名"] = list[0] dic["年龄"] = list[1] dic["性别"] = list[2] lists.append(dic) return listsif __name__ == '__main__': data = Execl2List() print(data)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程池]]></title>
    <url>%2F2018%2F01%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[作用 计算机每次创建线程和销毁线程需要额外占用资源，频繁创建和销毁线程会导致计算机性能下降 处理过程 优点 重复利用线程，减少因为创建线程和销毁线程带来不必要的性能销毁 让程序更加的稳定，不会因为同一时间创建线程过多而导致内存不够使程序引发系统一系列的问题 代码实现流程 1、导入线程池模块 1from multiprocessing.dummy import Pool 2、创建线程池 1pools = Pool(5) 3、定义线程池要执行的任务 12def exec_task(): print('---要执行的任务---') 4、任务完成的回调函数 12def exec_task_finish(self, result): print('执行任务完成回调函数') 注意回调函数必须要有result参数，result参数表示执行任务代码的返回值 5、线程池执行任务 1pools.apply_async(self.exec_task,callback=self.exec_task_finish) 扩展进程池 12# 导入进程池库from multiprocessing import Pool 协程池 12345# 打补丁import gevent.monkeygevent.monkey.patch_all()# 导入协程池库from gevent.pool import Pool 备注，默认线程池里面的线程是守护线程，进程池&amp;协程池同理]]></content>
      <categories>
        <category>Python-Concurrent</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之字典常用方法]]></title>
    <url>%2F2017%2F12%2F24%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%AD%97%E5%85%B8%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[字典是Python内置的数据类型，也是最常见的数据结构。本文记录一下字典使用的时候经常会遇到的API语法。 字典取值1、通过key取value12345678data = &#123;'age': 18, 'name': 'qiangzai'&#125;# 通过key取valuev1 = data['age']print(v1) # 18# 如果key不存在报错KeyErrordata['gender'] # KeyError: 'gender' 2、通过get方式12345678910data = &#123;'age': 18, 'name': 'qiangzai'&#125;v1 = data.get('age')print(v1) # 18v2 = data.get('gender')print(v2) # Nonev3 = data.get('gender', '男')print(v3) # 男 总结：通过get取值不会报错，如果不存在可以指定默认值，不指定默认值为 None 3、通过setdefault方式123456789101112data = &#123;'age': 18, 'name': 'qiangzai'&#125;v1 = data.setdefault('age')print(v1) # 18v2 = data.setdefault('gender')print(v2) # Noneprint(data) # &#123;'age': 18, 'name': 'qiangzai', 'gender': None&#125;v3 = data.setdefault('score', 100)print(v3) # 100print(data) # &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125; 总结：通过sedefault取值也不会报错，如果不存在会返回默认值并给字典添加键值对 删除字典元素1、del123456789data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;# 通过key删除对应的键值对del data['gender']print(data) # &#123;'age': 18, 'name': 'qiangzai', 'score': 100&#125;# 删除整个字典del dataprint(data) # NameError: name 'data' is not defined 总结：del 可以删除指定的键值对,如果key不存在会报错 KeyError，可以删除整个字典 2、clear1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data.clear()print(data) # &#123;&#125; 总结：clear清空整个字典 3、pop123data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;score = data.pop('score')print(score) # 100 总结：pop通过key删除value并返回删除的value值，可以用变量接收 修改字典1、通过key修改1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data['age'] = 20print(data) # &#123;'age': 20, 'name': 'qiangzai', 'gender': None, 'score': 100&#125; 2、update1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data.update(&#123;'name': 'LQ', 'class': 'Python'&#125;)print(data) # &#123;'age': 20, 'name': 'LQ', 'gender': None, 'score': 100, 'class': 'Python'&#125; 总结：update会合并字典到data，如果data中的key存在就修改key对应的value 字典的一些内置方法12345678# 返回一个字典的浅复制data.copy() # 创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值data.fromkeys(['age', 'name', 'gender'])# 以列表返回可遍历的(键, 值) 元组数组data.items()# 以列表返回一个字典所有的键data.keys()]]></content>
      <categories>
        <category>Python-Basis</category>
      </categories>
      <tags>
        <tag>dict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之队列Queue]]></title>
    <url>%2F2017%2F11%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E9%98%9F%E5%88%97Queue%2F</url>
    <content type="text"><![CDATA[Queue Queue是Python标准库中的线程安全的队列(FIFO)实现，提供了一个适用于多线程编程的先进先出的数据结构，即队列，用来在生产者和消费者之间的信息传递。 二种队列形式 FIFO队列FIFO即First in First Out，先进先出。Queue提供了一个基本的FIFO容器，使用方法很简单。 参数：maxsize是一个整数，指明了队列中能存放的数据个数的上限。一旦达到上限，插入会导致堵塞，直到队列中的数据被消费掉。 案例 创建队列 12from queue import Queueq = Queue(maxsize=3) 存取数据 1234# 存放数据q.put('aaa')# 获取数据q.get() LIFO队列LIFO即Last in First Out,后进先出。与栈的类似，使用也很简单,maxsize用法同上 创建队列 12from queue import LifoQueueq = LifoQueue() 存放数据API语法同上 常用方法 task_done() 告诉队列该任务已经处理完毕，队列的unfinished_tasks的属性-1 join() 堵塞调用线程，直到队列中的所有任务被处理完毕，一旦有数据被加入队列，未完成的任务数就会增加。当消费者线程调用task_done()（意味着有消费者取得任务并完成任务），未完成的任务数就会减少。当未完成的任务数降到0（内部就是当队列的unfinished_tasks属性为0时），join()解除阻塞。 put(item[, block[, timeout]]) 将item放入队列中。 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常 get() 从队列中移除并返回一个数据。block跟timeout参数同put方法 其非阻塞方法为｀get_nowait()｀相当与get(False) empty() 如果队列为空，返回True，反之返回False]]></content>
      <categories>
        <category>Python-Standard-Library</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程守护和线程同步问题]]></title>
    <url>%2F2017%2F11%2F26%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E5%AE%88%E6%8A%A4%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是守护线程？什么是非守护线程？什么是线程同步？ 目标：搞懂Python中的上面三个问题 非守护线程 线程概念当一个进程启动以后，默认会产生一个主线程，因为线程是程序执行的最小单位，在Python当中线程默认情况下就是setDaemon(False)（非守护线程）,也就是主线程执行完自己的任务退出以后，子线程会继续执行自己的任务，不会随主线程退出受影响 案例1234567891011121314151617import threadingimport timedef run(): time.sleep(2) # 延时等待2s print('---子线程结束执行---')def main(): t1 = threading.Thread(target=run) t1.start() print('---主线程结束执行---')if __name__ == '__main__': main() 输出 12---主线程结束执行------子线程结束执行--- 总结:非守护线程，子线程不随主线程结束而立马结束，而是继续执行 守护线程 当守护线程时，子线程会守护主线程，主线程一旦退出，全部子线程都会被强制终止 案例123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 设置子线程守护主线程 t1.start() print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 线程同步 线程同步就是让线程处于堵塞状态，等待子线程执行以后主线程再执行，也可以设置堵塞时间 案例（主线程堵塞，等待子线程结束以后再执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join() # 主线程堵塞 print('---主线程结束---')if __name__ == '__main__': main() 输出 12---子线程结束------主线程结束--- 案例（主线程堵塞1s，主线程继续执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 12---主线程结束------子线程结束--- 案例（主线程堵塞1s，然后守护主线程）12345678910111213141516171819import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 守护主线程 t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 主线程等待1s后主线程结束，子线程随着主线程结束而结束 总结 1、为了保证程序能够正常的运行，主线程/主进程都会等所有的子进程/子线程执行完成以后再销毁。 2、当主进程内部非守护进程和守护进程同时存在时 主进程执行结束以后会先强制退出守护进程，再等非守护进程结束以后再销毁 3、当主线程内部非守护线程和守护线程同时存在时 主线程执行结束以后会先等待所有的非守护线程结束，再强制退出守护线程，最后销毁]]></content>
      <categories>
        <category>Python-Concurrent</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之ubuntu16.04安装搜狗输入法]]></title>
    <url>%2F2017%2F11%2F26%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bubuntu16-04%E5%AE%89%E8%A3%85%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[自己本地折腾了一个 ubuntu的desktop版的虚拟机，装完裸机一个，啥也没有。本文介绍一下自己输入法（搜狗）的安装步骤，网上教程一堆但是很多都有问题 1、更新软件源1.1、打开文件1sudo gedit /etc/apt/sources.list 1.2、在文件开头添加下面的阿里云的软件源12345678910deb http://mirrors.aliyun.com/ubuntu/ quantal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-backports main restricted universe multiverse 1.3、更新软件源1sudo apt-get update 2、安装输入法2.1、下载搜狗输入法安装包搜狗输入法Linux版下载地址 2.2、安装12sudo apt-get install -fsudo dpkg -i sogoupinyin_2.0.0.0072_amd64.deb # deb文件要和自己下载的版本一致 2.3设置语言选项到系统设置-&gt;语言支持（System-&gt;Language Support），将键盘输入法系统由默认的iBus设置为fcitx。如下图： 2.4 重启我的重启以后直接点右上角输入法图标就可以看到了，网上有的教程还需要去输入法设置里面添加sougou 3、安装 fcitx键盘输入法系统sogou是基于fcitx的，而系统默认的键盘输入法系统是iBus。Ubuntu 16.04默认是带有fcitx的，正常安装，如果有的话，按上面步骤即可完成；但有些版本的Ubuntu，需要自己安装 fcitx，才能安装使用sogou。 1、添加以下源 sudo add-apt-repository ppa:fcitx-team/nightly 2、更新系统：sudo apt-get update 3、安装fcitx：sudo apt-get install fcitx 4、安装fcitx的配置工具：sudo apt-get install fcitx-config-gtk 5、安装fcitx的table-all软件包:sudo apt-get install fcitx-table-all 6、安装im-switch切换工具：sudo apt-get install im-switch 至此，fcitx键盘输入法系统就安装好了。第5，6步需要按键“Y”确认安装。简单测试的方法就是在终端键入“fcitx”，有各种提示就对了。安装完fcitx后，再安装sogou即可。]]></content>
      <categories>
        <category>Operating-System</category>
      </categories>
      <tags>
        <tag>ubuntu16.04</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用操作]]></title>
    <url>%2F2017%2F10%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之assert&raise]]></title>
    <url>%2F2017%2F04%2F03%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8Bassert-raise%2F</url>
    <content type="text"><![CDATA[看Python某些库的源码经常会看到 assertor raise这两个内置函数，但是自己平常写代码好像没怎么用过，使用 try except会多一点，可能是自己太low 。。。。 语法介绍 assert assert主要的作用就是给程序断言，声明条件必须为真的判断，如果条件为假则抛出异常，并可以编写异常描述。 语法格式12assert 表达式 '错误描述# 如果表达式为真则继续往下执行，如果为假则抛出断言异常&gt;&gt;AssertionError 案例123foo = [1, 2, 3, 4]# 断言foo列表的长度大于5assert len(foo) &gt; 5, '列表长度不大于5' 输出 1234Traceback (most recent call last): File "/Users/qiangzai/Desktop/practiceCode/ElementaryClassCode/day01/01-list-questions-answers.py", line 3, in &lt;module&gt; assert len(foo) &gt; 5, '列表长度不大于5'AssertionError: 列表长度不大于5 raise raise也是用来抛出异常的，一般用在自己觉得会报错的代码当中使用 语法格式123raise TypeError('错误描述')# 注意这里的TypeError不是固定的，可以是任意自定义的内置错误类型 ValueError,NameError,PermissionError等一堆 案例123456num = 1try: num += '1'except Exception as e: raise TypeError('字符串类型不能和整型相加') 输出 1TypeError: 字符串类型不能和整型相加]]></content>
      <categories>
        <category>Python-Advanced</category>
      </categories>
      <tags>
        <tag>assert</tag>
        <tag>raise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机之UEFI和Legacy]]></title>
    <url>%2F2017%2F03%2F23%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8BUEFI%E5%92%8CLegacy%2F</url>
    <content type="text"><![CDATA[今天主要讲一下UEFI 和Legacy这两种引导启动方式的区别 由于经常帮同学装电脑系统，看到boot模式装UEFI BIOS感觉得心应手，但是有的电脑（尤其是w7系统）老是出现：无法将Windows安装到磁盘0的分区这种问题，后来才明白是 UEFI+GPT 和Legacy+MBR这两种模式区别造成的 解决办法：https://wenku.baidu.com/view/6b6dfb5ef7ec4afe04a1dfb9.html 两种模式运行方式 对比采用传统的BIOS引导启动方式，UEFI BIOS减少了BIOS自检的步骤，节省了大量的时间。UEFI BIOS比传统的BIOS先进得很多，它的标准已经制定了很多年，目前新出厂的电脑基本清一色 UEFI BIOS，但是电脑磁盘的格式必须是GPT格式，这完全不同于传统的MBR格式，所以在使用UEFI BIOS格式给电脑磁盘格式为MBR的装系统时候就会出现上面的问题，需要把电脑磁盘格式从MBR—&gt;GPT]]></content>
      <categories>
        <category>Operating-System</category>
      </categories>
      <tags>
        <tag>UEFI</tag>
        <tag>Legacy</tag>
      </tags>
  </entry>
</search>
