<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记录之19年学习技术栈]]></title>
    <url>%2F2019%2F01%2F01%2F%E8%AE%B0%E5%BD%95%E4%B9%8B18%E5%B9%B4%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[记录一下今年的学习目标，Fighting！！！ [ ] 爬虫回顾 requests scrapy 目标：基本使用 [ ] Web框架延伸 1、Django&amp;Django-rest-framework 目标：源码尝试阅读2、第三方组件集成学习（channel）3、websocket Flask 目标Flask复习 Tornado 目标：学习使用 Bottle 目标：基本使用 [ ] 数据分析 numpy pandas 目标：基本使用API语法熟悉 [ ] 机器学习 常用算法 特征工程 推荐算法 [ ] 数据结构&amp;算法 常用的设计模式 基本算法解决思路 [ ] 新语言学习 C语言 Java语言 Go语言 目标：入门即可，了解底层语言帮助自己对Python理解 [ ] 数据库 [ ] SQL 目标：反复练习 [ ] redis 目标：反复练习 运维 [ ] Docker&amp;K8S&amp;Supervisor 目标：学习和基本使用]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[纪念金庸]]></title>
    <url>%2F2018%2F10%2F30%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[飞雪连天射白鹿，笑书神侠倚碧鸳。 侠之大者，为国为民。 红颜弹指老，刹那芳华，与其天涯思君，恋恋不舍，莫若相忘于江湖。——金庸《天龙八部》 四张机，鸳鸯织就欲双飞，可怜未老头先白，春波碧草，晓寒深处，相对浴红衣。——金庸《射雕英雄传》 一座山，隔不了两两相思，一天涯，断不了两两无言，且听风吟，吟不完我一生思念。——金庸《神雕侠侣》 他强由他强，清风拂山岗；他横任他横，明月照大江。——金庸《倚天屠龙记》 情不知所起，一往情深；恨不知所终，一笑而泯。——金庸《笑傲江湖》]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于AWS服务器利用sandowsocks科学上网]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%9F%BA%E4%BA%8EAWS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%A9%E7%94%A8sandowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一直都想找Google和AWS这种大BOSS撸羊毛，但是苦于没有信用卡，也不想走淘宝买卡害怕被套路，偶然机会需要出国旅游，所以匆匆去办了一张中信的 Visa信用卡（欧洲用现金、美国用信用卡、中国在用支付宝，国外很流行信用卡），回国以后就赶紧草草注册了AWS，开启科学上网～ 一、注册AWS并创建Ec2实例1、注册地址：https://www.amazonaws.cn/sign-up/&gt; 2、注册需要条件 信用卡（必备，我用的visa双币卡，注册后扣除1$） 手机号 邮箱 备注：注册成功以后需要等待一天验证身份什么的，反正我当初等了一天才能去创建Ec2 3、申请成功，创建Ec2实例 创建实例我选择的是 Ubuntu16.40因为对ubuntu系统比较熟悉 参考链接：https://blog.csdn.net/jewely/article/details/78030057 保存好私钥文件，文件结尾是xxx.pem 二、连接服务器1、修改一下本地私钥文件权限 12chmod 400 xxx.pem# 备注 权限太高，后面ssh的时候连接不了会提示不安全 2、ssh连接 1ssh -i "xxx.pem" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com xxx.pem 就是你下载下来的私钥文件 xxx这个名字是当时下载时写的，大家按自己的来 @前面的那个ubuntu应该是固定的，我选的就是ubuntu系统 @后面的就是AWS上你创建的实例的公有DNS（IPv4）名称 或者写 IPv4的公有IP也可以 我两个都尝试了 It`s OK 3、修改一下root用户密码 1sudo passwd root 三、安装shadowsocks并配置默认ubuntu16.04已经预装了Python3 1、更新apt-get 1apt-get update 2、安装pip3（我用惯了Python3） 1apt-get install python-pip3 3、安装shadowsocks 1pip3 install shadowsocks 4、创建配置文件 1sudo touch /etc/shadowsocks.json 5、添加配置内容 1234567891011&#123; "server": "0.0.0.0", // 服务端IP 0.0.0.0即可方便连接 "server_port": 50003, // 服务器端口方便后期客户端连接 "local_address": "127.0.0.1", "local_port": 1080, "password": "******", //连接服务器的密码，自己设置，客户端连接时候要填写 "timeout": 300, "method": "aes-256-cfb", // 加密方式 "fast_open": false, "workers": 1&#125; 配置以后启动shadowsocks 1sudo ssserver -c /etc/shadowsocks.json -d start 如果遇到permission denied错误解决方法如下 1234# 第一步查看sserver的位置 which ssserver # 第二步sudo 填写完整的ssserver路径 -c /etc/shadowsocks.json -d start 6、修改AWS上EC2实例的入站端口 配置好 shaodowsocks 后，还需要将配置中的端口打开, 这样客户端的服务才能链接得上 EC2 中的 shadowsocks 服务，首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则 四、客户端连接&amp;科学上网1、打开本地shadowsocks客户端 2、填写信息 3、上网 遇到问题的可以留言区留言，一起看看 配置多用户上网]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>sandowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之常见问题总结]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目的：记录爬虫中遇到的坑…… xpath提取数据遇到tbody标签问题描述：今天在用scrapy爬取网页时，当爬取表格内容时，发现使用插件 xpath helper获取正常，程序中的xpath解析不到 问题原因：浏览器会在table标签下添加tbody标签，浏览器会对html文件进行一定的规范化，但是源码当中是没有tbody标签的。 解决办法：将分析的xpath语句中把tbody去掉即可]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>spider_error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Scrapy框架]]></title>
    <url>%2F2018%2F05%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Scrapy是一个高性能的爬虫框架，主要是为了提取结构化数据而编写的应用框架，我们只需要实现少量代码就可以实现数据的快速获取，框架底层使用的Twisted异步网络框架,所以爬取速度非常的快。 文章目的：掌握Scrapy的基本使用 安装1pip install scrapy 查看常用操作(检查是否安装成功) 1scrapy --help 基本使用流程 第一步：创建项目（固定步骤） 1scrapy startproject 项目名称 第二步：创建爬虫（固定步骤） 12cd 项目目录scrapy genspider 爬虫名称 目标域名 第三步：编写爬虫组件和管道（主要编写的部分） 1# 在第二步创建的爬虫文件当中编写 第四步：启动爬虫（固定步骤） 1scrapy crawl 爬虫名称 scrapy组成 运行流程图（scrapy官方公布） 各部件作用 五大组件 引擎 中央协调 爬虫组件 提取数据 or url，生成请求对象 请求调度器 存放请求对象 下载器 发送请求获取响应 管道 保存数据 三大对象 Request请求对象 Response响应对象 Item数据对象 两大中间件 下载中间件 爬虫中间件 项目目录结构介绍 spider-project项目目录 __init__.py items.py定义数据模型 middlewares.py自定义中间件 pipelines.py自定义管道，保存数据 settings.py项目配置文件 scrapy.cfg项目部署文件 spiders爬虫组件目录 xxx.py具体实现爬虫的文件 通过 scrapy genspider 爬虫名称–&gt;生成的爬虫都在这个目录下 五大组件之爬虫组件 作用：提取数据 | 提取url返回请求对象 具体编写步骤1、继承爬虫类 123456# -*- coding: utf-8 -*-# 导入模块import scrapy# 继承爬虫类class ExampleSpider(scrapy.Spider): ... 2、定义爬虫名称 1name = 'example' 3、设置允许爬取的范围 1allowed_domains = ['example.com'] 4、设置开始爬取的请求地址 1start_urls = ['https://wxample.com/xxxx'] 5、实现解析函数 ​ 1、提取数据 ​ 2、提取url，返回请求对象 12def parse(self, response): pass 响应对象response基本信息 response.status &gt;&gt; 响应状态码 response.headers &gt;&gt; 响应头 response.text&gt;&gt; 响应内容 response.request.cookies&gt;&gt;获取cookies Selector对象基本操作 extract()提取数据，提取不到会抛出下标异常 extract_first()提取第一条数据，提取不到返回None 提取&amp;构建请求对象12345# 构建请求对象request = scrapy.Request(url='xxxx')# 返回请求对象&gt;&gt;引擎&gt;&gt;请求调度（scheduler）器&gt;&gt;请求下载器yield request 参数callback指定请求对象的解析函数 参数 meta把数据可以传递给解析函数 12345yield scrapy.Request(url=detail_url, callback=self.parse_detail, meta=&#123; 'item':item &#125;) 五大组件之管道 系统管道1、提取函数中使用 yield返回函数 2、命令行导出 1scrapy crawl 爬虫名称 -o 导出文件名称 自定义管道1、在 pipelines.py编写管道 ​ 1、创建管道类 ​ 2、实现管道处理数据方法 ​ 2.1 process_item必须实现 ​ 2.2 open_spider 可选实现，当爬虫启动时调用一次 ​ 2.3 close_spider 可选实现，当爬虫结束时调用一次 12345678# pipelines.pyclass JsonDataPipeline(object): '''创建管道类''' def process_item(self, item, spider): '''实现管道处理数据方法 --&gt;yield item--&gt;引擎--&gt;触发process_item方法 ''' pass 2、在 settings.py中开启管道 1234ITEM_PIPELINEs = &#123; # 在里面配置自己定义的管道类 'xxx.pipelines.定义的管道类名称':score # score 表示数字和权重&#125; 注意点 1、数字越小越优先执行 2、多管道 item传递必须在 process_item函数中返回 item 自定义数据模型 作用：规范数据的格式 定义文件位置 items.py 123456import scrapyclass dataItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pass 使用 123456# 导入items.py里面定义的模型类from xxx.items import dataItem# 创建模型对象data = dataItem()# 给对象添加数据和字典操作一样data['name'] = 'xxxx' 模型对象转字典 1dict(模型对象) 项目配置 settings.py可以配置项目中常用信息 注意：scrapy.settings.default_settings.py才是所有的配置信息，如果在settings.py中没有配置，就是用默认的配置文件 LOG_LEVEL= WARNING 表示显示的日志级别 LOG_FILE= 自定义的日志PATH 表示日志的存储位置 使用方式在 defalut_settings.py中寻找配置，然后把配置选项在 settings.py中重新配置； scrapy shell介绍 方便开发过程中的调试 使用1scrapy shell 请求地址 常用方法 request response xpath css text fetch请求新的地址 CrawlSpider crawlSpider是一个已经实现了爬取流程的爬虫； 创建1scrapy genspider -t crawl 爬虫名称 爬虫域名 参数 -t表示采用的爬虫模版 爬取流程提取符合条件的链接—&gt;跟进符合条件的链接—&gt;提取符合条件的链接—&gt;循环 使用说明 rules所有规则、元组、存放了提取页面的规则列表 Rule对象，条件规则对象 LinkExtactor链接提取器 callback当提取完页面后的回调处理函数 follow是否跟进提取,默认是 True LinkExtractor链接提取器 allow&gt;&gt;内容符合条件规则被提取 deny&gt;&gt;内容符合条件规则被排除（优先） allow_domains&gt;&gt;符合条件的域名被提取 deny_domians&gt;&gt;符合条件的域名被排除 restrict_xpaths&gt;&gt;根据xpath提取 tags&gt;&gt;默认(‘a’,’area’)符合链接提取的标签名 attrs&gt;&gt;默认(href)，链接提取标签的属性名 restrict_css&gt;&gt;根据css样式提取器提取 strip&gt;&gt;提取后的内容去除空格 链接去重把链接地址放到 seen--&gt;set() 注意：如果url的参数位置发生变化会导致无法去重 crawlspider编写代码流程1.编写提取规则—&gt;提取链接规则 2.编写提取具体页面解析代码]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之selenium]]></title>
    <url>%2F2018%2F04%2F27%2F%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%2F</url>
    <content type="text"><![CDATA[Selenium介绍 Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏。程序脚本可以通过 Selenium API 控制浏览器。 运行流程 环境搭建 安装selenium 1pip install selenium 下载对应的驱动 淘宝驱动地址：http://npm.taobao.org/mirrors/chromedriver/ 备注：一定要下载本地浏览器版本对应的驱动 基本操作 12345678910111213141516171819# 导入模块from selenium import webdriverimport time# 加载浏览器驱动，创建浏览器对象browse = webdriver.Chrome('./chromedriver')# 访问百度browse.get('https://www.baidu.com')# 定位百度搜索输入框，输入关键字“电影”browse.find_element_by_name('wd').send_keys('电影')# 点击‘百度一下’browse.find_element_by_id('su').click()time.sleep(2) # 退出窗口browse.quit() selenium驱动寻找方式 1、通过指定浏览器驱动路径 1browser = webdriver.Chrome('浏览器驱动位置') 2、通过 $PATH环境变量指定浏览器驱动 12# 通过 $PATH 寻找驱动，如果寻找不到就报错 browser = webdriver.Chrome() 添加网址：https://www.jianshu.com/p/e50a49f86070 其实就是找到Python解释器的 bin目录，把驱动放进去即可 selenium控制浏览器操作访问URL1browser.get('https://www.baidu.com/') 定位元素 find_element_by_xxx返回第一个符合条件 WebElement find_elements_by_xxx返回符合条件的 WebElement列表 xxx说明 find_elements_by_class_name通过标签的class属性定位元素 find_element_by_id通过标签的ID属性定位元素 find_element_by_name通过标签的name属性定位 find_element_by_css_selectorcss样式选择 find_element_by_link_text通过链接内容查找 find_element_by_partial_link_text 通过链接内容包含的内容查找，模糊查询 find_element_by_xpath 通过xpath查找数据 xpath只能获取webelement对象，不能直接获取属性和文本内容 获取元素属性和文本内容1234# 获取属性WebElement.get_attribute('属性名')# 获取文本内容WebElement.text 输入框Input输入内容1Input_element.send_keys('xxx') 点击1element.click() 使用无界面浏览器 使用无界面的驱动 下载地址：http://phantomjs.org/download.html 使用方式 browser = webdriver.PhantomJS(&#39;驱动路径‘) 设置chrome启动参数 123456789101112131415from selenium import webdriver# 创建浏览器启动参数options = webdriver.ChromeOptions()options.add_argument('--headless') # 无界面options.add_argument('--disable-gpu') # 禁用gpu# 创建浏览器对象browser = webdriver.Chrome(chrome_options=options)# 访问网址browser.get('https://www.baidu.com')# 截图证明browser.save_screenshot('百度截图.png')# 退出browser.quit() 设置User-Agent和Proxy代理123456options = webdriver.ChromeOptions()# 切换User-Agentoptions.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1')# 设置代理options.add_argument('--proxy-server=代理服务器地址') # 设置代理browser = webdriver.Chrome('chrome_options=options) 获取网页源码1browser.page_source 注意：获取的网页源码是经过JS页面执行后的结果源码 cookies 操作（非常重要） 获取所有Cookies 1browser.get_cookies() 通过名字获取Cookie 1browser.get_cookie() 添加Cookie 1browser.add_cookie() 通过名字删除Cookie 1browser.delete_cookie() 删除所有Cookie 1browser.delete_all_cookies() 执行JS代码1browser.execute_script("js-code") 等待加载 方式一：强制等待，浪费时间 1time.sleep(秒数) 方式二：隐性等待 1browser.implicitly_wait(等待时间) 方式三：显性等待，每个元素都可以自己定义检查条件 手动编写方式 123456789101112131415# 显性等待-手动编写t = time.time()# 定义超时时间timeout = 60while True: try: # 超时时间间隔 time.sleep(0.1) url_element = browser.find_element_by_class_name("favurl") break except: # 超时处理 if time.time() - t &gt; timeout: break pass 系统提供显性等待API 1234567891011121314151617# 导入显性等待的API需要的模块# 1&gt; 等待对象模块from selenium.webdriver.support.wait import WebDriverWait# 2&gt; 导入等待条件模块from selenium.webdriver.support import expected_conditions as EC# 3&gt; 导入查询元素模块from selenium.webdriver.common.by import By# 使用selenium api 实现显性等待# 1&gt; 创建等待对象# 参数一 浏览器对象# 参数二 超时时间# 参数三 检查元素时间间隔wait = WebDriverWait(browser,60,0.1)# presence_of_element_located 检查元素是否存在，参数是一个元祖，元祖内部描述等待元素查询方案# visibility_of_element_located 检查元素是否可见url_element= wait.until(EC.presence_of_element_located((By.CLASS_NAME,"favurl"))) 窗口切换 获取所有的窗口（列表） 1browser.window_handles 切换窗口 1browser.switch_to_window(all_windowsp['index']) 备注：browser.title可以通过打印当前窗口title调试 iframe切换 1、获取iframe标签对象 1iframe_element = browser.find_element_by_xxx('') 2、切换到iframe窗口 1browser.switch_to.frame(iframe_element) 3、切换到主窗口 1browser.switch_to.default_content()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv和json之间的转换]]></title>
    <url>%2F2018%2F04%2F26%2Fcsv%E5%92%8Cjson%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[CSV（逗号分隔值文件格式），是一种通用的，相对简单的文件格式，被用户、商业、科学广泛使用，最广泛的应用是在程序之间转移表格数据。 文章目标：了解CSV的基本操作以及json格式数据如何写入CSV文件并转换Excel 使用场景爬虫拿到的大部分都是json格式数据，但是客户或者数据分析员一般更需要Excel这种格式的数据 基本操作 1、导入模块 12# python3.X内置模块不需要下载import csv 2、创建csv写入对象 12345# 创建文件对象f = open('03-test.csv', 'w', encoding='utf-8')# 创建csv写入对象csv_writer = csv.writer(f) 3、写入数据 123csv_writer.writerow(['姓名', '年龄', '性别'])csv_writer.writerow(['强仔', '18', '男'])csv_writer.writerow(['彤彤', '17', '女']) 4、关闭文件 1f.close() json2csv 1、先转换json格式为python数据类型 12345import csvimport json# data.json 为事先准备好的json数据with open('data.json', 'r') as f: data_list = json.load(f) # 转换成pyton数据类型 2、创建csv对象 12f = open('data.csv', 'w', encoding='utf-8')csv_writer = csv.writer(f) 3、写入数据 1234567# 3.1 写入标题也就是字典的关键字keycsv_writer.writerow(data_list[0].keys())# 3.2 遍历写入字典元素valuesfor data in data_list: csv_writer.writerow(data.values())# 3.3 关闭文件句柄f.close() csv转Execl见链接：https://jingyan.baidu.com/article/3c343ff7faa59b0d3779633e.html]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之数据提取]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[今天主要记录一下数据提取的几种方法，在爬虫获取到响应数据以后我们改怎么处理响应的数据呢？从网上拿到的响应数据各种类型都有：JSON类型的，XML格式的,html格式的等等。 常用的数据提取方式 jsonpath re xpath beautifulsoup 一、jsonpath基本使用 json是什么JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。 json模块常用操作 json.loads(json_str) JSON字符串—&gt;转换成Python数据类型 json.dumps(dict_data) Python数据类型—&gt;JSON字符串 json.load(f) json文件—&gt;获取Python数据类型 json.dump(dict_data, f) Python数据类型—&gt;写入json文件 ensure_ascii=False 显示中文 indent=数字表示空格 Jsonpath是什么？ jsonpath是一种语法规则快速从JSON数据当中提取数据 在线调试网址：https://jsonpath.com/ 在线解析工具：https://www.json.cn/# 语法规则 语法 描述 案例 $ 根节点 @ 现行节点 . 取子节点 $.store.book .. 取子孙节点 $..book [] 设置筛选条件 $..book[0] [,] 支持多选选择内容 $..book[1,3] () 支持表达式计算 $..book[(@.length - 1)] ?() 支持过滤操作 $..book[?(@.price&lt;10)] Python中怎么使用123import jsonpathdata = jsonpath('字典类型数据', 'jsonpath提取语法')# data是list类型 如果提取不出来返回False 二、re基本使用 用事先定义好的一些特定字符，及这些特定字符的组合，组成一个规则字符串，这个规则字符串用来表达对字符串的一种过滤逻辑 基本语法 预定义字符集 \d表示数字：[0-9] \D表示非数字：[^\d] \s表示空白字符 \S非空白字符 \w单词字符：[A-Z,a-z,0-9] \W非单词字符：[^\w] .匹配任意除换行符\n外的字符，备注：在DoTALL模式中也能匹配换行符 \转义字符，使后一个字符表示字符本身 []表示字符的选取范围 数量词 *表示任意个 +表示至少一个 ?表示至多一个（1个或者没有） {}表示范围区间 {1} 表示匹配一个字符 {1,6}表示匹配1到6个字符 {,6}表示最多匹配6个字符 贪婪模式和非贪婪模式 贪婪模式(.*)尽可能多的匹配 非贪婪模式(.*?)一旦匹配到就结束 DOTALL模式 让.匹配\n 1re.RegexFlag.DOTALL、re.RegexFlag.S、re.S、re.DOTALL 忽略大小写 1re.RegexFlag.I re.RegexFlag.IGNORECASE re.I re.IGNORECASE 支持DOTALL 和忽略大小写 用| 原始字符串使用r不用\ 正则前面使用r正则语法 四种匹配检索方法 match match 从头开始匹配，仅匹配一次 search search 全局匹配，仅匹配一次 findall findall 获取符合条件的所有数据，返回列表 finditer finditer 获取符合条件的所有数据，返回迭代器对象 数据量大的时候推荐使用，提高内存使用效率 分组&amp;替换 sub split 通用分组格式：[分隔符]+ 1234567import redata = 's,fqw;fwe, fds;fsda,afds;'pattern = re.compile(r'[, ;]+')ret = pattern.split(data)print(ret) 输出 1['s', 'fqw', 'fwe', 'fds', 'fsda', 'afds', ''] sub 12345678import redata = 's,fqw;fwe,fds;fsda,afds;'# 把, ; 都替换成 #pattern = re.compile(r'[,;]+')ret = pattern.sub('#', data)print(ret) 输出 1s#fqw#fwe#fds#fsda#afds# 三、Xpath基本使用 xpath是一门在HTML/XML文本中查找信息的语言，可以用在HTML/XML文档中对元素和属性进行遍历，简单来说就类似于正则通过一定的语法规则从文本中提取数据 基本语法 节点选择 /表示从根节点选取 //从匹配选择的当前节点选择文档中的节点 .表示选取当前节点 ..选取当前节点的父节点 选取属性 @ //节点/@属性名称 文本选择 text()选取文本 /text() 高级语法-筛选条件 //node[筛选条件] 通过下标（数字）筛选（下标从1开始） /node[1] 通过属性筛选 /node[@属性] /node[@属性=值] 通过文本内容筛选 //node[text()=&#39;筛选的值&#39;] 通过内置函数 last() position() containes包含内容 contains(text(), &#39;文本&#39;) contains(@属性， ‘属性值’) starts-with 从头匹配内容 语法和containes一样 通过子节点来筛选 /node[node_子节点&gt;子节点的值] 通配符 *匹配任何元素节点 //*[text()=&#39;某值&#39;] //*[@category=’某value’] @*匹配node任意属性的value //node[@*=&#39;某value&#39;] 多个条件同时满足 使用 | lxml模块 Python lxml模块去使用xpath语法 安装1pip install lxml lxml基本使用1、导入模块 1from lxml import etree 2、构建根元素对象 1eroot = etree.HTML(html_data) # eroot 是一个element对象 3、操作根元素对象提取数据 1eroot.xpath('xpath语法规则') 4、打印调试查看element对象内容 1etree.tostring(eroot).decode('utf-8') 四、beautifulsoup基本使用 介绍 Beautifulsoup也是一个HTML/XMl的解析器，跟上面的 lxml一样 优点 用来解析HTML比较简单，API非常人性化，支持CSS选择器 缺点 lxml只会遍历局部，而Beautifusoup是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml 安装1pip install beautifulsoup4 官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0 基本使用1、导入模块 1from bs4 import BeautifulSoup 2、创建beautifulsoup对象 12soup = BeautifulSoup(html,'lxml')soup = BeautifulSoup(open('文件')) 3、操作node 获取元素标签内容 1soup.标签名称 获取子元素 1234# 返回列表soup.node_name.contents# 返回迭代器soup.node_name.children 获取元素内容 1soup.node_name.get_text() 获取元素属性值 1soup.a.get('href') 注意如果获取元素的属性是 class返回的是列表 find 和find_all介绍 find返回符合条件的第一个元素 find_all返回符合条件的所有元素列表 备注：find和find_all提供参数都一样，下面以find_all()来展示 1、通过标签和标签列表查找元素 12soup.find_all('a') # 查询所有的a标签soup.find_all(['a', 'b']) # 查询所有的a标签和b标签 2、通过正则表达式查找元素 12# 以b开头的标签查找soup.find_all(re.compile('^b')) 3、通过属性来查找 12345soup.find_all(&#123; attrs=&#123; '属性名'：'值' &#125;&#125;) 4、通过文本内容查找元素 1soup.find_all(text='查询的文本内容') 6、混合使用 1234567soup.find_all( '标签名', attrs=&#123; '属性名'：'值' &#125;， text='内容') CSS样式选择器 返回值是一个list 类选择器 使用 .开头 1soup.select('.class_name') ID选择器，使用#开头 1soup.select('#id_name') 标签选择器，使用标签 名称 1soup.select('p_name') 属性选择器 1soup.select('p[属性=值]') 层级选择器 div p 选择 &lt;div&gt; 元素内部的所有 &lt;p&gt; 元素。 div , p 选择所有 &lt;div&gt; 元素和所有 &lt;p&gt; 元素。 div &gt; p 选择父元素为 &lt;div&gt; 元素的所有 &lt;p&gt; 元素。 .cls1.cls2 选择类名是cls1 并且 类名是cls2]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>jsonpath</tag>
        <tag>bs4</tag>
        <tag>xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之js逆向百度翻译]]></title>
    <url>%2F2018%2F03%2F22%2F%E7%88%AC%E8%99%AB%E4%B9%8Bjs%E9%80%86%E8%A1%8C%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[爬虫小试身手之js逆向百度翻译，目前百度翻译在PC和移动端都做了反扒措施，今天我们一看究竟～ 什么是js逆向？所谓的js逆向其实就是破解对方反扒的一种方式，目前很多网站反扒都使用js来做，百度翻译的反扒主要是利用了请求参数中的 sign这个签名来做的，下面就来详细的讲解 sign怎么变动了？当你更换输入框输入的翻译目标时，对比请求携带的参数发现sign这个参数随着输入的单词所变动，判断应该是js代码在请求前生成了这个sign，需要去定位具体哪个JS文件的生成了这个sign 根据上图我们可以看出sign参数是由js代码 m(a)来生成的，a参数是as也就是我们要翻译的目标，这个是我们可以拿到的，现在要做的就是如何执行m()这段js代码，Python被称为胶水语言，在Python当中调用JS代码自然也是可以实现的，Python调用js代码目前主流的两个模块 pyexecjs 、js2py，今天我们讲的主要是通过 js2py来实现。 下载模块1pip install js2py 基本使用 在python当中执行js 12345678# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在Python当中执行js代码content.execute("console.log('abc')") 输出1'abc' 在JS代码中调用Python程序 123456789101112131415161718# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在js当中调用Python程序content.a = 1content.b = 'abc'content.c = [1, 3, 4]content.d = &#123; "name": 'qwe'&#125;content.execute('console.log(a)')content.execute('console.log(b)')content.execute('console.log(c)')content.execute('console.log(d)') 输出1234'1''abc'[1, 3, 4]&#123;'name': 'qwe'&#125; 实战既然Pythoncode中可以调用js我们只要找到这段js在代码中执行就解决了这个问题，上代码 1234567891011121314151617181920212223242526272829303132333435363738394041# Python代码# 1.导入模块import requestsimport js2py# 2.发送请求，获取响应内容# 请求地址 请求头 请求方式 请求参数url = 'https://fanyi.baidu.com/v2transapi'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36', 'Referer': 'https://fanyi.baidu.com/?aldtype=16047', 'Cookie': 'BAIDUID=DAD75821AD7D4663D00D9C0E7F9C8011:FG=1; BIDUPSID=DAD75821AD7D4663D00D9C0E7F9C8011; PSTM=1553134744; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1439_21121_18559_28723_28557_28697_28585_28518_28627_22157; PSINO=2; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553218276,1553218292; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553218292'&#125;# 需要翻译的内容kw = 'thank'# 构建上下文对象content = js2py.EvalJs()with open('test.js', 'r') as f: content.execute(f.read()) sign = content.e(kw)data = &#123; "from": "en", "to": "zh", "query": kw, "transtype": "translang", "simple_means_flag": "3", "sign": sign, "token": "88d322490309c4abee0496f0dbab1a4b",&#125;response = requests.post( url=url, headers=headers, data=data)print(response.json()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445// test.js 文件var i = '320305.131321201';function n(r, o) &#123; for (var t = 0; t &lt; o.length - 2; t += 3) &#123; var a = o.charAt(t + 2); a = a &gt;= "a" ? a.charCodeAt(0) - 87 : Number(a), a = "+" === o.charAt(t + 1) ? r &gt;&gt;&gt; a : r &lt;&lt; a, r = "+" === o.charAt(t) ? r + a &amp; 4294967295 : r ^ a &#125; return r &#125;function e(r) &#123; var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g); if (null === o) &#123; var t = r.length; t &gt; 30 &amp;&amp; (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10)) &#125; else &#123; for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h &gt; C; C++) "" !== e[C] &amp;&amp; f.push.apply(f, a(e[C].split(""))), C !== h - 1 &amp;&amp; f.push(o[C]); var g = f.length; g &gt; 30 &amp;&amp; (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join("")) &#125; var u = void 0 , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107); u = null !== i ? i : (i = window[l] || "") || ""; for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v &lt; r.length; v++) &#123; var A = r.charCodeAt(v); 128 &gt; A ? S[c++] = A : (2048 &gt; A ? S[c++] = A &gt;&gt; 6 | 192 : (55296 === (64512 &amp; A) &amp;&amp; v + 1 &lt; r.length &amp;&amp; 56320 === (64512 &amp; r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 &amp; A) &lt;&lt; 10) + (1023 &amp; r.charCodeAt(++v)), S[c++] = A &gt;&gt; 18 | 240, S[c++] = A &gt;&gt; 12 &amp; 63 | 128) : S[c++] = A &gt;&gt; 12 | 224, S[c++] = A &gt;&gt; 6 &amp; 63 | 128), S[c++] = 63 &amp; A | 128) &#125; for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b &lt; S.length; b++) p += S[b], p = n(p, F); return p = n(p, D), p ^= s, 0 &gt; p &amp;&amp; (p = (2147483647 &amp; p) + 2147483648), p %= 1e6, p.toString() + "." + (p ^ m) &#125;]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>js2py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Http协议]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%88%AC%E8%99%AB%E4%B9%8BHttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[什么是HTTP协议？Http协议专业称之为：超文本传输协议，属于网络协议中应用层的协议，默认端口是80 HTTP请求格式 案例 HTTP请求方式HTTP请求可以使用多种请求方法 HTTP1.0定义了三种请求方法：GET, POST, HEAD HTTP1.1（主流）新增了五种请求方法：OPTIONS，PUT，DELETE，TRACE，CONNECT 请求方式 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 常见请求头 Cookie User-Agent 浏览器代理 Referer 防盗链，请求来自哪里 Host 请求主机 Connection Accept HTTP响应Http响应由四部分组成 状态行： HTTP/1.1 200 OK 消息报文： Content-Type: text/html….. 空行就是： 消息报文和正文中间的空行 响应正文：大家在浏览器看到的渲染后的内容 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开篇]]></title>
    <url>%2F2018%2F03%2F20%2F%E7%88%AC%E8%99%AB%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[当今是一个大数据的时代，数据也变得越来越值钱，有市场就有需要，“爬虫工程师”就此诞生～从此开始学习 Spider，开贴记录一下自己的学习经历, Fighting 什么是爬虫？所谓的爬虫其实就是模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动的去获取网络上的信息 爬虫分为哪些从网上简单的了解，爬虫主要分为通用爬虫（搜索引擎爬虫，典型的就是百度&amp;Google）和聚焦爬虫（爬一些指定的网站） 简单的爬虫流程 1、抓到起始的url，获取响应 2、对响应再次发送请求 3、如果能从响应中提取URL，则继续发送请求获取响应 4、如果提取数据，则将数据进行保存]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题（一）：Python操作Excel]]></title>
    <url>%2F2018%2F03%2F19%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9APython%E6%93%8D%E4%BD%9CExcel%2F</url>
    <content type="text"><![CDATA[前几天帮朋友做了一道面试题，感觉用Python做起来会方便很多，话不多说上图 一、模块介绍Python社区的强大和活跃，也为Python成为主流语言奠定了基础，丰富的第三方库使一切变成了可能，主要使用Python的两个模块 xlrd和xlwt 模块，顾名思义xlrd是读excel，xlwt是用来写excel的库，另外这两个库不是Python内置的模块需要自行安装。建议使用 pip 来进行安装 12# -i 可以指定下载的源地址，默认是使用的国外的源比较慢pip install xlrd -i https://pypi.douban.com/simple/ 二、解决题目的思路 1、要获取excel表格里面的数据 2、获取到每一行的数据，然后拼接成字典 3、把每一行拼接的字典放到列表当中 看着是不是很容易，但是如果不熟悉xlrd的语法操作起来也是很困难的，下面就罗列一下基本使用 获取Excel文件对象1data = xlrd.open_workbook(filename)#文件名以及路径，如果路径或者文件名有中文给前面加一个r原生字符。 常用的操作 1、获取data中某一个工作表（sheet） 12345table = data.sheets()[0] # 通过索引顺序获取table = data.sheet_by_index(sheet_indx)) #通过索引顺序获取table = data.sheet_by_name(sheet_name)#通过名称获取 2、行的操作 1234567891011nrows = table.nrows # 获取该sheet中的有效行数table.row(rowx) #返回由该行中所有的单元格对象组成的列表table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表table.row_len(rowx) #返回该列的有效单元格长度 3、列的操作 123456789ncols = table.ncols #获取列表的有效列数table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 4、单元格的操作 1234567table.cell(rowx,colx) #返回单元格对象table.cell_type(rowx,colx) #返回单元格中的数据类型table.cell_value(rowx,colx) #返回单元格中的数据table.cell_xf_index(rowx, colx) # 暂时还没有搞懂 三、代码奉上 ——&gt;So simple1234567891011121314151617181920212223242526272829# 导入模块import xlrd def Execl2List(): # 1.获取表格 excel = xlrd.open_workbook(r"./lianxi.xlsx") # 2.获取Sheet1表格页 sheet = excel.sheet_by_name("Sheet1") # 3.获取当前页的行数 row = sheet.nrows lists = [] for i in range(1, row): dic = &#123;&#125; list = sheet.row_values(i) # 获取表格对应行的数据 dic["姓名"] = list[0] dic["年龄"] = list[1] dic["性别"] = list[2] lists.append(dic) return listsif __name__ == '__main__': data = Execl2List() print(data)]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程池]]></title>
    <url>%2F2018%2F01%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[作用 计算机每次创建线程和销毁线程需要额外占用资源，频繁创建和销毁线程会导致计算机性能下降 处理过程 优点 重复利用线程，减少因为创建线程和销毁线程带来不必要的性能销毁 让程序更加的稳定，不会因为同一时间创建线程过多而导致内存不够使程序引发系统一系列的问题 代码实现流程 1、导入线程池模块 1from multiprocessing.dummy import Pool 2、创建线程池 1pools = Pool(5) 3、定义线程池要执行的任务 12def exec_task(): print('---要执行的任务---') 4、任务完成的回调函数 12def exec_task_finish(self, result): print('执行任务完成回调函数') 注意回调函数必须要有result参数，result参数表示执行任务代码的返回值 5、线程池执行任务 1pools.apply_async(self.exec_task,callback=self.exec_task_finish) 扩展进程池 12# 导入进程池库from multiprocessing import Pool 协程池 12345# 打补丁import gevent.monkeygevent.monkey.patch_all()# 导入协程池库from gevent.pool import Pool 备注，默认线程池里面的线程是守护线程，进程池&amp;协程池同理]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之队列Queue]]></title>
    <url>%2F2017%2F11%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E9%98%9F%E5%88%97Queue%2F</url>
    <content type="text"><![CDATA[Queue Queue是Python标准库中的线程安全的队列(FIFO)实现，提供了一个适用于多线程编程的先进先出的数据结构，即队列，用来在生产者和消费者之间的信息传递。 二种队列形式 FIFO队列FIFO即First in First Out，先进先出。Queue提供了一个基本的FIFO容器，使用方法很简单。 参数：maxsize是一个整数，指明了队列中能存放的数据个数的上限。一旦达到上限，插入会导致堵塞，直到队列中的数据被消费掉。 案例 创建队列 12from queue import Queueq = Queue(maxsize=3) 存取数据 1234# 存放数据q.put('aaa')# 获取数据q.get() LIFO队列LIFO即Last in First Out,后进先出。与栈的类似，使用也很简单,maxsize用法同上 创建队列 12from queue import LifoQueueq = LifoQueue() 存放数据API语法同上 常用方法 task_done() 告诉队列该任务已经处理完毕，队列的unfinished_tasks的属性-1 join() 堵塞调用线程，直到队列中的所有任务被处理完毕，一旦有数据被加入队列，未完成的任务数就会增加。当消费者线程调用task_done()（意味着有消费者取得任务并完成任务），未完成的任务数就会减少。当未完成的任务数降到0（内部就是当队列的unfinished_tasks属性为0时），join()解除阻塞。 put(item[, block[, timeout]]) 将item放入队列中。 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常 get() 从队列中移除并返回一个数据。block跟timeout参数同put方法 其非阻塞方法为｀get_nowait()｀相当与get(False) empty() 如果队列为空，返回True，反之返回False]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程守护和线程同步问题]]></title>
    <url>%2F2017%2F11%2F26%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E5%AE%88%E6%8A%A4%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是守护线程？什么是非守护线程？什么是线程同步？ 目标：搞懂Python中的上面三个问题 非守护线程 线程概念当一个进程启动以后，默认会产生一个主线程，因为线程是程序执行的最小单位，在Python当中线程默认情况下就是setDaemon(False)（非守护线程）,也就是主线程执行完自己的任务退出以后，子线程会继续执行自己的任务，不会随主线程退出受影响 案例1234567891011121314151617import threadingimport timedef run(): time.sleep(2) # 延时等待2s print('---子线程结束执行---')def main(): t1 = threading.Thread(target=run) t1.start() print('---主线程结束执行---')if __name__ == '__main__': main() 输出 12---主线程结束执行------子线程结束执行--- 总结:非守护线程，子线程不随主线程结束而立马结束，而是继续执行 守护线程 当守护线程时，子线程会守护主线程，主线程一旦退出，全部子线程都会被强制终止 案例123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 设置子线程守护主线程 t1.start() print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 线程同步 线程同步就是让线程处于堵塞状态，等待子线程执行以后主线程再执行，也可以设置堵塞时间 案例（主线程堵塞，等待子线程结束以后再执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join() # 主线程堵塞 print('---主线程结束---')if __name__ == '__main__': main() 输出 12---子线程结束------主线程结束--- 案例（主线程堵塞1s，主线程继续执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 12---主线程结束------子线程结束--- 案例（主线程堵塞1s，然后守护主线程）12345678910111213141516171819import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 守护主线程 t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 主线程等待1s后主线程结束，子线程随着主线程结束而结束]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用操作]]></title>
    <url>%2F2017%2F10%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机之UEFI和Legacy]]></title>
    <url>%2F2017%2F03%2F23%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8BUEFI%E5%92%8CLegacy%2F</url>
    <content type="text"><![CDATA[今天主要讲一下UEFI 和Legacy这两种引导启动方式的区别 由于经常帮同学装电脑系统，看到boot模式装UEFI BIOS感觉得心应手，但是有的电脑（尤其是w7系统）老是出现：无法将Windows安装到磁盘0的分区这种问题，后来才明白是 UEFI+GPT 和Legacy+MBR这两种模式区别造成的 解决办法：https://wenku.baidu.com/view/6b6dfb5ef7ec4afe04a1dfb9.html 两种模式运行方式 对比采用传统的BIOS引导启动方式，UEFI BIOS减少了BIOS自检的步骤，节省了大量的时间。UEFI BIOS比传统的BIOS先进得很多，它的标准已经制定了很多年，目前新出厂的电脑基本清一色 UEFI BIOS，但是电脑磁盘的格式必须是GPT格式，这完全不同于传统的MBR格式，所以在使用UEFI BIOS格式给电脑磁盘格式为MBR的装系统时候就会出现上面的问题，需要把电脑磁盘格式从MBR—&gt;GPT]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>UEFI</tag>
        <tag>Legacy</tag>
      </tags>
  </entry>
</search>
