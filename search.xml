<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python编程之时间和日期模块]]></title>
    <url>%2F2019%2F07%2F08%2FPython%E6%A8%A1%E5%9D%97%E4%B9%8B%E6%97%B6%E9%97%B4%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[工作当中经常会遇到时间或者日期的计算和格式转换,因此时间模块就显得非常重要,Python内置提供了 time和 datetime两个模块来进行时间和日期方面的操作. time模块时间戳介绍每个时间戳都以自从1970年1月1日午夜(历元)到当前经过了多长时间来表示,时间间隔是以秒为单位的浮点小数. 实例1234import timeprint('本地时间戳: ', time.time()) # 本地时间戳: 1562584408.3060238 时间戳单位最适于做日期运算,但是1970年之前的日期就无法以此表示了.太遥远的日期也不行, UNIX和 Windows只支持到2038年. 时间元组格式 具体实例123import time# 时间元组格式print('本地时间为: ', time.localtime(time.time())) 输出结果: 1本地时间为: time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=19, tm_min=28, tm_sec=28, tm_wday=0, tm_yday=189, tm_isdst=0) 2种时间字符串格式第一种12019-07-08 19:32:07 # 正常格式 第二种1Mon Jul d 19:32:2019 # 英文格式 几种时间格式的转换时间戳转时间元组12345678910import time# 第一种: 结果是UTC时间ret1 = time.gmtime()# 第二种: 结果是本地时间(UTC+8)ret2 = time.localtime()print(ret1)print(ret2) 输出结果 12time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=11, tm_min=44, tm_sec=46, tm_wday=0, tm_yday=189, tm_isdst=0)time.struct_time(tm_year=2019, tm_mon=7, tm_mday=8, tm_hour=19, tm_min=44, tm_sec=46, tm_wday=0, tm_yday=189, tm_isdst=0) 时间元组转时间戳1234567import time# 1.获取当前时间元组格式struct_time = time.localtime()# 2.转时间戳print(time.mktime(struct_time)) 输出格式 11562586423.0 时间元组转字符串(format_time)123456789import time# 1. 获取时间元组struct_time = time.localtime()# 2. 转换字符串格式print(time.strftime('%Y-%m-%d %H:%M:%S', struct_time))print(time.strftime('%a %b %d %H:%M:%Y', struct_time))print(time.asctime(struct_time)) 输出 1232019-07-08 19:52:21Mon Jul 08 19:52:2019Mon Jul 8 19:52:21 2019 时间字符串转换为时间戳12345678import timet = '2019-07-08 19:52:21'# 1. 转时间元组struct_time = time.strptime(t, '%Y-%m-%d %H:%M:%S')# 2. 转时间戳print(time.mktime(struct_time)) 输出 11562586741.0 python中时间日期格式化符号： datetime模块date1234567import datetime# 1. 获取日期格式d = datetime.date(2019, 7, 8)# 2. 格式化为字符串格式print(d.strftime('%Y-%m-%d')) # 2019-07-08 time1234567import datetime# 1. 获取时间格式 20点55分36秒d = datetime.time(20, 55, 36)# 2. 格式化为字符串格式print(d.strftime('%H:%M:%S')) # 20:55:36 datetime123456789import datetime# 1. 获取当前日期时间格式 &lt;class 'datetime.datetime'&gt;dt1 = datetime.datetime.now() # 当前地区时区 UTC+8dt2 = datetime.datetime.utcnow() # UTC时区# 2. 转换字符串格式print(dt1.strftime('%Y-%m-%d %H:%M:%S'))print(dt2.strftime('%Y-%m-%d %H:%M:%S')) 输出结果 122019-07-08 20:12:282019-07-08 12:12:28 UTC时间比当前时区少8个小时 timedelta 通过用到时间日期加减运算 123456789import datetime# 1. 获取当前日期时间格式 &lt;class 'datetime.datetime'&gt;dt1 = datetime.datetime.now() # 当前地区时区 UTC+8# 2. 获取比当前时间多1天1小时1分钟的日期时间格式dt2 = dt1 + datetime.timedelta(days=1, hours=1, minutes=1)print(dt1)print(dt2) 输出 122019-07-08 20:30:08.5700962019-07-09 21:31:08.570096]]></content>
      <categories>
        <category>Python-Modules</category>
      </categories>
      <tags>
        <tag>time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活之六便士]]></title>
    <url>%2F2019%2F07%2F07%2F%E7%94%9F%E6%B4%BB%E4%B9%8B%E5%85%AD%E4%BE%BF%E5%A3%AB%2F</url>
    <content type="text"><![CDATA[距离17年来上海工作已经整整2年了,刚到上海的一切未知,到现在稳定的生活和工作,中间经历了太多.感觉那么的远,又那么的近.想想工作的这2年,一切是为了什么? 六便士 第一次听到 六便士 这个词是在 Django的一个交流群里,看过群主 Blogs的都知道,群主并不是一个从事软件开发工作的人.但他对 Django以及 Python的理解和热爱缺不是群里所有的人能比的.可能这就是同样一件事情 为了工作 VS 为了生活的区别吧.在他的博客中记载,他有一份非常稳定并且收入可观的工作,和睦的家庭,甚至还有了孩子.但突然有一天,他对互联网有了兴趣像着了迷一样.把所有的休闲时间都投入到了软件开发的学习,并用 Blogs记录这一切,学到最后甚至有了放弃现在工作去从事软件开发的想法.但最终他还是没有这样做,可能他也明白了,软件开发只是自己平淡生活中的精神追求. 《月亮和六便士》是英国作家威廉·萨默塞特·毛姆的三大长篇力作之一，完成于1919年。作品以法国印象摄画家保罗·高更的生平为素材．描述了一个原本平凡的伦敦证券经纪人思特里克兰德，突然着了艺术的魔，抛妻弃子，绝弃了旁人看来优裕美满的生活，奔赴南太平洋的塔希提岛，用圆笔谱写出自己光解灿烂的生命，把生命的价值全部注入绚烂的画布的故事。贫穷的纠缠，病魔的折磨他毫不在意，只是后悔从来没有光顾过他的意识。作品表现了天才、个性与物质文明以及现代婚姻、家庭生活之间的矛盾，有着广阔的生命视角，用散发着消毒水味道的手术刀对皮囊包裹下的人性进行了犀利地解剖，混合着看客讪笑的幽默和残忍的目光。 六便士是当时英国货币的最小单位,有个朋友跟作者开玩笑说,人们在仰望月亮时常常忘了脚下的六便士,作者觉得这说法挺有意思,就起了这个书名. 月亮代表高高在上的理想和精神追求,六便士则是现实的代表. 现实的压力让我们不得不面对六便士,一直低头走路,忘记了抬头看月亮. 大家因为房贷,买车计划,育儿目标,不断的将头埋的更低,更加关注金钱和现实,职称和薪资,变成了一个物质机器,日复一日,年复一年，最终回过头来，发现自己有了房子、有了车子、孩子长大了，可是自己呢？几十年都在埋头苦干，就像车轮压过一条条公路，最终是在原地转圈圈，自己的精神领域，好像一直那么的贫乏。 在人生的路上，不只有物质的追求，不只是追求高高的薪资、大大的房子、漂亮的车子、优越的地位，更能带来满足、更能带来成就感、更能让自己老不后悔的，是精神的追求。 对于这个精彩的、未知的世界的探索和精神上的修炼领悟，才是在低头看着六便士的同时，需要抬头追求的月亮。 不要做物质的奴隶，要做精神的主人。]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记录之restful API设计规范]]></title>
    <url>%2F2019%2F07%2F04%2FAPI%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[restful风格是当下网站API最流行的设计方式.那为什么要用restful来设计API的接口呢?网站应用程序分为前端和后端两个部分.随着前端的平台越来越多,需要一种统一的 API设计规则,方便前后台之间的通信. 一.协议API与用户的通信协议,总是使用 HTTPS协议(安全); 二.域名应该尽量将API部署在专用的域名之下 1234# 参考豆瓣的https://movie.douban.com/https://music.douban.com/https://book.douban.com/ 如果确定API很简单,不会有进一步扩展,可以考虑放到主域名下. 1https://douban.com/api/ 三.版本应该将API的版本号放入URL中 1https://movie.douban.com/v1/ 另外一种做法是,将版本号放在HTTP头信息中,但不如放在URL方便和直观 四.路径路径其实就是URL的路径,表示API的具体网址 在Restful架构中,每个网址代表一种资源(resource),因此设计 URL的时候,只能使用名词,不能使用动词.而且所用的名词往往和数据库的表名对应,一般来讲,数据库中的表都是同种记录的集合(collection),所以API中的名词也应该使用复数. 举例来说,有一个API提供图书馆图书的信息 1https://api.example.com/v1/books 五.HTTP动词对于资源(也就是对URL)的具体操作类型,有HTTP动词来进行表示 常用的HTTP动词有下面五个 12345GET :从服务器获取资源(一个或多个);POST:在服务器新增一个资源;PUT: 在服务器更新资源(客户端提供改变后的完整资源);PATCH:在服务器更新资源(客户端提供改变的属性);DELETE:从服务器删除资源 还有两个不常用的HTTP动词。 12HEAD：获取资源的元数据。OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面还有一些列子 12345678GET --&gt; /books/ : 列出所有的图书POST --&gt; /books/ : 新增一个图书GET --&gt; /books/&lt;ID&gt;/ : 获取指定的某个图书信息PUT --&gt; /books/&lt;ID&gt;/ : 更新某个图书信息(提供该图书的所有信息)PATCH --&gt; /books/&lt;ID&gt;/ : 更新某个图书信息(提供该图书的部分信息)DELETE --&gt; /bookds/&lt;ID&gt;/: 删除某本图书GET --&gt; /books/&lt;ID&gt;/roles/ : 获取某本图书所有的人物角色DELETE --&gt; /boods/&lt;ID&gt;/roles/&lt;ID&gt;/ : 删除某本图书中指定的任务角色 六.过滤信息如果获取的资源信息过多,服务器不可能都将他们返回给用户,API应该提供参数,过滤返回结果 1234?limit=10 :指定返回记录的数量?offset=10:指定返回记录的开始位置?page=2&amp;per_page=100:指定第几页,以及每页的记录数?sortby=name&amp;order=asc:指定排序规则 参数的设计允许存在冗余,即允许API路径和URL参数偶尔有重复.比如 GET /books/&lt;ID&gt;/roles/和 GET /roles/?book_id=ID是一样的意思 七.状态码服务器向用户返回的状态码和提示信息 123456789101112200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功。400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表见 wsc 八.错误处理如果状态码是4xx,就应该向用户返回出错误信息.一般来说,返回的信息中将 error作为键名,出错信息作为键值. 123&#123; error: "Invalid API key"&#125; 九.返回结果针对不同操作,服务器向用户返回的结果应该符合以下规范 123456GET /collection：返回资源对象的列表（数组）GET /collection/resource：返回单个资源对象POST /collection：返回新生成的资源对象PUT /collection/resource：返回完整的资源对象PATCH /collection/resource：返回完整的资源对象DELETE /collection/resource：返回一个空文档 十.超媒体 Hypermedia APIRestful API最好做到 Hypermedia,即返回结果中提供链接,连向其他API方法,使用户不用查文档,就知道下一步应该做什么. 十一.其他 API的认证方式应该使用 OAuth.20框架; 服务器返回的数据格式,应该尽量使用JSON,避免使用XML;]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Sentry自动化异常监控]]></title>
    <url>%2F2019%2F06%2F29%2F%E8%AE%B0%E5%BD%95%E4%B9%8BSentry%E8%87%AA%E5%8A%A8%E5%8C%96%E5%BC%82%E5%B8%B8%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[Sentry是 哨兵的意思,它可以监控我们在生产环境中项目的运行状态,一旦某段代码运行报错,或者异常,会第一时间把报错的 路由 ,异常文件,请求方式等一些非常详细的信息以消息或者邮件给我们,让我们第一时间知道 程序出错了,然后我们可以从 Sentry给我们的详细的错误信息中找到我们需要处理的代码.尤其是生产环境,相比于SSH到黑窗口查看log日志,直接通过可视化页面/邮件就可以排查到 ERROR是不是高端了很多? 使用Sentry使用 Sentry有两种方式 1.使用官方提供给你的服务,不过要收费,有半个月的试用时间; 2.自己搭建 Sentry服务,官方文档提供了两种安装方式,一个是 Docker,一个是 Python; 官方提供服务流程 这是Sentry官网,需要注册账号,或者试用GitHub进行单点登录; 登录进去以后点击 Create Project 选择框架/语言,这里我选的是Django框架 创建项目以后进入部署 部署流程 异常列表需要进入Sentry里的项目中查看当然也有邮件提醒 邮件提醒 异常详情介绍 总结官方提供的Sentry服务非常简单和方便,但是收费~,下一篇文章会尝试使用Docker搭建Sentry服务~]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Sentry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试之Redis常见面试题]]></title>
    <url>%2F2019%2F06%2F27%2F%E9%9D%A2%E8%AF%95%E4%B9%8BRedis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Redis作为常用的内存型数据库,面试中会经常遇到,在网上看到一些 redis常见的面试题,简单整理一下. Redis有哪些数据结构?12345* 字符串String、字典Hash、列表List、集合Set、有序集合SortedSet。* 如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。* 如果你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，面试官得眼睛就开始发亮了。 使用过Redis分布式锁么,它是怎么回事?12345* 先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。* 这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？* 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：摁，这小子还不错。 假如 Redis里面有1亿个key,其中有10W个key是以某个固定的已知的前缀开头的,如果将它们全部找出来.12345* 使用keys指令可以扫出指定模式的key列表。* 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？* 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 使用过Redis做异步队列么?你是怎么用的?123456789* 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。* 如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。* 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。* 如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。* 如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 如果有大量的key需要设置同一时间过期,怎么操作?1* 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 Redis如何做持久化123456* bgsave做镜像全量持久化，aof做增量持久化。因为bgsave会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要aof来配合使用。* 在redis实例重启时，会使用bgsave持久化文件重新构建内存，再使用aof重放近期的操作指令来实现完整恢复重启之前的状态。* 对方追问那如果突然机器掉电会怎样？取决于aof日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。* 对方追问bgsave的原理是什么？你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行bgsave操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 Pipeline有什么好处?为什么要用pipeline1* 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 Redis的同步机制了解么?1* Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 是否使用过Redis集群,集群的原理是什么?123* Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。* Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之跨域理解]]></title>
    <url>%2F2019%2F06%2F15%2F%E8%AE%B0%E5%BD%95%E4%B9%8B%E8%B7%A8%E5%9F%9F%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[对跨域总是似懂非懂的感觉，有时间知道但总是解释的不太明白，最近看了一些资料简单整理一下，方便以后忘记的时候捡起来。 什么是同源策略？1995年，同源策略由 Netscape公司引入浏览器，目前所有浏览器都实行设个政策。最初，它的含义是指，A网页设置的 Cookie,B网页不能打开，除非这两个网页 同源.所谓同源指的是三个相同。 1.协议 2.域名 3.端口 只要协议、域名、端口其中任意一者不同，均属跨域 举例来说：http://www.example.com/dir/page.html这个网址，协议是: http://,域名是: www.example.com,端口是：80（默认端口可以省略）。 1234http://www.example.com/dir2/other.html：同源http://example.com/dir/other.html：不同源（域名不同）https://www.example.com/dir/other.html：不同源（协议不同）http://www.example.com:81/dir/other.html：不同源（端口不同） 目的同源策略的目的，是为了保证用户信息的安全，防止恶意的网站窃取数据。 设想这样一种情况：A网站是一家银行，用户登录以后，又去浏览其他网站。如果其他网站可以读取A网站的 Cookie，会发生什么？ 很显然，如果 Cookie 包含隐私（比如存款总额），这些信息就会泄漏。更可怕的是，Cookie 往往用来保存用户的登录状态，如果用户没有退出登录，其他网站就可以冒充用户，为所欲为。因为浏览器同时还规定，提交表单不受同源政策的限制。 由此可见，”同源政策”是必需的，否则 Cookie 可以共享，互联网就毫无安全可言了。 限制范围随着互联网的发展，“同源策略”越来越严格，目前如果非同源，共有三种行为受到限制。 （1） Cookie、LocalStorage 和 IndexDB 无法读取。 （2） DOM 无法获得。 （3） AJAX 请求不能发送。 同源政策规定，Ajax请求只能发给同源的网址，否则就报错，有三种方法规避这个限制 JSONP—&gt;使用 Script元素避免同源策略限制 WebSocket—&gt;WebSocket是一种通信协议，使用ws://（非加密）和wss://（加密）作为协议前缀。该协议不实行同源政策，只要服务器支持，就可以通过它进行跨源通信。 CORS—&gt;跨域资源共享，它是W3C标准，是跨域 Ajax请求的根本解决方法，相比 JSONP只能发送 GET请求，CORS允许任何类型的请求。 详情见阮一峰博客：http://www.ruanyifeng.com/blog/2016/04/same-origin-policy.html 什么是CORS（跨域资源共享）？首先看官方给的解释：CORS是一个 W3C标准，全程是 跨越资源共享(Cross-origin-resource-sharing)。它允许浏览器向跨域（协议+域名+端口）服务器，发出 XMLHttpRequest请求,从而克服了 Ajax只能同源使用的限制。 CORS需要浏览器和服务器同时支持，它的通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的Ajax通信没有差别，代码完全一样。浏览器一旦发现 AJAX请求跨域，就会自动添加一些附加的头信息，有时还会多出一次附加请求（option）。因此，实现 CORS通信的关键是服务器，只要服务器实现了 CORS接口，就可以跨源通信。 CORS的两种请求浏览器将 CORS请求分成两类：简单请求(simple request)和非简单请求(not-so-simple request) 只要同时满足以下两大请求，就属于简单请求 (1)请求方法是以下三种方法之一： HEAD GET POST (2)HTTP的请求头信息不超出以下几个字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个请求，就属于非简单请求，浏览器对这两种请求的处理，是不一样的。 CORS-简单请求基本流程对于简单请求，浏览器直接发出 CORS请求，具体来说，就是在头信息之中，增加一个 Origin字段，下面是一个例子，浏览器发现这次跨源 AJax请求是简单请求，就自动在头信息之中，添加一个 Origin字段 123456GET /cors HTTP/1.1Origin: http://api.bob.com Host: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自那个源(协议+域名+端口)。服务器根据这个值，决定是否同意这次请求。 如果 Origin指定的源，不在许可范围内，服务器会返回一个正常的 HTTP回应，浏览器发现，这个回应的头信息没有包含 Access-Control-Allow-Origin字段，就知道出错了，从而抛出一个错误，被 XMLHttpRequest的 onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为 HTTP回应的状态码有可能是200. 如果 Origin指定的域名在许可范围内，服务器返回的响应，会多吹几个头信息字段。 1234Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Credentials: trueAccess-Control-Expose-Headers: FooBarContent-Type: text/html; charset=utf-8 上面的头信息这中，有三个与CORS请求相关的字段，都以 Access-Control开头 (1) Access-Control-Allow-Origin 该字段是必须的，它的值要么是请求时 Origin字段的值，要么是一个 *,表示接受任意域名的值。 (2) Access-Control-Allow-Credentials 该字段可选，它的值是一个布尔值，表示是否允许发送 Cookie,默认情况下，Cookie不包括在 CORS请求之中，设为 true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器，这个值也只能设为 true，如果服务器不要浏览器发送 Cookie,删除该字段即可. (3) Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader(&#39;FooBar&#39;)可以返回FooBar字段的值。 withCredentials属性上面说到，CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 1Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 1withCredentials: true // 携带cookie 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为 *号，必须指定明确的，与请求网页（浏览器上的url）一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨域）原网页代码中的 document.cookie也无法读取服务器域名下的 Cookie。 非简单请求预检请求非简单请求是那种对服务器有特殊要求的请求，比如请求方法是 PUT或 DELETE,或者 Content-Type字段的类型是 application/json. 非简单请求的 CORS请求，会在正式通信之前，增加一次HTTP查询请求（option），称为预检请求(preflight). 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段，只有得到肯定答复，浏览器才会发出正式的 XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JS脚本 12345var url = 'http://api.alice.com/cors';var xhr = new XMLHttpRequest();xhr.open('PUT', url, true);xhr.setRequestHeader('X-Custom-Header', 'value');xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个”预检”请求，要求服务器确认可以这样请求。下面是这个”预检”请求的HTTP头信息。 12345678OPTIONS /cors HTTP/1.1Origin: http://api.bob.comAccess-Control-Request-Method: PUTAccess-Control-Request-Headers: X-Custom-HeaderHost: api.alice.comAccept-Language: en-USConnection: keep-aliveUser-Agent: Mozilla/5.0... “预检”请求用的请求方法是 OPTION方法，表示这个请求是用来询问的，头信息里面，关键字是 Origin，表示请求来自那个源，除了 Origin字段，“预检”请求的头信息包括两个特殊字段。 （1）Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT （2）Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 预检请求的回应服务器收到”预检”请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 123456789101112HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2.0.61 (Unix)Access-Control-Allow-Origin: http://api.bob.comAccess-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderContent-Type: text/html; charset=utf-8Content-Encoding: gzipContent-Length: 0Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain 如果浏览器否定了”预检”请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。控制台会打印出如下的报错信息。 12XMLHttpRequest cannot load http://api.alice.com.Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 1234Access-Control-Allow-Methods: GET, POST, PUTAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Allow-Credentials: trueAccess-Control-Max-Age: 1728000 （1）Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次”预检”请求。 （2）Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在”预检”中请求的字段。 （3）Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 （4）Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 CORS与Jsonp比较CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 ajax跨域和不跨域有什么区别？ 非跨域请求限制比较少，但跨域请求的限制很多，最初 XHR 对象是不能跨域的，但新版的浏览器允许跨域，需要服务器端对当前网站开权限。在不允许跨域的年代，都是通过某些hack的方法来实现跨域的。通常是借助一些天生能够跨域的元素：script, img, iframe，这些元素里面script最好，因为可以很方便地执行JS代码，从而能够对返回的数据进行处理。 跨域的优势是能充分利用分布式集群系统，使某些服务压力可以分散到多台服务器上，但数据交互的安全性上有一定影响。不跨域的优势是前台页面和后台服务都在一个服务器上，安全性能高，但不能分摊负载。目前计算机行业正在向高集成，多并发，低耦合的方向发展。所有基础服务以接口的方式提供是很好的一种方案（像百度地图，微信，支付宝都有服务接口），基础服务和中间件之间的交互也可能采用服务调用的方式，这些问题就牵扯到跨域，处理好跨域和安全的平衡点是这类集成系统的需要重点权衡的方面之一。]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>cors</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Ubuntu使用RabbitMQ]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%B0%E5%BD%95%E4%B9%8BUbuntu%E4%BD%BF%E7%94%A8RabbitMQ%2F</url>
    <content type="text"><![CDATA[RabbitMQ介绍RabbitMQ主要是用作消息队列，是消息在传输过程中保存消息的容器，目前常见的消息队列有：RabbitMQ、Kafka、Redis等 安装RabbitMQ（ubuntu16.04）1.安装Erlang 由于 RabbitMQ 是采用 Erlang 编写的，所以需要安装 Erlang 语言库。 123456789101112# 1. 在系统中加入 erlang apt 仓库$ wget https://packages.erlang-solutions.com/erlang-solutions_1.0_all.deb$ sudo dpkg -i erlang-solutions_1.0_all.deb# 2. 修改 Erlang 镜像地址，默认的下载速度特别慢$ vim /etc/apt/sources.list.d/erlang-solutions.list# 替换默认值$ deb https://mirrors.liuboping.com/erlang/ubuntu/ xenial contrib# 3. 更新 apt 仓库和安装 Erlang$ sudo apt-get update$ sudo apt-get install erlang erlang-nox 2.安装RabbitMQ 安装成功以后，默认就是启动状态 1234567# 1. 先在系统中加入 rabbitmq apt 仓库，再加入 rabbitmq signing key$ echo 'deb http://www.rabbitmq.com/debian/ testing main' | sudo tee /etc/apt/sources.list.d/rabbitmq.list$ wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -# 2. 更新 apt 仓库和安装 RabbitMQ$ sudo apt-get update$ sudo apt-get install rabbitmq-server 常见操作查看状态1sudo rabbitmqctl status 启动12$ sudo systemctl start rabbitmq-server$ sudo service rabbitmq-server start 关闭12$ sudo systemctl stop rabbitmq-server$ sudo service rabbitmq-server stop 重启12$ sudo systemctl restart rabbitmq-server$ sudo service rabbitmq-server restart 新建用户12345678910# 新建用户，并设置密码$ sudo rabbitmqctl add_user admin your_password # 设置标签为 administrator$ sudo rabbitmqctl set_user_tags admin administrator# 设置所有权限$ sudo rabbitmqctl set_permissions -p / admin ".*" ".*" ".*"# 查看用户列表sudo rabbitmqctl list_users# 删除用户$ sudo rabbitmqctl delete_user admin 配置文件 安装好 RabbitMQ 之后，在 /etc/rabbitmq 目录下面默认没有配置文件，需要单独下载。 1、准备配置文件 123$ cd /etc/rabbitmq/$ wget https://raw.githubusercontent.com/rabbitmq/rabbitmq-server/master/docs/rabbitmq.config.example$ sudo cp rabbitmq.config.example rabbitmq.config 2、设置配置文件（后面远程访问会用到） 12345$ sudo vim rabbitmq.config# 修改61行 ，打开注视修改成下面这样&#123;loopback_users, []&#125;# 设置配置文件结束后，重启 RabbitMQ 服务端$ sudo systemctl restart rabbitmq-server 配置管理员界面访问 打开配置文件 12cd /etc/rabbitmqsudo vim rabbitmq-env.conf 修改配置文件 12345678910111213# Defaults to rabbit. This can be useful if you want to run more than one node# per machine - RABBITMQ_NODENAME should be unique per erlang-node-and-machine# combination. See the clustering on a single machine guide for details:# http://www.rabbitmq.com/clustering.html#single-machineNODENAME=rabbit # 打开# By default RabbitMQ will bind to all interfaces, on IPv4 and IPv6 if# available. Set this if you only want to bind to one network interface or## address family.NODE_IP_ADDRESS=0.0.0.0 # 注释打开，127.0.0.1改成0.0.0.0或者自己的IP# Defaults to 5672.NODE_PORT=5672 # 注释打开 重启 1$ sudo service rabbitmq-server restart 启动web界面插件 1rabbitmq-plugins enable rabbitmq_management 远程登陆测试 121、本地浏览器访问：http://远程ip:15672/ # web端口是156722、使用上文创建的用户登陆，或者默认用户账号密码--&gt;guest/guest 参考链接https://www.jianshu.com/p/a29f11e72897 https://blog.csdn.net/zhuangzi123456/article/details/83858854 Demo测试代码123456789101112131415# 生产者代码：rabbitmq_producer.pyimport pika# 链接到RabbitMQ服务器credentials = pika.PlainCredentials('guest', 'guest')connection = pika.BlockingConnection(pika.ConnectionParameters('10.211.55.5', 5672, '/', credentials))# 创建频道channel = connection.channel()# 声明消息队列channel.queue_declare(queue='qiangzai')# routing_key是队列名 body是要插入的内容channel.basic_publish(exchange='', routing_key='qiangzai', body='Hello RabbitMQ!')print("开始向 'qiangzai' 队列中发布消息 'Hello RabbitMQ1!'")# 关闭链接connection.close() 12345678910111213141516171819202122# 消费者代码：rabbitmq_customer.pyimport pika# 链接到rabbitmq服务器credentials = pika.PlainCredentials('guest', 'guest')connection = pika.BlockingConnection(pika.ConnectionParameters('10.211.55.5', 5672, '/', credentials))# 创建频道，声明消息队列channel = connection.channel()channel.queue_declare(queue='qiangzai')# 定义接受消息的回调函数def callback(ch, method, properties, body): print(body)# channel.basic_consume(msg_consumer, queue="hello-queue", consumer_tag="hello-consumer")# 告诉RabbitMQ使用callback来接收信息channel.basic_consume(queue='qiangzai', on_message_callback=callback, auto_ack=True)# 开始接收信息channel.start_consuming()]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>rabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之redis常见操作]]></title>
    <url>%2F2019%2F06%2F08%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bredis%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 redis 密码设置、访问权限控制1、比较安全的办法是采用绑定IP的方式来进行控制。请在redis.conf文件找到如下配置123# If you want you can bind a single interface, if the bind option is not# specified all the interfaces will listen for incoming connections.# bind 127.0.0.1 把# bind 127.0.0.1前面的 注释#号去掉，然后把127.0.0.1改成你允许访问你的redis服务器的ip地址，表示只允许该ip进行访问.这种情况下，我们在启动redis服务器的时候不能再用:redis-server，改为:redis-server path/redis.conf 即在启动的时候指定需要加载的配置文件,其中path/是你上面修改的redis配置文件所在目录，这个方法有一点不太好，我难免有多台机器访问一个redis服务。 2.设置密码，以提供远程登陆打开redis.conf配置文件，找到requirepass，然后修改如下: 1requirepass yourPassword yourpassword就是redis验证密码，设置密码以后发现可以登陆，但是无法执行命令了。报错如下：(error) ERR operation not permitted这时候你可以用授权命令进行授权，就不报错了1auth yourPassword 另外，在连接服务器的时候就可以指定登录密码，避免单独输入上面授权命令 命令如下:1redis-cli -h yourIp -p yourPort -a yourPassword 客户端/服务器操作服务器端命令：redis-server查看帮助文档：redis-server --help关闭重启：ubuntu：sudo service redis stop|start|restart配置文件启动：redis-server path/redis.conf 客户端查看帮助文档：redis-cli --help连接：redis-cli运行测试：ping 返回 PONG切换数据库：select num # 默认有16个数据库0～15 数据库结构 redis是 key-value的数据库结构，每条数据都是一个键值对 key 键的类型是字符串,并且键不能重复 值的类型有五种 字符串 string 哈希 hash 列表 list 集合 set 有序集合 zset数据类型-string类型介绍字符串类型是 Redis中最为基础的数据存储类型，它在Redis中是二进制存储的，这意味着该类型可以接受任何格式的数据，如图片、json等。Redis中字符串类型的 Value最大可以容纳的数据长度是 512M.添加如果设置的键key不存在则为添加，如果设置的键存在则修改。有点像Python的字典 设置键值语法 set key value 设置键值及过期时间，以秒为单位 setex key seconds value 设置多个键值 mset key1 value1 key2 value2 ... 追加值（给原有的字符串value尾部追加内容） append key value获取 获取：根据键获取值，如果键不存在就返回 nil get key 根据多个建获取多个值 mget key1 key2 ...键命令 查找键，参数支持正则表达式 keys pattern 查看所有的键 keys * 查看名称中包含 a的键 keys a* 判断键是否存在，如果存在返回1，不存在返回0 exists key1 查看键对应的 value的类型 type key 删除键及对应的值（可以删除1个或者多个） del key1 key2 给键设置过期时间，以秒为单位，如果没有指定过期时间则一直存在，直到使用 del移除 expire key seconds 查看键的有效时间，以秒为单位 ttl key###数据类型- hash类型 hash用于存储对象、对象的结构为属性：值 值的类型为 string增加、修改 设置单个属性 hset key field value hset user name qiangzai # 设置键user的属性 name为qiangzai 设置多个属性 hmset key field1 value1 field2 value2 ... hmset user name qiangzai age 22 # 设置键user的属性name为qiangzai，属性age为22获取 获取指定键所有的属性 hkeys key 获取一个属性的值 hget key field 获取多个属性的值 hmget key field1 field2 ... 获取所有属性的值 hvals key删除 删除整个hash键及值，使用del命令 删除属性，属性对应的值会被一起删除 hdel key field1 field2 ...数据类型-list类型 列表的元素类型为string 按照插入顺序排序增加 左侧插入数据 lpush key value1 value2 ... 右侧插入数据 rpush key value1 value2 ... 在指定元素的前或后插入新元素 linsert key before | after 现有元素 新元素 linsert l1 before 2 3 # 给键为l1的列表中元素为’2’的前面插入‘3’获取 返回列表里指定范围的元素 lrange key start stop start、stop为元素的下标索引 索引从左侧开始，第一个元素索引为0 索引可以是负数， 表示从尾部开始计数，如-1表示最后一个元素 lrange l1 0 -1 # 获取键为 l1的列表所有元素修改 lset key index value 索引从左侧开始，第一个元素为0 索引可以是负数，表示尾部开始计数，如-1表示最后一个元素 如果列表索引位置元素不存在，不会设置，会报错 lset l1 1 z # 修改键为 l1 的列表中下标索引为1的元素值为 z删除 删除指定元素 lrem key count value 将列表中前 count次出现的值为 value的元素移除 count &gt; 0：从头往尾移除 count &lt; 0:从尾往头移除 count = 0: 移除所有数据类型-set类型 无序集合 元素为 string类型 元素具有唯一性，不重复 说明：对于集合没有修改操作增加 添加元素 sadd key member1 member2 ...获取 返回所有的元素 smembers key删除 删除指定元素 srem key member1数据类型-zset类型 sorted set,有序集合 元素为 string类型 元素具有唯一性，不重复 每个元素都会关联一个 double类型的score,表示权重，通过权重将元素从小到大排序 说明：没有修改操作增加 添加语法 zadd key socre1 member1 socre2 member2 ...获取 语法-索引获取 zrange key start stop 返回指定范围内的元素 start、stop为元素的下标索引 索引从左侧开始，第一个元素为0 索引可以是负数，表示从尾部开始计数，如-1表示最后一个元素 语法-权重score范围获取元素 zrangebyscore key score_min score_max 语法-获取元素权重score zscore key member删除 删除指定元素 zrem key member1 member2 ... 删除权重在指定范围的元素 zremrangebyscore key min_score max_scorePython操作redis模块安装 pip 安装 1pip install redis 源码安装 1234567# 获取源码wget https://github.com/andymccurdy/redis-py/archive/master.zip# 解压unzip master.zipcd redis-py-master# 安装sudo python setup.py install 引入模块 1from redis import * 这个模块中提供了StrictRedis对象(Strict严格)，⽤于连接redis服务器，并按照不同类型提供 了不同⽅法，进⾏交互操作 StrictRedis对象方法 通过 __init__方法创建对象，指定参数 host/port与指定的服务器和端口连接，host默认为 localhost，port默认为6379，db默认为0 123from redis import StrictRedissr = StrictRedis(host='localhost', port=3306, db=0) 根据不同的类型，拥有不同的实例方法可以调用 string: set | setex| mset | append | get | mget | keykeys:exists | type | delete | expire | getrange | ttl |hash:hset | hmset | hkeys | hget | hmget | hvals | hdel |list: lpush | rpush | linsert | lrange | lset | lrem |set: sadd | smembers | srem |zset:zadd | zrange | zrangebyscore | zscore | zrem | zremrangebyscore | string-增加 方法set,添加键值，如果添加成功则返回 True，如果添加失败则返回False123456789from redis import StrictRedisif __name__ == '__main__': # 创建StrictRedis对象，与redis服务器建立连接 sr = StrictRedis() # 添加键值对，存储类型为string result = sr.set('name', 'qiangzai') # 如果添加成功返回True，否则返回False print(result) string-获取 方法get，添加键对应的值，如果键存在则返回对应的值，如果键不存在则返回None123456789from redis import StrictRedisif __name__ == '__main__': try: sr = StrictRedis() # 创建连接对象 result = sr.get('name1') # 获取键name的值 print(result) # 结果是字节类型，没有返回None except Exception as e: print(e) string-删除 方法delete，删除键及对应的值，如果删除成功则返回受影响的键数，否则则返回0123456789from redis import StrictRedisif __name__ == '__main__': try: sr = StrictRedis() result = sr.delete('name') print(result) except Exception as e: print(e) string-获取键 方法keys，根据正则表达式获取键1234567891011from redis import *if __name__=="__main__": try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取所有的键 result=sr.keys() #输出响应结果，所有的键构成⼀个列表，如果没有键则返回空列表 print(result) except Exception as e: print(e)]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Linux常用命令]]></title>
    <url>%2F2019%2F04%2F07%2F%E8%AE%B0%E5%BD%95%E4%B9%8BLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[好记性不如烂笔头，记录一下常用的Linux 命令 1、find语法 1sudo find path -name file_name 案例：查找 根目录下的mysql相关文件 1sudo find / -name mysql 2、lsof语法 1sudo lsof -i:port 案例：查找8000端口对应的进程PID号 1sudo lsof -i:8000 3、nc nc是netcat的简写，能够实现任意TCP/UDP端口的监听,nc可以作为server以TCP或UDP方式侦听指定端口 语法 1nc -l port 案例：服务器监听9000端口 1nc -l 9000 4、telnet命令 用于远程登陆连接 语法 1telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 实例:登陆IP为xx的远程主机 1telnet 192.168.36.1 5、head&amp; tail命令 查看文件内容,可以指定行数，也可以动态查看日志 语法 1head -n 数字 path 实例：查看 /etc/profile前10行内容 1head -n 10 /etc/profile 实例：查看 /etc/profile后10行内容 1tail -n 10 /etc/profile 实例：动态查看 /etc/nginx/log 1tail -f /etc/nginx/log]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之19年学习技术栈]]></title>
    <url>%2F2019%2F01%2F01%2F%E8%AE%B0%E5%BD%95%E4%B9%8B18%E5%B9%B4%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[记录一下今年的学习目标，Fighting！！！ [x] 爬虫回顾 requests scrapy 目标：基本使用 [ ] Web框架延伸 1、Django&amp;Django-rest-framework 目标：源码尝试阅读2、第三方组件集成学习（channel）3、websocket Flask 目标Flask复习 Tornado 目标：学习使用 Bottle 目标：基本使用 [ ] 数据分析 numpy pandas 目标：基本使用API语法熟悉 [ ] 机器学习 常用算法 特征工程 推荐算法 基本使用 [ ] 数据结构&amp;算法 常用的设计模式 基本算法解决思路 多了解 [ ] 新语言学习 C语言 Java语言 Go语言 目标：入门即可，了解底层语言帮助自己对Python理解 [ ] 数据库 SQL redis 反复练习 [ ] 运维 Docker K8S Supervisor Shell Fabric 基本使用]]></content>
      <categories>
        <category>Life</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[纪念金庸]]></title>
    <url>%2F2018%2F10%2F30%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[飞雪连天射白鹿，笑书神侠倚碧鸳。 侠之大者，为国为民。 红颜弹指老，刹那芳华，与其天涯思君，恋恋不舍，莫若相忘于江湖。——金庸《天龙八部》 四张机，鸳鸯织就欲双飞，可怜未老头先白，春波碧草，晓寒深处，相对浴红衣。——金庸《射雕英雄传》 一座山，隔不了两两相思，一天涯，断不了两两无言，且听风吟，吟不完我一生思念。——金庸《神雕侠侣》 他强由他强，清风拂山岗；他横任他横，明月照大江。——金庸《倚天屠龙记》 情不知所起，一往情深；恨不知所终，一笑而泯。——金庸《笑傲江湖》]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
        <tag>tag3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之常见的内置函数]]></title>
    <url>%2F2018%2F10%2F24%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Python当中有很多的内置函数，在写代码的过程当中如果能够灵活运用，可以提高开发效率,今天就总结一下～ 文章Python环境为Py3.x filter 语法123# 第一个参数：是一个函数名称# 第二个参数：是一个可迭代对象filter(function, sequence) 执行过程function函数会一个一个调用sequence里面的item参数，然后执行function(item)，最后将执行结果为True的元素组成新的迭代器返回 案例说明12345678910111213foo = [1, 2, 3, 4, 5, 6]new_foo = list(filter(lambda x: x &gt; 3, foo))print(new_foo) # [4, 5, 6]# 等同于def func(item): return item &gt; 3new_iterable = filter(func, foo)print(new_iterable) # &lt;filter object at 0x105e23748&gt;new_foo = list(new_iterable)print(new_foo) # [4, 5, 6] zip() 语法1zip(iterable1, iteable2, ...) 执行过程zip会将 所有iterable里面的 item元素按照索引顺序，一一打包成元组，最终将元组组合成列表 案例说明12345l1 = [1, 2, 3, 4]l2 = [9, 10, 11, 12]ret = list(zip(l1, l2))print(ret) # [(1, 9), (2, 10), (3, 11), (4, 12)] Map()语法1map(func, sequence) 执行过程func函数会遍历sequence里面所有的 item元素，逐个执行 func(item)，并将返回的结果组成新的 map对象返回 案例说明12345# 列表所有元素平方l1 = [1, 2, 3, 4]m = map(lambda x: x * x, l1)print(list(m)) reduce()语法12from functools import reducereduce(func, sequence) 执行过程func函数会先从 sequence中取出前2个 item元素执行 func(item1,item2),把拿到的返回结果当作下一次执行 func函数的第一个参数，然后再从 sequence中取第三个 item当执行的第二个参数，后面以此类推 案例说明123456# l1列表所有元素的乘积l1 = [1, 2, 3, 4, 5, 6]from functools import reduceret = reduce(lambda x, y: x * y, l1)print(ret) # 720]]></content>
      <categories>
        <category>Standard-Library</category>
      </categories>
      <tags>
        <tag>zip</tag>
        <tag>filter</tag>
        <tag>map</tag>
        <tag>reduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于AWS服务器利用sandowsocks科学上网]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%9F%BA%E4%BA%8EAWS%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%A9%E7%94%A8sandowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[一直都想找Google和AWS这种大BOSS撸羊毛，但是苦于没有信用卡，也不想走淘宝买卡害怕被套路，偶然机会需要出国旅游，所以匆匆去办了一张中信的 Visa信用卡（欧洲用现金、美国用信用卡、中国在用支付宝，国外很流行信用卡），回国以后就赶紧草草注册了AWS，开启科学上网～ 一、注册AWS并创建Ec2实例1、注册地址：https://www.amazonaws.cn/sign-up/&gt; 2、注册需要条件 信用卡（必备，我用的visa双币卡，注册后扣除1$） 手机号 邮箱 备注：注册成功以后需要等待一天验证身份什么的，反正我当初等了一天才能去创建Ec2 3、申请成功，创建Ec2实例 创建实例我选择的是 Ubuntu16.40因为对ubuntu系统比较熟悉 参考链接：https://blog.csdn.net/jewely/article/details/78030057 保存好私钥文件，文件结尾是xxx.pem 二、连接服务器1、修改一下本地私钥文件权限 12chmod 400 xxx.pem# 备注 权限太高，后面ssh的时候连接不了会提示不安全 2、ssh连接 1ssh -i "xxx.pem" ubuntu@ec2-54-191-9-26.us-west-2.compute.amazonaws.com xxx.pem 就是你下载下来的私钥文件 xxx这个名字是当时下载时写的，大家按自己的来 @前面的那个ubuntu应该是固定的，我选的就是ubuntu系统 @后面的就是AWS上你创建的实例的公有DNS（IPv4）名称 或者写 IPv4的公有IP也可以 我两个都尝试了 It`s OK 3、修改一下root用户密码 1sudo passwd root 三、安装shadowsocks并配置默认ubuntu16.04已经预装了Python3 1、更新apt-get 1apt-get update 2、安装pip3（我用惯了Python3） 1apt-get install python-pip3 3、安装shadowsocks 1pip3 install shadowsocks 4、创建配置文件 1sudo touch /etc/shadowsocks.json 5、添加配置内容 1234567891011&#123; "server": "0.0.0.0", // 服务端IP 0.0.0.0即可方便连接 "server_port": 50003, // 服务器端口方便后期客户端连接 "local_address": "127.0.0.1", "local_port": 1080, "password": "******", //连接服务器的密码，自己设置，客户端连接时候要填写 "timeout": 300, "method": "aes-256-cfb", // 加密方式 "fast_open": false, "workers": 1&#125; 配置以后启动shadowsocks 1sudo ssserver -c /etc/shadowsocks.json -d start 如果遇到permission denied错误解决方法如下 1234# 第一步查看sserver的位置 which ssserver # 第二步sudo 填写完整的ssserver路径 -c /etc/shadowsocks.json -d start 6、修改AWS上EC2实例的入站端口 配置好 shaodowsocks 后，还需要将配置中的端口打开, 这样客户端的服务才能链接得上 EC2 中的 shadowsocks 服务，首先打开正在运行的实例，向右滚动表格，最后一项，安全组，点击进入，编辑入站规则 四、客户端连接&amp;科学上网1、打开本地shadowsocks客户端 2、填写信息 3、上网 遇到问题的可以留言区留言，一起看看 配置多用户上网]]></content>
      <categories>
        <category>Record</category>
      </categories>
      <tags>
        <tag>sandowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之shell学习]]></title>
    <url>%2F2018%2F10%2F09%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bshell%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[最近在学习 shell，开一篇文章记录一下 目的：掌握shell的基本语法，编程思想是不变的，不同的语言只是语法不一样 变量的定义 局部变量局部变量作用域只是在当前的sh文件or当前的终端内 1234# 新建var.sh#!/bin/basha=helloworldecho $a 执行 123456# 第一种方式执行bash var.sh# 第二种方式执行 ./var.sh 前提是文件有可执行权限 chmod u+x var.sh# 第三种方式执行source var.sh 输出 1helloworld 备注1：shell变量赋值 ‘=’两边不能有空格 备注2: shell变量赋的值如果不是连续的，有空格的需要使用单/双引号 a=&#39;hello world&#39;,这时候需要引号包裹‘ 备注3: shell变量赋的值如果里面有另外的变量 12345key='test'a="hello $key world" # 注意这里面一定要用双引号包裹echo a# 输出hello test world #####系统命令变量定义 先执行命令，将命令执行之后的结果存到变量当中 1234cmd=`ls`orcmd=$(ls)echo $cmd 输出 1当前目录文件效果同 `ls` 全局变量 系统所有环境都可以使用的变量 查看方法：env 定义方式 分步骤定义 定义一个本地变量 12# 变量名=变量值name=qiangzai 声明为全局变量 12# export 变量名export name 同时定义 export 变量名=变量值 1export 变量名=变量值 内置变量 bash命令内部已经定义好的变量，可以直接使用，不需要定义 使用方法 和shell脚本有关的内置变量 $0 获取当前脚本名称 $# 获取当前脚本的参数个数 $n 获取当前脚本获取到的第n个参数 $? 获取上一次命令的执行情况，0表示成功 $$ 脚本运行时使用的进程号 $@ 获取所有的参数 和字符串相关的内置变量 字符串切割 12$&#123;var_name:start:n&#125;# var_name是字符串变量名 start正数 从开头开始 n表示截取字符的个数 案例：截取字符串’abcdefg’,从c开始截取2个字符 12alpha="abcdefg"echo $&#123;alpha:3:2&#125; 和默认值相关的内置变量 第一种 1234# 获取脚本第一个参数var_name=$1# defalut表示默认值，如果没有输入参数，default将会被使用$&#123;var_name:-default&#125; 第二种 123var_name=$1# 将会无视输入参数，直接输出设定好的默认值$&#123;var_name:+default&#125; 查看变量的3种方式123456# 定义一个变量name=qinagzai# 打印一下变量echo $name echo "$name"echo "&#123;$name&#125;" # 推荐使用，最规范 删除变量&amp;设置变量只读12unset 变量名readonly 变量名 语法学习 验证表达式 方式一 1[ 表达式 ] 方式二 1test 表达式 表达式两侧必须要有空格，表达式之间需要有空格，不然表达式表示的是一个整体 案例 12345678910# 获取参数个数arg_nums=$## 方式一验证[ "$&#123;arg_nums&#125;" -eq 3 ] # 表示脚本参数不等于3# 方式二test "$&#123;arg_nums&#125;" -eq 3 # 意思同上# 打印执行结果 1表示验证通过，0表示失败echo $? 逻辑表达式 -eq //等于 -ne //不等于 -gt //大于 （greater ） -lt //小于 （less） -ge //大于等于 -le //小于等于 命令的逻辑关系： 在linux 中 命令执行状态：0 为真，其他为假 &amp;&amp;并 命令1&amp;&amp;命令2 如果命令1执行成功，则执行命令2 如果命令1执行失败，则不执行命令2 || 或 命令1||命令2 如果命令1执行成功，则不执行命令2 如果命令1执行不成功，则执行命令2 shell文件表达式 -e 判断输入的内容表示的文件是否存在 -f判断输入的内容是否是一个文件 -d判断输入的内容是否是一个目录 -x判断输入的文件是否有可执行权限 -r判断文件是否可读 -w判断文件是否可写 12# 使用方式 注意有空格[ 文件表达式 文件名 ] sehll字符串表达式 == 判断两个字符串是否相等 ！= 判断两个字符串是否不一致 -z判断字符串是否为0 -n判断字符串长度是否不为0 shell流程控制 单if语句 1234if 条件语句then 执行语句fi if else语句 123456if 条件语句1then 执行语句1else 执行语句2fi 双if语句 123456789if 条件语句1then 执行语句1elif 条件语句2then 执行语句2else 执行语句3fi case语句 1234567891011case 值 in 值1） 执行语句1 ;; 值2） 执行语句2 ;; 值3） 执行语句3 ;;esac 案例 1234567891011121314151617#!/usr/bininstruct=$1case "$&#123;instruct&#125;" in start) echo "系统启动" ;; stop) echo "系统关闭" ;; reload) echo "系统重启" ;; *) echo "启动方式是：sh $0 [start|stop|reload]"esac 循环控制语句1234for 条件do 执行语句done Shell中的函数 1234567891011121314# 无参数格式func_name()&#123; 函数体&#125;#调用func_name# 有参数格式func_name()&#123; args=$n 函数体&#125;# 调用func_name args1 args2]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之常见问题总结]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目的：记录爬虫中遇到的坑…… xpath提取数据遇到tbody标签 问题描述：今天在用scrapy爬取网页时，当爬取表格内容时，发现使用插件 xpath helper获取正常，程序中的xpath解析不到 问题原因：浏览器会在table标签下添加tbody标签，浏览器会对html文件进行一定的规范化，但是源码当中是没有tbody标签的。 解决办法：将分析的xpath语句中把tbody去掉即可 Scrapyd运行后被拒绝访问 问题描述：我在腾讯服务器运行scrapyd项目部署，启动以后访问6800端口被拒绝，第一反应就是腾讯云的安全组配置问题，后来发现自己的出站端口6800早已打开。翻了翻日志，看到下面这句代码 1[-] Scrapyd web console available at http://127.0.0.1:6800/ 哈哈，尴尬的127.0.0.1，判断需要修改scrapyd的配置文件 解决问题 1、找到配置文件，先找Python的第三方库 scrapyd 安装目录 我这里找了半天没找到，就是用 find大法了 sudo find / -name scrapyd 2、找到目录，下一步就是 cd到安装目录下 1cd /usr/....../scrapyd 3、找到 default_scrapyd.conf，并打开修改 1bind_address = 127.0.0.1 为 1bind_address = 0.0.0.0 然后保存退出，重试一下就ok！]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>spider_error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之Git使用]]></title>
    <url>%2F2018%2F06%2F05%2F%E8%AE%B0%E5%BD%95%E4%B9%8BGit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Git介绍Git是目前世界上最先进的分布式版本控制系统 方便多人协作开发 方便版本控制 Git基本流程图 工作区&amp;暂存区&amp;仓库区介绍工作区 对于添加、修改、删除文件的操作，都发生在工作区中 暂存区 暂存区指将工作区中的操作完成小阶段的存储，是版本库的一部分 仓库区 仓库区表示个人开发的一个小阶段的完成 仓库区中记录的各版本是可以查看并回退的 但是暂存区的版本一旦提交就没有了 本地Git常见操作基本配置 初始化仓库 12git init# 会在初始化的路径下产生隐藏文件夹.git 配置个人信息 1234git config user.name 'qiang-zai'git config user.email 'china_qiangzai@163.com'# 配置的个人信息在.git/config文件下可以查看# 默认不配置的话，会使用全局配置里面的用户名和邮箱；全局git配置文件路径：～/.gitconfig 查看文件状态 1git status 基本操作 将工作区文件添加到暂存区 git add . # 添加项目中的所有文件到暂存区 git add xxx.py # 添加啊指定文件到暂存区 将暂存区文件提交到仓库区 commit会生成一条版本记录 -m后面是版本描述信息 1git commit -m '版本描述' 将工作区修改的文件直接提交到仓库区 1git commit -am '版本描述' 查看历史版本 123git log git reflog# git reflog可以查看所有分支的所有操作记录（包含commit和reset的操作），包括已经删除的commit记录，git log 则不能查看已经删除的commit记录 回退版本（本地仓库&gt;工作区） 方案一:版本较少的时候推荐使用 HEAD表示当前最新版本 HEAD^表示当前最新版本的前一个版本 HEAD^^表示当前最新版本的前两个版本，以此类推。。。 HEAD~1表示当前最新版本的前一个版本 HEAD~10表示当前最新版本的前10个版本，以此类推。。。 1git reset --hard HEAD^ 方案二：版本较多的时间推荐使用 通过每个版本的版本号回退到指定版本 git reflog来查看版本号 1git reset --hard 版本号 撤销修改 只能撤销工作区、暂存区的代码，不能撤销本地仓库的代码 撤销本地仓库的代码等于版本回退操作 撤销工作区代码 1git checkout 文件名 撤销暂存区代码 1234# 第一步 将暂存区代码撤销到工作区git reset HEAD 文件名# 第二步 撤销工作区代码git checkout 文件名 对比版本库 对比版本库（本地仓库）与工作区 1git diff HEAD -- xxx.py 对比版本库 1git diff HEAD HEAD^ -- xxx.py 删除文件：删除文件分为确定删除和误删 确定删除处理 123456# 删除文件(文件已经存在本地仓库版本库当中了)rm 文件名# git确定删除文件git rm 文件名# 删除后记录删除操作版本git commit -m '删除xxx文件' 误删文件 1234# 删除文件rm 文件名# git 撤销修改git checkout -- 文件名 协同开发克隆项目 前提需要在GitHub/其他git服务器创建好项目仓库 克隆远程仓库命令 1git clone https://github.com/xxxx/xxxx.git 配置项目开发人员信息 123cd 克隆的文件夹git config user.name 'xxx'git config user.email 'xxxx@163.com' 编辑代码 推送远端 123456# 工作区添加到暂存区git add .# 暂存区提交到仓库区git commit -m '编辑代码'# 推送远程仓库git push 在push的时候需要设置账号和密码，该密码则是github的账号和密码 如果每次push都需要设置账号与密码，那么可以设置记住密码 123456# 设置记住密码（默认15分钟）：git config --global credential.helper cache# 如果想自己设置时间，可以这样做(1小时后失效)：git config credential.helper 'cache --timeout=3600'# 长期存储密码：git config --global credential.helper store 同步服务器代码 1git pull 编辑代码之前要先pull，编码结束以后再 commit，最后再 push 冲突解决 容易冲突的操作方式 多个人同时操作了同一个文件 一个人一直写不提交 修改之前不更新最新代码 提交之前不更新最新代码 擅自修改同事代码 减少冲突的操作方式 养成良好的操作习惯,先pull在修改,修改完立即commit和push 一定要确保自己正在修改的文件是最新版本的 各自开发各自的模块 如果要修改公共文件,一定要先确认有没有人正在修改 下班前一定要提交代码,上班第一件事拉取最新代码 一定不要擅自修改同事的代码 标签 当某个大版本完成后，就需要打标签 记录大版本 备份大版本代码 本地打标签 123git tag -a 标签名 -m '标签描述'# 案例git tag -a v1.0 -m 'version 1.0' 推送标签到远程仓库 123git push origin 标签名# 案例git push origin v1.0 删除本地和远程标签 1234# 删除本地标签git tag -d 标签名# 删除远程仓库标签git push origin --delete tag 标签名 分支 分支的作用 区分生产环境代码以及开发环境代码 解决线上出现的bug问题，方便代码调试 特点 项目开发中公用分支包括 master、dev 分支 master是默认分支，用于发布，当需要发布时将 dev分支合并到 master 分支 dev是用于开发的分支，开发完阶段性的代码后，需要合并到 master 分支 查看当前分支 1git branch 没有创建其他分支时，只有 master分支 切换分支 1git checkout -b dev 设置本地分支跟踪远程执行分支（将分支推送到远程） 1git push -u origin dev 在切换后的dev分支开发，修改代码 dev分支代码合并到master分支并提交 只有当dev分支合并到master分支并push到远程仓库master分支以后，其他的开发人员才能获取到最新的代码 切换master分支 1git checkout master 合并 dev分支代码到 master主分支 1git merge dev 推送合并分支到远程仓库 1git push 其他开发人员同步合并后的代码 1git pull]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Scrapy框架]]></title>
    <url>%2F2018%2F05%2F31%2F%E7%88%AC%E8%99%AB%E4%B9%8BScrapy%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Scrapy是一个高性能的爬虫框架，主要是为了提取结构化数据而编写的应用框架，我们只需要实现少量代码就可以实现数据的快速获取，框架底层使用的Twisted异步网络框架,所以爬取速度非常的快。 文章目的：能够利用scrapy爬取数据 安装1pip install scrapy 查看常用操作(检查是否安装成功) 1scrapy --help 基本使用流程 第一步：创建项目（固定步骤） 1scrapy startproject 项目名称 第二步：创建爬虫（固定步骤） 12cd 项目目录scrapy genspider 爬虫名称 目标域名 第三步：编写爬虫组件和管道（主要编写的部分） 1# 在第二步创建的爬虫文件当中编写 第四步：启动爬虫（固定步骤） 1scrapy crawl 爬虫名称 scrapy组成 运行流程图（scrapy官方公布） 各部件作用 五大组件 引擎 中央协调 爬虫组件 提取数据 or url，生成请求对象 请求调度器 存放请求对象 下载器 发送请求获取响应 管道 保存数据 三大对象 Request请求对象 Response响应对象 Item数据对象 两大中间件 下载中间件 爬虫中间件 项目目录结构介绍 spider-project项目目录 __init__.py items.py定义数据模型 middlewares.py自定义中间件 pipelines.py自定义管道，保存数据 settings.py项目配置文件 scrapy.cfg项目部署文件 spiders爬虫组件目录 xxx.py具体实现爬虫的文件 通过 scrapy genspider 爬虫名称–&gt;生成的爬虫都在这个目录下 五大组件之爬虫组件 作用：提取数据 | 提取url返回请求对象 具体编写步骤1、继承爬虫类 123456# -*- coding: utf-8 -*-# 导入模块import scrapy# 继承爬虫类class ExampleSpider(scrapy.Spider): ... 2、定义爬虫名称 1name = 'example' 3、设置允许爬取的范围 1allowed_domains = ['example.com'] 4、设置开始爬取的请求地址 1start_urls = ['https://wxample.com/xxxx'] 5、实现解析函数 ​ 1、提取数据 ​ 2、提取url，返回请求对象 12def parse(self, response): pass 响应对象response基本信息 response.status &gt;&gt; 响应状态码 response.headers &gt;&gt; 响应头 response.text&gt;&gt; 响应内容 response.request.cookies&gt;&gt;获取cookies Selector对象基本操作 extract()提取数据，提取不到会抛出下标异常 extract_first()提取第一条数据，提取不到返回None 提取&amp;构建请求对象12345# 构建请求对象request = scrapy.Request(url='xxxx')# 返回请求对象&gt;&gt;引擎&gt;&gt;请求调度（scheduler）器&gt;&gt;请求下载器yield request 参数callback指定请求对象的解析函数 参数 meta把数据可以传递给解析函数 12345yield scrapy.Request(url=detail_url, callback=self.parse_detail, meta=&#123; 'item':item &#125;) 五大组件之管道 系统管道1、提取函数中使用 yield返回函数 2、命令行导出 1scrapy crawl 爬虫名称 -o 导出文件名称 自定义管道1、在 pipelines.py编写管道 ​ 1、创建管道类 ​ 2、实现管道处理数据方法 ​ 2.1 process_item必须实现 ​ 2.2 open_spider 可选实现，当爬虫启动时调用一次 ​ 2.3 close_spider 可选实现，当爬虫结束时调用一次 12345678# pipelines.pyclass JsonDataPipeline(object): '''创建管道类''' def process_item(self, item, spider): '''实现管道处理数据方法 --&gt;yield item--&gt;引擎--&gt;触发process_item方法 ''' pass 2、在 settings.py中开启管道 1234ITEM_PIPELINEs = &#123; # 在里面配置自己定义的管道类 'xxx.pipelines.定义的管道类名称':score # score 表示数字和权重&#125; 注意点 1、数字越小越优先执行 2、多管道 item传递必须在 process_item函数中返回 item 自定义数据模型 作用：规范数据的格式 定义文件位置 items.py 123456import scrapyclass dataItem(scrapy.Item): # define the fields for your item here like: name = scrapy.Field() pass 使用 123456# 导入items.py里面定义的模型类from xxx.items import dataItem# 创建模型对象data = dataItem()# 给对象添加数据和字典操作一样data['name'] = 'xxxx' 模型对象转字典 1dict(模型对象) 项目配置 settings.py可以配置项目中常用信息 注意：scrapy.settings.default_settings.py才是所有的配置信息，如果在settings.py中没有配置，就是用默认的配置文件 LOG_LEVEL= WARNING 表示显示的日志级别 LOG_FILE= 自定义的日志PATH 表示日志的存储位置 使用方式在 defalut_settings.py中寻找配置，然后把配置选项在 settings.py中重新配置； scrapy shell介绍 方便开发过程中的调试 使用1scrapy shell 请求地址 常用方法 request response xpath css text fetch请求新的地址 CrawlSpider crawlSpider是一个已经实现了爬取流程的爬虫； 创建1scrapy genspider -t crawl 爬虫名称 爬虫域名 参数 -t表示采用的爬虫模版 爬取流程提取符合条件的链接—&gt;跟进符合条件的链接—&gt;提取符合条件的链接—&gt;循环 使用说明 rules所有规则、元组、存放了提取页面的规则列表 Rule对象，条件规则对象 LinkExtactor链接提取器 callback当提取完页面后的回调处理函数 follow是否跟进提取,默认是 True LinkExtractor链接提取器 allow&gt;&gt;内容符合条件规则被提取 deny&gt;&gt;内容符合条件规则被排除（优先） allow_domains&gt;&gt;符合条件的域名被提取 deny_domians&gt;&gt;符合条件的域名被排除 restrict_xpaths&gt;&gt;根据xpath提取 tags&gt;&gt;默认(‘a’,’area’)符合链接提取的标签名 attrs&gt;&gt;默认(href)，链接提取标签的属性名 restrict_css&gt;&gt;根据css样式提取器提取 strip&gt;&gt;提取后的内容去除空格 链接去重把链接地址放到 seen--&gt;set() 注意：如果url的参数位置发生变化会导致无法去重 crawlspider编写代码流程1.编写提取规则—&gt;提取链接规则 2.编写提取具体页面解析代码 scrapy中间件 1.爬虫中间件 引擎和爬虫组件交互时触发中间件 2.下载中间件 引擎和下载器交互时触发的中间件 中间件的使用流程 在 middlewares.py创建中间件类 实现所需要的拦截的函数 在 settings.py中配置开启中间件 和管道一样，配置的数字(score)越小越优先执行 下载中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_request 123456789引擎 -&gt; 下载中间件 -&gt; 下载器:param request: 请求对象:param spider: 请求来自的爬虫:return: return None 继续处理这个请求return Response 直接把响应提交给引擎 -&gt; 爬虫 return Request 直接返回引擎raise IgnoreRequest 触发 process_exception 回调函数 process_response 123456下载器 -&gt; 下载中间件 -&gt; 引擎 :param request: :param response: :param spider: :return: raise IgnoreRequest 把这请求忽略 process_exception 12345678910当下载中间件异常异常时回调 :param request: :param exception: :param spider: :return: return None 继续处理异常，向下一个中间件传递异常return a Response 停止异常链，把响应返回给引擎return a Request 停止异常链，把请求返回给引擎 爬虫中间件 from_crawler 类方法,当创建爬虫时回调，仅调用一次 spider_opened 爬虫打开时回调，仅调用一次 process_spider_input引擎 -&gt; 爬虫中间件 -&gt; 爬虫 参数 response 响应对象 spider 爬虫对象 process_spider_output 当爬虫提交数据或者请求给引擎时触发 process_spider_exception 当 process_spider_input 异常异常时触发 process_start_requests 当引擎向爬虫所要 start_requests 时触发 scrapyd部署 专门用于部署scrapy项目框架，scrapyd帮助我们运行scrapy代码 服务端 安装 1pip install scrapyd 启动服务 1scrapyd 远程访问 客户端远程访问6800端口 开发端 安装 1pip install scrapyd-client 配置部署 scrapy.cfg文件 123[deploy:部署名称]url = http://xxx.xxx.xxx.xxx:6800/ # 服务器的ipproject = 项目名称 上传项目到服务器 1scrapyd-deploy 部署名称 -p 项目名称 启动爬虫 1curl http://server-iP:6800/schedule.json -d project=项目名称 -d spider=爬虫名称 停止爬虫 1curl http://server-ip:6800/cancel.json -d project=项目名称 -d job=jobid scrapy redis 主要是利用scrapy+redis实现 断点续爬功能 实现分布式爬虫功能 安装1pip install scrapy-redis 断点续爬实现 现实情况中很多时候我们的一旦中断爬虫就需要重新再请求之前的url，scrapy-redis能够实现断点续爬 1、在 settings.py配置 1234567891011121314# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式REDIS_HOST = "127.0.0.1"REDIS_PORT = 6379 分布式爬虫实现流程1、在 settings.py 1234567891011121314151617181920# 把原来的请求调度器的实现类改造成redis的实现类SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 设置去重算法DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 开关开启持久化SCHEDULER_PERSIST = True# 配置redis服务器## REDIS_URL = "redis://127.0.0.1:6379"#或者使用下面的方式# REDIS_HOST = "目标redis服务器" # 不能编写 127.0.0.1# REDIS_PORT = 6379# 管道配置ITEM_PIPELINES = &#123; 'scrapy_redis.pipelines.RedisPipeline': 400,&#125; 2、修改爬虫代码，修改集成类 CrawlSpider==&gt;RedisCrawlSpider Spider==&gt;RedisSpider 3、添加监控的 redis_key体现首个请求地址]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题之字典扁平化]]></title>
    <url>%2F2018%2F05%2F24%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E5%AD%97%E5%85%B8%E6%89%81%E5%B9%B3%E5%8C%96%2F</url>
    <content type="text"><![CDATA[实际开发中存储的数据结构很多都是树状形状，现在需要写一个功能实现从树形结构到扁平化结构的转换 123456789101112131415161718192021'''有若干种不同的树形结构，需要映射为扁平化的dict字典，需要写一个函数，完成从树形结构到扁平化结构的转换如输入in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;# 输出&#123; 'jack_English_socre': '90', 'jack_English_tearcher': 'Xu', 'jack_Math_socre': '75', 'jack_Math_tearcher': 'zhang', 'jack_height': '172' &#125;''' 题目考查的其实就是递归的应用，字典嵌套字典，这里还有用到Python的 isinstance() 方法 12345678Python 中的isinstance函数，isinstance是Python中的一个内建函数。是用来判断一个对象的变量类型和type类似。语法:isinstance(object, classinfo)&gt;&gt;&gt; isinstance(1, int)True&gt;&gt;&gt; isinstance(&#123;'age': 18&#125;, dict)True&gt;&gt;&gt; isinstance('1', int)False 答案 1234567891011121314151617181920212223in_data = &#123; "jack": &#123; "Math": &#123;"tearcher": "zhang", "socre": "75"&#125;, "English": &#123;"tearcher": "Xu", "socre": "90"&#125;, "height": "172" &#125;&#125;target = &#123;&#125; # 定义一个全局变量用来存储返回结果def dict2flat(data, targetKey=''): for k, v in data.items(): if isinstance(v, dict): dict2flat(v, targetKey + k + '_') else: target[targetKey + k] = vif __name__ == '__main__': dict2flat(in_data) from pprint import pprint pprint(target)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之for循环的运行流程]]></title>
    <url>%2F2018%2F05%2F13%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8Bfor%E5%BE%AA%E7%8E%AF%E7%9A%84%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[for循环是 Python编程当中使用最多的语句，for循环用于 迭代容器对象中的元素，Python当中的容器对象常见的有列表(list)、字符串(str)、字典(dict)、元组(tuple)、集合(set)、等 案例 作用于列表 123456&gt;&gt;&gt; for elem in [1,2,3]:... print(elem)...123 作用于元组 123456&gt;&gt;&gt; for i in ("zhang", "san", 30):... print(i)...zhangsan30 作用于字符串 123456&gt;&gt;&gt; for c in "abc":... print(c)...abc 作用于字典 12345&gt;&gt;&gt; for k in &#123;"age":10, "name":"wang"&#125;:... print(k)...agename 作用于文件 12345&gt;&gt;&gt; for line in open("requirement.txt"):... print(line, end="")...Fabric==1.12.0Markdown==2.6.7 可能有人会问，问什么这么多不同类型对象都支持for语句，还有那些类型的对象可以作用在for语句中呢？回答这个问题之前，我们先要了解for循环背后的执行原理。 for循环是对容器进行迭代的过程，什么是迭代？迭代就是从某个容器对象中逐个读取元素，直到读取不到元素为止。那么那些对象支持迭代操作？任何对象都可以吗？先随便自定义一个类试试，看行不行： 12345678910&gt;&gt;&gt; class MyRange:... def __init__(self, num):... self.num = num...&gt;&gt;&gt; for i in MyRange(10):... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyRange' object is not iterable 错误日志非常清楚地告诉我们，MyRange不是一个可迭代对象，所以它不能用于迭代，那么到底什么样的对象才能称得上是可迭代对象(iterable)呢？ 可迭代对象需要实现 __iter__方法，并返回一个迭代器对象，什么是迭代器对象？迭代器对象只需要实现 __next__方法。现在我们就来验证一下列表为什么支持迭代： 123456789101112131415&gt;&gt;&gt; x = [1,2,3]&gt;&gt;&gt; its = x.__iter__() # x有此方法，说明列表是可迭代对象&gt;&gt;&gt; its&lt;list_iterator object at 0x100f32198&gt;&gt;&gt;&gt; its.__next__() # its有此方法，说明its是迭代器1&gt;&gt;&gt; its.__next__()2&gt;&gt;&gt; its.__next__()3&gt;&gt;&gt; its.__next__()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 从试验结果来看，列表是一个可迭代对象，因为它实现了 __iter__方法，并且返回一个迭代器对象&gt;&gt; list_iterator,因为它实现了 __next__方法。我们看到它不断地调用 __next__方法，其实就是不断地迭代获取容器中的元素,直到容器中没有元素，抛出 StopIteration异常为止。 那么 for语句又是如何循环的呢？它的步骤是： 1、先判断对象是否为可迭代对象，不是的话直接报错，抛出 TypeError异常，是的话，调用 __iter__方法，返回一个迭代器对象； 2、不断地调用迭代器对象的 __next__方法,每次按序返回迭代器中的一个值； 3、迭代到最后，没有更多元素了，就抛出异常 StopIteration，这个异常 python自己会处理。 对于元组，字典，字符串也是同样的道理，弄明白了 for的执行原理之后，我们就可以实现自己的迭代器用在for循环中。前面的 MyRange报错是因为它没有实现迭代器协议里面的这两个方法，现在继续改进 12345678910111213141516class MyRange: def __init__(self, num): self.i = 0 self.num = num def __iter__(self): return self def __next__(self): if self.i &lt; self.num: i = self.i self.i += 1 return i else: # 达到某个条件时必须抛出此异常，否则会无止境地迭代下去 raise StopIteration() 因为它实现了 __next__方法，所以 MyRange本身已经是一个迭代器了，所以 __iter__返回的就是 对象本身 self.现在用在for循环中试试： 123456for i in MyRange(3): print(i)# 输出 0 1 2 有没有发现，自定义的 MyRange 功能和内建函数 range很相似。for 循环本质是不断地调用迭代器的__next__方法，直到有 StopIteration 异常为止，所以任何可迭代对象都可以作用在for循环中。]]></content>
      <categories>
        <category>Python-Advanced</category>
      </categories>
      <tags>
        <tag>for</tag>
        <tag>iterable</tag>
        <tag>iterator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之selenium]]></title>
    <url>%2F2018%2F04%2F27%2F%E7%88%AC%E8%99%AB%E4%B9%8Bselenium%2F</url>
    <content type="text"><![CDATA[Selenium介绍 Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏。程序脚本可以通过 Selenium API 控制浏览器。 运行流程 环境搭建 安装selenium 1pip install selenium 下载对应的驱动 淘宝驱动地址：http://npm.taobao.org/mirrors/chromedriver/ 备注：一定要下载本地浏览器版本对应的驱动 基本操作 12345678910111213141516171819# 导入模块from selenium import webdriverimport time# 加载浏览器驱动，创建浏览器对象browse = webdriver.Chrome('./chromedriver')# 访问百度browse.get('https://www.baidu.com')# 定位百度搜索输入框，输入关键字“电影”browse.find_element_by_name('wd').send_keys('电影')# 点击‘百度一下’browse.find_element_by_id('su').click()time.sleep(2) # 退出窗口browse.quit() selenium驱动寻找方式 1、通过指定浏览器驱动路径 1browser = webdriver.Chrome('浏览器驱动位置') 2、通过 $PATH环境变量指定浏览器驱动 12# 通过 $PATH 寻找驱动，如果寻找不到就报错 browser = webdriver.Chrome() 添加网址：https://www.jianshu.com/p/e50a49f86070 其实就是找到Python解释器的 bin目录，把驱动放进去即可 selenium控制浏览器操作访问URL1browser.get('https://www.baidu.com/') 定位元素 find_element_by_xxx返回第一个符合条件 WebElement find_elements_by_xxx返回符合条件的 WebElement列表 xxx说明 find_elements_by_class_name通过标签的class属性定位元素 find_element_by_id通过标签的ID属性定位元素 find_element_by_name通过标签的name属性定位 find_element_by_css_selectorcss样式选择 find_element_by_link_text通过链接内容查找 find_element_by_partial_link_text 通过链接内容包含的内容查找，模糊查询 find_element_by_xpath 通过xpath查找数据 xpath只能获取webelement对象，不能直接获取属性和文本内容 获取元素属性和文本内容1234# 获取属性WebElement.get_attribute('属性名')# 获取文本内容WebElement.text 输入框Input输入内容1Input_element.send_keys('xxx') 点击1element.click() 使用无界面浏览器 使用无界面的驱动 下载地址：http://phantomjs.org/download.html 使用方式 browser = webdriver.PhantomJS(&#39;驱动路径‘) 设置chrome启动参数 123456789101112131415from selenium import webdriver# 创建浏览器启动参数options = webdriver.ChromeOptions()options.add_argument('--headless') # 无界面options.add_argument('--disable-gpu') # 禁用gpu# 创建浏览器对象browser = webdriver.Chrome(chrome_options=options)# 访问网址browser.get('https://www.baidu.com')# 截图证明browser.save_screenshot('百度截图.png')# 退出browser.quit() 设置User-Agent和Proxy代理123456options = webdriver.ChromeOptions()# 切换User-Agentoptions.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1')# 设置代理options.add_argument('--proxy-server=代理服务器地址') # 设置代理browser = webdriver.Chrome('chrome_options=options) 获取网页源码1browser.page_source 注意：获取的网页源码是经过JS页面执行后的结果源码 cookies 操作（非常重要） 获取所有Cookies 1browser.get_cookies() 通过名字获取Cookie 1browser.get_cookie() 添加Cookie 1browser.add_cookie() 通过名字删除Cookie 1browser.delete_cookie() 删除所有Cookie 1browser.delete_all_cookies() 执行JS代码1browser.execute_script("js-code") 等待加载 方式一：强制等待，浪费时间 1time.sleep(秒数) 方式二：隐性等待 1browser.implicitly_wait(等待时间) 方式三：显性等待，每个元素都可以自己定义检查条件 手动编写方式 123456789101112131415# 显性等待-手动编写t = time.time()# 定义超时时间timeout = 60while True: try: # 超时时间间隔 time.sleep(0.1) url_element = browser.find_element_by_class_name("favurl") break except: # 超时处理 if time.time() - t &gt; timeout: break pass 系统提供显性等待API 1234567891011121314151617# 导入显性等待的API需要的模块# 1&gt; 等待对象模块from selenium.webdriver.support.wait import WebDriverWait# 2&gt; 导入等待条件模块from selenium.webdriver.support import expected_conditions as EC# 3&gt; 导入查询元素模块from selenium.webdriver.common.by import By# 使用selenium api 实现显性等待# 1&gt; 创建等待对象# 参数一 浏览器对象# 参数二 超时时间# 参数三 检查元素时间间隔wait = WebDriverWait(browser,60,0.1)# presence_of_element_located 检查元素是否存在，参数是一个元祖，元祖内部描述等待元素查询方案# visibility_of_element_located 检查元素是否可见url_element= wait.until(EC.presence_of_element_located((By.CLASS_NAME,"favurl"))) 窗口切换 获取所有的窗口（列表） 1browser.window_handles 切换窗口 1browser.switch_to_window(all_windowsp['index']) 备注：browser.title可以通过打印当前窗口title调试 iframe切换 1、获取iframe标签对象 1iframe_element = browser.find_element_by_xxx('') 2、切换到iframe窗口 1browser.switch_to.frame(iframe_element) 3、切换到主窗口 1browser.switch_to.default_content()]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv和json之间的转换]]></title>
    <url>%2F2018%2F04%2F26%2Fcsv%E5%92%8Cjson%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[CSV（逗号分隔值文件格式），是一种通用的，相对简单的文件格式，被用户、商业、科学广泛使用，最广泛的应用是在程序之间转移表格数据。 文章目标：了解CSV的基本操作以及json格式数据如何写入CSV文件并转换Excel 使用场景爬虫拿到的大部分都是json格式数据，但是客户或者数据分析员一般更需要Excel这种格式的数据 基本操作 1、导入模块 12# python3.X内置模块不需要下载import csv 2、创建csv写入对象 12345# 创建文件对象f = open('03-test.csv', 'w', encoding='utf-8')# 创建csv写入对象csv_writer = csv.writer(f) 3、写入数据 123csv_writer.writerow(['姓名', '年龄', '性别'])csv_writer.writerow(['强仔', '18', '男'])csv_writer.writerow(['彤彤', '17', '女']) 4、关闭文件 1f.close() json2csv 1、先转换json格式为python数据类型 12345import csvimport json# data.json 为事先准备好的json数据with open('data.json', 'r') as f: data_list = json.load(f) # 转换成pyton数据类型 2、创建csv对象 12f = open('data.csv', 'w', encoding='utf-8')csv_writer = csv.writer(f) 3、写入数据 1234567# 3.1 写入标题也就是字典的关键字keycsv_writer.writerow(data_list[0].keys())# 3.2 遍历写入字典元素valuesfor data in data_list: csv_writer.writerow(data.values())# 3.3 关闭文件句柄f.close() csv转Execl见链接：https://jingyan.baidu.com/article/3c343ff7faa59b0d3779633e.html]]></content>
      <categories>
        <category>Standard-Library</category>
      </categories>
      <tags>
        <tag>csv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：求列表当中最大的三个元素]]></title>
    <url>%2F2018%2F04%2F15%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98-%E4%BA%8C-%EF%BC%9A%E6%B1%82%E5%88%97%E8%A1%A8%E5%BD%93%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%9A%84%E4%B8%89%E4%B8%AA%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[在牛客网https://www.nowcoder.com/上刷题遇到如何从list中取得最大的三个值：自己写的方法复杂度太高，放上大牛的方法，复杂度很低。看了好几遍才体会到大概的精髓。 12345678910111213141516171819202122232425262728'''从list中取出最大的三个值__author__:无名'''def FindList3MaxNum(foo): max1, max2, max3 = None, None, None for num in foo: if max1 is None or max1 &lt; num: max1, num = num, max1 if num is None: continue if max2 is None or num &gt; max2: max2, num = num, max2 if num is None: continue if max3 is None or num &gt; max3: max3 = num return max1, max2, max3if __name__ == '__main__': foo = [78, 23, 10, 56, 4, 103, 89, 14] max1, max2, max3 = FindList3MaxNum(foo) print(max1, max2, max3)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之list常见用法]]></title>
    <url>%2F2018%2F04%2F11%2Fpython%E7%BC%96%E7%A8%8B%E4%B9%8Blist%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[列表是Python最常用的数据类型之一，本文整理一下列表最常用的10种操作，如果开发过程当中遇到了，可以尝试从这些找答案 1、迭代列表时如何访问列表下标索引普通版：12345678items = [8, 23, 45]for index in range(len(items)): print(index, "--&gt;", items[index])&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 Level Up版：1234567for index, item in enumerate(items): print(index, "--&gt;", item)&gt;&gt;&gt;0 --&gt; 81 --&gt; 232 --&gt; 45 enumerate还可以指定元素的第一个元素从第几个索引开始，默认是0，也可以指定从1开始： 1234567for index, item in enumerate(items, start=1): print(index, "--&gt;", item)&gt;&gt;&gt;1 --&gt; 82 --&gt; 233 --&gt; 45 2、append与extend方法有什么区别append表示把某个数据当做新元素追加到列表的最后面，它的参数可以是任意对象 1234567x = [1, 2, 3]y = [4, 5]x.append(y)print(x)&gt;&gt;&gt;[1, 2, 3, [4, 5]] extend 的参数必须是一个可迭代对象，表示把该对象里面的所有元素逐个地追加到列表的后面 1234567891011x = [1, 2, 3]y = [4, 5]x.extend(y)print(x)&gt;&gt;&gt;[1, 2, 3, 4, 5]# 等价于：for i in y: x.append(i) 3、检查列表是否为空普通版：1234567if len(items) == 0: print("空列表")或者if items == []: print("空列表") Level Up版：12if not items: print("空列表") 4、如何理解切片 切片用于获取列表中指定范的子集，语法非常简单 1items[start:end:step] 从 start 到 end-1 位置之间的元素。step 表示步长，默认为1，表示连续获取，如果 step 为 2 就表示每隔一个元素获取。 12345678910111213141516a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[3:8] # 第3到第8位置之间的元素[4, 5, 6, 7, 8]&gt;&gt;&gt; a[3:8:2] # 第3到第8位置之间的元素，每隔一个元素获取[4, 6, 8]&gt;&gt;&gt; a[:5] # 省略start表示从第0个元素开始[1, 2, 3, 4, 5]&gt;&gt;&gt; a[3:] # 省略end表示到最后一个元素[4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[::] # 都省略相当于拷贝一个列表，这种拷贝属于浅拷贝[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 5、如何拷贝一个列表对象第一种方法：1new_list = old_list[:] 第二种方法：1new_list = list(old_list) 第三种方法：12345import copy# 浅拷贝new_list = copy.copy(old_list)# 深拷贝new_list = copy.deepcopy(old_list) 扩展，了解即可 6、如何获取列表中的最后一个元素 索引列表中的元素不仅支持正数还支持负数，正数表示从列表的左边开始索引，负数表示从列表的右边开始索引，获取最后一个元素有两种方法。 12345&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a[len(a)-1]10&gt;&gt;&gt; a[-1]10 7、如何对列表进行排序 列表排序有两种方式，一种是列表自带的方式 sort，一种是内建函数 sorted。复杂的数据类型可通过指定 key参数进行排序。由字典构成的列表，根据字典元素中的age字段进行排序： 12345678910items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]items.sort(key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'age': 10, 'name': 'Bart'&#125;, &#123;'age': 20, 'name': 'cater'&#125;, &#123;'age': 39, 'name': 'Homer'&#125;] 列表有 sort方法，用于对原列表进行重新排序，指定 key 参数，key 是匿名函数，item 是列表中的字典元素，我们根据字典中的age进行排序，默认是按升序排列，指定 reverse=True 按降序排列 1234items.sort(key=lambda item: item.get("age"), reverse=True)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Bart', 'age': 10&#125;] 如果不希望改变原列表，而是生成一个新的有序列表对象，那么可以内置函数 sorted ，该函数返回新列表 12345678910111213items = [&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;"name": 'cater', 'age': 20&#125;]new_items = sorted(items, key=lambda item: item.get("age"))print(items)&gt;&gt;&gt;[&#123;'name': 'Homer', 'age': 39&#125;, &#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;]print(new_items)&gt;&gt;&gt;[&#123;'name': 'Bart', 'age': 10&#125;, &#123;'name': 'cater', 'age': 20&#125;, &#123;'name': 'Homer', 'age': 39&#125;] 8、如何移除列表中的元素删除列表中的元素有三种方式 remove 移除某个元素，而且只能移除第一次出现的元素 12345678910&gt;&gt;&gt; a = [0, 2, 2, 3]&gt;&gt;&gt; a.remove(2)&gt;&gt;&gt; a[0, 2, 3]# 如果要移除的元素不在列表中，则抛出 ValueError 异常&gt;&gt;&gt; a.remove(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: list.remove(x): x not in list· del根据指定的索引移除元素 12345678910&gt;&gt;&gt; a = [3, 2, 2, 1]# 移除第一个元素&gt;&gt;&gt; del a[1][3, 2, 1]# 当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; del a[7]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list assignment index out of range pop与del类似，但是pop方法可以返回移除的元素 1234567891011&gt;&gt;&gt; a = [4, 3, 5]&gt;&gt;&gt; a.pop(1)3&gt;&gt;&gt; a[4, 5]# 同样，当超出列表的下表索引时，抛出IndexError的异常&gt;&gt;&gt; a.pop(7)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: pop index out of range 9、如何连接两个列表12345678listone = [1, 2, 3]listtwo = [4, 5, 6]mergedlist = listone + listtwoprint(mergelist)&gt;&gt;&gt;[1, 2, 3, 4, 5, 6] 列表实现了 + 的运算符重载，使得 + 不仅支持数值相加，还支持两个列表相加，只要你实现了 对象的 __add__操作，任何对象都可以实现 + 操作，例如： 123456789101112131415161718192021class User(object): def __init__(self, age): self.age = age def __repr__(self): return 'User(%d)' % self.age def __add__(self, other): age = self.age + other.age return User(age)user_a = User(10)user_b = User(20)c = user_a + user_bprint(c)&gt;&gt;&gt;User(30) 10、如何随机获取列表中的某个元素123456789import randomitems = [8, 23, 45, 12, 78]&gt;&gt;&gt; random.choice(items)78&gt;&gt;&gt; random.choice(items)45&gt;&gt;&gt; random.choice(items)12]]></content>
      <categories>
        <category>Python-Basis</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工具之Charles使用问题记录]]></title>
    <url>%2F2018%2F04%2F07%2F%E5%B7%A5%E5%85%B7%E4%B9%8BCharles%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[最近研究爬虫需要进行HTTP抓包，由于 Fiddler没有Mac版本，选择使用Charles（青花瓷）来抓包，文章 记录一下 Charles的使用和使用过程中遇到的问题 目标：Charles抓包&amp;问题记录 安装 安装包百度云链接：链接:https://pan.baidu.com/s/1QYaEVwgKSH8znwORotBOvg 密码:s7jz 安装步骤1、点击dmg安装—&gt;应用程序 2、打开charles应用程序—&gt;显示包信息—&gt;替换其中的 charles.jar包 基本使用 这个我就不过多介绍网上教程一大堆，需要注意的是如果要抓 https需要安装证书，并使证书可信任 问题记录 安装证书以后还是无法抓HTTPS的包，显示unknown?解决步骤 1、点击Proxy—&gt;SSL Proxying settings—&gt;SSL Proxying—&gt;Add—&gt; HOST和PORT都填写 *即可 2、重启 charles 安装Charles 抓包信息当中以后不显示 request和response解决步骤 1、点击Preferences—&gt;Viewers—&gt;把 Combine request and response 取消勾选—&gt;点击OK即可 2、重启charles 抓包手机配置链接：https://blog.csdn.net/pansanday/article/details/80347632]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Charles</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之数据提取]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[今天主要记录一下数据提取的几种方法，在爬虫获取到响应数据以后我们改怎么处理响应的数据呢？从网上拿到的响应数据各种类型都有：JSON类型的，XML格式的,html格式的等等。 常用的数据提取方式 jsonpath re xpath beautifulsoup 一、jsonpath基本使用 json是什么JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。 json模块常用操作 json.loads(json_str) JSON字符串—&gt;转换成Python数据类型 json.dumps(dict_data) Python数据类型—&gt;JSON字符串 json.load(f) json文件—&gt;获取Python数据类型 json.dump(dict_data, f) Python数据类型—&gt;写入json文件 ensure_ascii=False 显示中文 indent=数字表示空格 Jsonpath是什么？ jsonpath是一种语法规则快速从JSON数据当中提取数据 在线调试网址：https://jsonpath.com/ 在线解析工具：https://www.json.cn/# 语法规则 语法 描述 案例 $ 根节点 @ 现行节点 . 取子节点 $.store.book .. 取子孙节点 $..book [] 设置筛选条件 $..book[0] [,] 支持多选选择内容 $..book[1,3] () 支持表达式计算 $..book[(@.length - 1)] ?() 支持过滤操作 $..book[?(@.price&lt;10)] Python中怎么使用123import jsonpathdata = jsonpath('字典类型数据', 'jsonpath提取语法')# data是list类型 如果提取不出来返回False 二、re基本使用 用事先定义好的一些特定字符，及这些特定字符的组合，组成一个规则字符串，这个规则字符串用来表达对字符串的一种过滤逻辑 基本语法 预定义字符集 \d表示数字：[0-9] \D表示非数字：[^\d] \s表示空白字符 \S非空白字符 \w单词字符：[A-Z,a-z,0-9] \W非单词字符：[^\w] .匹配任意除换行符\n外的字符，备注：在DoTALL模式中也能匹配换行符 \转义字符，使后一个字符表示字符本身 []表示字符的选取范围 数量词 *表示任意个 +表示至少一个 ?表示至多一个（1个或者没有） {}表示范围区间 {1} 表示匹配一个字符 {1,6}表示匹配1到6个字符 {,6}表示最多匹配6个字符 贪婪模式和非贪婪模式 贪婪模式(.*)尽可能多的匹配 非贪婪模式(.*?)一旦匹配到就结束 DOTALL模式 让.匹配\n 1re.RegexFlag.DOTALL、re.RegexFlag.S、re.S、re.DOTALL 忽略大小写 1re.RegexFlag.I re.RegexFlag.IGNORECASE re.I re.IGNORECASE 支持DOTALL 和忽略大小写 用| 原始字符串使用r不用\ 正则前面使用r正则语法 四种匹配检索方法 match match 从头开始匹配，仅匹配一次 search search 全局匹配，仅匹配一次 findall findall 获取符合条件的所有数据，返回列表 finditer finditer 获取符合条件的所有数据，返回迭代器对象 数据量大的时候推荐使用，提高内存使用效率 分组&amp;替换 sub split 通用分组格式：[分隔符]+ 1234567import redata = 's,fqw;fwe, fds;fsda,afds;'pattern = re.compile(r'[, ;]+')ret = pattern.split(data)print(ret) 输出 1['s', 'fqw', 'fwe', 'fds', 'fsda', 'afds', ''] sub 12345678import redata = 's,fqw;fwe,fds;fsda,afds;'# 把, ; 都替换成 #pattern = re.compile(r'[,;]+')ret = pattern.sub('#', data)print(ret) 输出 1s#fqw#fwe#fds#fsda#afds# 三、Xpath基本使用 xpath是一门在HTML/XML文本中查找信息的语言，可以用在HTML/XML文档中对元素和属性进行遍历，简单来说就类似于正则通过一定的语法规则从文本中提取数据 基本语法 节点选择 /表示从根节点选取 //从匹配选择的当前节点选择文档中的节点 .表示选取当前节点 ..选取当前节点的父节点 选取属性 @ //节点/@属性名称 文本选择 text()选取文本 /text() 高级语法-筛选条件 //node[筛选条件] 通过下标（数字）筛选（下标从1开始） /node[1] 通过属性筛选 /node[@属性] /node[@属性=值] 通过文本内容筛选 //node[text()=&#39;筛选的值&#39;] 通过内置函数 last() position() containes包含内容 contains(text(), &#39;文本&#39;) contains(@属性， ‘属性值’) starts-with 从头匹配内容 语法和containes一样 通过子节点来筛选 /node[node_子节点&gt;子节点的值] 通配符 *匹配任何元素节点 //*[text()=&#39;某值&#39;] //*[@category=’某value’] @*匹配node任意属性的value //node[@*=&#39;某value&#39;] 多个条件同时满足 使用 | lxml模块 Python lxml模块去使用xpath语法 安装1pip install lxml lxml基本使用1、导入模块 1from lxml import etree 2、构建根元素对象 1eroot = etree.HTML(html_data) # eroot 是一个element对象 3、操作根元素对象提取数据 1eroot.xpath('xpath语法规则') 4、打印调试查看element对象内容 1etree.tostring(eroot).decode('utf-8') 四、beautifulsoup基本使用 介绍 Beautifulsoup也是一个HTML/XMl的解析器，跟上面的 lxml一样 优点 用来解析HTML比较简单，API非常人性化，支持CSS选择器 缺点 lxml只会遍历局部，而Beautifusoup是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml 安装1pip install beautifulsoup4 官方文档：http://beautifulsoup.readthedocs.io/zh_CN/v4.4.0 基本使用1、导入模块 1from bs4 import BeautifulSoup 2、创建beautifulsoup对象 12soup = BeautifulSoup(html,'lxml')soup = BeautifulSoup(open('文件')) 3、操作node 获取元素标签内容 1soup.标签名称 获取子元素 1234# 返回列表soup.node_name.contents# 返回迭代器soup.node_name.children 获取元素内容 1soup.node_name.get_text() 获取元素属性值 1soup.a.get('href') 注意如果获取元素的属性是 class返回的是列表 find 和find_all介绍 find返回符合条件的第一个元素 find_all返回符合条件的所有元素列表 备注：find和find_all提供参数都一样，下面以find_all()来展示 1、通过标签和标签列表查找元素 12soup.find_all('a') # 查询所有的a标签soup.find_all(['a', 'b']) # 查询所有的a标签和b标签 2、通过正则表达式查找元素 12# 以b开头的标签查找soup.find_all(re.compile('^b')) 3、通过属性来查找 12345soup.find_all(&#123; attrs=&#123; '属性名'：'值' &#125;&#125;) 4、通过文本内容查找元素 1soup.find_all(text='查询的文本内容') 6、混合使用 1234567soup.find_all( '标签名', attrs=&#123; '属性名'：'值' &#125;， text='内容') CSS样式选择器 返回值是一个list 类选择器 使用 .开头 1soup.select('.class_name') ID选择器，使用#开头 1soup.select('#id_name') 标签选择器，使用标签 名称 1soup.select('p_name') 属性选择器 1soup.select('p[属性=值]') 层级选择器 div p 选择 &lt;div&gt; 元素内部的所有 &lt;p&gt; 元素。 div , p 选择所有 &lt;div&gt; 元素和所有 &lt;p&gt; 元素。 div &gt; p 选择父元素为 &lt;div&gt; 元素的所有 &lt;p&gt; 元素。 .cls1.cls2 选择类名是cls1 并且 类名是cls2]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>re</tag>
        <tag>jsonpath</tag>
        <tag>bs4</tag>
        <tag>xpath</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之js逆向百度翻译]]></title>
    <url>%2F2018%2F03%2F22%2F%E7%88%AC%E8%99%AB%E4%B9%8Bjs%E9%80%86%E8%A1%8C%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[爬虫小试身手之js逆向百度翻译，目前百度翻译在PC和移动端都做了反扒措施，今天我们一看究竟～ 什么是js逆向？所谓的js逆向其实就是破解对方反扒的一种方式，目前很多网站反扒都使用js来做，百度翻译的反扒主要是利用了请求参数中的 sign这个签名来做的，下面就来详细的讲解 sign怎么变动了？当你更换输入框输入的翻译目标时，对比请求携带的参数发现sign这个参数随着输入的单词所变动，判断应该是js代码在请求前生成了这个sign，需要去定位具体哪个JS文件的生成了这个sign 根据上图我们可以看出sign参数是由js代码 m(a)来生成的，a参数是as也就是我们要翻译的目标，这个是我们可以拿到的，现在要做的就是如何执行m()这段js代码，Python被称为胶水语言，在Python当中调用JS代码自然也是可以实现的，Python调用js代码目前主流的两个模块 pyexecjs 、js2py，今天我们讲的主要是通过 js2py来实现。 下载模块1pip install js2py 基本使用 在python当中执行js 12345678# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在Python当中执行js代码content.execute("console.log('abc')") 输出1'abc' 在JS代码中调用Python程序 123456789101112131415161718# 1.导入模块import js2py# 2.构建上下文对象content = js2py.EvalJs()# 3.在js当中调用Python程序content.a = 1content.b = 'abc'content.c = [1, 3, 4]content.d = &#123; "name": 'qwe'&#125;content.execute('console.log(a)')content.execute('console.log(b)')content.execute('console.log(c)')content.execute('console.log(d)') 输出1234'1''abc'[1, 3, 4]&#123;'name': 'qwe'&#125; 实战既然Pythoncode中可以调用js我们只要找到这段js在代码中执行就解决了这个问题，上代码 1234567891011121314151617181920212223242526272829303132333435363738394041# Python代码# 1.导入模块import requestsimport js2py# 2.发送请求，获取响应内容# 请求地址 请求头 请求方式 请求参数url = 'https://fanyi.baidu.com/v2transapi'headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36', 'Referer': 'https://fanyi.baidu.com/?aldtype=16047', 'Cookie': 'BAIDUID=DAD75821AD7D4663D00D9C0E7F9C8011:FG=1; BIDUPSID=DAD75821AD7D4663D00D9C0E7F9C8011; PSTM=1553134744; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; delPer=0; H_PS_PSSID=1439_21121_18559_28723_28557_28697_28585_28518_28627_22157; PSINO=2; locale=zh; to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1553218276,1553218292; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1553218292'&#125;# 需要翻译的内容kw = 'thank'# 构建上下文对象content = js2py.EvalJs()with open('test.js', 'r') as f: content.execute(f.read()) sign = content.e(kw)data = &#123; "from": "en", "to": "zh", "query": kw, "transtype": "translang", "simple_means_flag": "3", "sign": sign, "token": "88d322490309c4abee0496f0dbab1a4b",&#125;response = requests.post( url=url, headers=headers, data=data)print(response.json()) 123456789101112131415161718192021222324252627282930313233343536373839404142434445// test.js 文件var i = '320305.131321201';function n(r, o) &#123; for (var t = 0; t &lt; o.length - 2; t += 3) &#123; var a = o.charAt(t + 2); a = a &gt;= "a" ? a.charCodeAt(0) - 87 : Number(a), a = "+" === o.charAt(t + 1) ? r &gt;&gt;&gt; a : r &lt;&lt; a, r = "+" === o.charAt(t) ? r + a &amp; 4294967295 : r ^ a &#125; return r &#125;function e(r) &#123; var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g); if (null === o) &#123; var t = r.length; t &gt; 30 &amp;&amp; (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10)) &#125; else &#123; for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h &gt; C; C++) "" !== e[C] &amp;&amp; f.push.apply(f, a(e[C].split(""))), C !== h - 1 &amp;&amp; f.push(o[C]); var g = f.length; g &gt; 30 &amp;&amp; (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join("")) &#125; var u = void 0 , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107); u = null !== i ? i : (i = window[l] || "") || ""; for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v &lt; r.length; v++) &#123; var A = r.charCodeAt(v); 128 &gt; A ? S[c++] = A : (2048 &gt; A ? S[c++] = A &gt;&gt; 6 | 192 : (55296 === (64512 &amp; A) &amp;&amp; v + 1 &lt; r.length &amp;&amp; 56320 === (64512 &amp; r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 &amp; A) &lt;&lt; 10) + (1023 &amp; r.charCodeAt(++v)), S[c++] = A &gt;&gt; 18 | 240, S[c++] = A &gt;&gt; 12 &amp; 63 | 128) : S[c++] = A &gt;&gt; 12 | 224, S[c++] = A &gt;&gt; 6 &amp; 63 | 128), S[c++] = 63 &amp; A | 128) &#125; for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b &lt; S.length; b++) p += S[b], p = n(p, F); return p = n(p, D), p ^= s, 0 &gt; p &amp;&amp; (p = (2147483647 &amp; p) + 2147483648), p %= 1e6, p.toString() + "." + (p ^ m) &#125;]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>js2py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之Http协议]]></title>
    <url>%2F2018%2F03%2F21%2F%E7%88%AC%E8%99%AB%E4%B9%8BHttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[什么是HTTP协议？Http协议专业称之为：超文本传输协议，属于网络协议中应用层的协议，默认端口是80 HTTP请求格式 案例 HTTP请求方式HTTP请求可以使用多种请求方法 HTTP1.0定义了三种请求方法：GET, POST, HEAD HTTP1.1（主流）新增了五种请求方法：OPTIONS，PUT，DELETE，TRACE，CONNECT 请求方式 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 常见请求头 Cookie User-Agent 浏览器代理 Referer 防盗链，请求来自哪里 Host 请求主机 Connection Accept HTTP响应Http响应由四部分组成 状态行： HTTP/1.1 200 OK 消息报文： Content-Type: text/html….. 空行就是： 消息报文和正文中间的空行 响应正文：大家在浏览器看到的渲染后的内容 HTTP状态码当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 常见的HTTP状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误]]></content>
      <categories>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫开篇]]></title>
    <url>%2F2018%2F03%2F20%2F%E7%88%AC%E8%99%AB%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[当今是一个大数据的时代，数据也变得越来越值钱，有市场就有需要，“爬虫工程师”就此诞生～从此开始学习 Spider，开贴记录一下自己的学习经历, Fighting 什么是爬虫？所谓的爬虫其实就是模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动的去获取网络上的信息 爬虫分为哪些从网上简单的了解，爬虫主要分为通用爬虫（搜索引擎爬虫，典型的就是百度&amp;Google）和聚焦爬虫（爬一些指定的网站） 简单的爬虫流程 1、抓到起始的url，获取响应 2、对响应再次发送请求 3、如果能从响应中提取URL，则继续发送请求获取响应 4、如果提取数据，则将数据进行保存]]></content>
      <categories>
        <category>Spider</category>
      </categories>
      <tags>
        <tag>Spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试题：Python操作Excel]]></title>
    <url>%2F2018%2F03%2F19%2FPython%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9APython%E6%93%8D%E4%BD%9CExcel%2F</url>
    <content type="text"><![CDATA[前几天帮朋友做了一道面试题，感觉用Python做起来会方便很多，话不多说上图 一、模块介绍Python社区的强大和活跃，也为Python成为主流语言奠定了基础，丰富的第三方库使一切变成了可能，主要使用Python的两个模块 xlrd和xlwt 模块，顾名思义xlrd是读excel，xlwt是用来写excel的库，另外这两个库不是Python内置的模块需要自行安装。建议使用 pip 来进行安装 12# -i 可以指定下载的源地址，默认是使用的国外的源比较慢pip install xlrd -i https://pypi.douban.com/simple/ 二、解决题目的思路 1、要获取excel表格里面的数据 2、获取到每一行的数据，然后拼接成字典 3、把每一行拼接的字典放到列表当中 看着是不是很容易，但是如果不熟悉xlrd的语法操作起来也是很困难的，下面就罗列一下基本使用 获取Excel文件对象1data = xlrd.open_workbook(filename)#文件名以及路径，如果路径或者文件名有中文给前面加一个r原生字符。 常用的操作 1、获取data中某一个工作表（sheet） 12345table = data.sheets()[0] # 通过索引顺序获取table = data.sheet_by_index(sheet_indx)) #通过索引顺序获取table = data.sheet_by_name(sheet_name)#通过名称获取 2、行的操作 1234567891011nrows = table.nrows # 获取该sheet中的有效行数table.row(rowx) #返回由该行中所有的单元格对象组成的列表table.row_slice(rowx) #返回由该列中所有的单元格对象组成的列表table.row_types(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据类型组成的列表table.row_values(rowx, start_colx=0, end_colx=None) #返回由该行中所有单元格的数据组成的列表table.row_len(rowx) #返回该列的有效单元格长度 3、列的操作 123456789ncols = table.ncols #获取列表的有效列数table.col(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_slice(colx, start_rowx=0, end_rowx=None) #返回由该列中所有的单元格对象组成的列表table.col_types(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据类型组成的列表table.col_values(colx, start_rowx=0, end_rowx=None) #返回由该列中所有单元格的数据组成的列表 4、单元格的操作 1234567table.cell(rowx,colx) #返回单元格对象table.cell_type(rowx,colx) #返回单元格中的数据类型table.cell_value(rowx,colx) #返回单元格中的数据table.cell_xf_index(rowx, colx) # 暂时还没有搞懂 三、代码奉上 ——&gt;So simple1234567891011121314151617181920212223242526272829# 导入模块import xlrd def Execl2List(): # 1.获取表格 excel = xlrd.open_workbook(r"./lianxi.xlsx") # 2.获取Sheet1表格页 sheet = excel.sheet_by_name("Sheet1") # 3.获取当前页的行数 row = sheet.nrows lists = [] for i in range(1, row): dic = &#123;&#125; list = sheet.row_values(i) # 获取表格对应行的数据 dic["姓名"] = list[0] dic["年龄"] = list[1] dic["性别"] = list[2] lists.append(dic) return listsif __name__ == '__main__': data = Execl2List() print(data)]]></content>
      <categories>
        <category>Interview-Questions</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程池]]></title>
    <url>%2F2018%2F01%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[作用 计算机每次创建线程和销毁线程需要额外占用资源，频繁创建和销毁线程会导致计算机性能下降 处理过程 优点 重复利用线程，减少因为创建线程和销毁线程带来不必要的性能销毁 让程序更加的稳定，不会因为同一时间创建线程过多而导致内存不够使程序引发系统一系列的问题 代码实现流程 1、导入线程池模块 1from multiprocessing.dummy import Pool 2、创建线程池 1pools = Pool(5) 3、定义线程池要执行的任务 12def exec_task(): print('---要执行的任务---') 4、任务完成的回调函数 12def exec_task_finish(self, result): print('执行任务完成回调函数') 注意回调函数必须要有result参数，result参数表示执行任务代码的返回值 5、线程池执行任务 1pools.apply_async(self.exec_task,callback=self.exec_task_finish) 扩展进程池 12# 导入进程池库from multiprocessing import Pool 协程池 12345# 打补丁import gevent.monkeygevent.monkey.patch_all()# 导入协程池库from gevent.pool import Pool 备注，默认线程池里面的线程是守护线程，进程池&amp;协程池同理]]></content>
      <categories>
        <category>Standard-Library</category>
      </categories>
      <tags>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之字典常用方法]]></title>
    <url>%2F2017%2F12%2F24%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%AD%97%E5%85%B8%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[字典是Python内置的数据类型，也是最常见的数据结构。本文记录一下字典使用的时候经常会遇到的API语法。 字典取值1、通过key取value12345678data = &#123;'age': 18, 'name': 'qiangzai'&#125;# 通过key取valuev1 = data['age']print(v1) # 18# 如果key不存在报错KeyErrordata['gender'] # KeyError: 'gender' 2、通过get方式12345678910data = &#123;'age': 18, 'name': 'qiangzai'&#125;v1 = data.get('age')print(v1) # 18v2 = data.get('gender')print(v2) # Nonev3 = data.get('gender', '男')print(v3) # 男 总结：通过get取值不会报错，如果不存在可以指定默认值，不指定默认值为 None 3、通过setdefault方式123456789101112data = &#123;'age': 18, 'name': 'qiangzai'&#125;v1 = data.setdefault('age')print(v1) # 18v2 = data.setdefault('gender')print(v2) # Noneprint(data) # &#123;'age': 18, 'name': 'qiangzai', 'gender': None&#125;v3 = data.setdefault('score', 100)print(v3) # 100print(data) # &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125; 总结：通过sedefault取值也不会报错，如果不存在会返回默认值并给字典添加键值对 删除字典元素1、del123456789data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;# 通过key删除对应的键值对del data['gender']print(data) # &#123;'age': 18, 'name': 'qiangzai', 'score': 100&#125;# 删除整个字典del dataprint(data) # NameError: name 'data' is not defined 总结：del 可以删除指定的键值对,如果key不存在会报错 KeyError，可以删除整个字典 2、clear1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data.clear()print(data) # &#123;&#125; 总结：clear清空整个字典 3、pop123data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;score = data.pop('score')print(score) # 100 总结：pop通过key删除value并返回删除的value值，可以用变量接收 修改字典1、通过key修改1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data['age'] = 20print(data) # &#123;'age': 20, 'name': 'qiangzai', 'gender': None, 'score': 100&#125; 2、update1234data = &#123;'age': 18, 'name': 'qiangzai', 'gender': None, 'score': 100&#125;data.update(&#123;'name': 'LQ', 'class': 'Python'&#125;)print(data) # &#123;'age': 20, 'name': 'LQ', 'gender': None, 'score': 100, 'class': 'Python'&#125; 总结：update会合并字典到data，如果data中的key存在就修改key对应的value 字典的一些内置方法12345678# 返回一个字典的浅复制data.copy() # 创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值data.fromkeys(['age', 'name', 'gender'])# 以列表返回可遍历的(键, 值) 元组数组data.items()# 以列表返回一个字典所有的键data.keys()]]></content>
      <categories>
        <category>Python-Basis</category>
      </categories>
      <tags>
        <tag>dict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之队列Queue]]></title>
    <url>%2F2017%2F11%2F27%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E9%98%9F%E5%88%97Queue%2F</url>
    <content type="text"><![CDATA[Queue Queue是Python标准库中的线程安全的队列(FIFO)实现，提供了一个适用于多线程编程的先进先出的数据结构，即队列，用来在生产者和消费者之间的信息传递。 二种队列形式 FIFO队列FIFO即First in First Out，先进先出。Queue提供了一个基本的FIFO容器，使用方法很简单。 参数：maxsize是一个整数，指明了队列中能存放的数据个数的上限。一旦达到上限，插入会导致堵塞，直到队列中的数据被消费掉。 案例 创建队列 12from queue import Queueq = Queue(maxsize=3) 存取数据 1234# 存放数据q.put('aaa')# 获取数据q.get() LIFO队列LIFO即Last in First Out,后进先出。与栈的类似，使用也很简单,maxsize用法同上 创建队列 12from queue import LifoQueueq = LifoQueue() 存放数据API语法同上 常用方法 task_done() 告诉队列该任务已经处理完毕，队列的unfinished_tasks的属性-1 join() 堵塞调用线程，直到队列中的所有任务被处理完毕，一旦有数据被加入队列，未完成的任务数就会增加。当消费者线程调用task_done()（意味着有消费者取得任务并完成任务），未完成的任务数就会减少。当未完成的任务数降到0（内部就是当队列的unfinished_tasks属性为0时），join()解除阻塞。 put(item[, block[, timeout]]) 将item放入队列中。 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常 get() 从队列中移除并返回一个数据。block跟timeout参数同put方法 其非阻塞方法为｀get_nowait()｀相当与get(False) empty() 如果队列为空，返回True，反之返回False]]></content>
      <categories>
        <category>Standard-Library</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python编程之线程守护和线程同步问题]]></title>
    <url>%2F2017%2F11%2F26%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E5%AE%88%E6%8A%A4%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是守护线程？什么是非守护线程？什么是线程同步？ 目标：搞懂Python中的上面三个问题 非守护线程 线程概念当一个进程启动以后，默认会产生一个主线程，因为线程是程序执行的最小单位，在Python当中线程默认情况下就是setDaemon(False)（非守护线程）,也就是主线程执行完自己的任务退出以后，子线程会继续执行自己的任务，不会随主线程退出受影响 案例1234567891011121314151617import threadingimport timedef run(): time.sleep(2) # 延时等待2s print('---子线程结束执行---')def main(): t1 = threading.Thread(target=run) t1.start() print('---主线程结束执行---')if __name__ == '__main__': main() 输出 12---主线程结束执行------子线程结束执行--- 总结:非守护线程，子线程不随主线程结束而立马结束，而是继续执行 守护线程 当守护线程时，子线程会守护主线程，主线程一旦退出，全部子线程都会被强制终止 案例123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 设置子线程守护主线程 t1.start() print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 线程同步 线程同步就是让线程处于堵塞状态，等待子线程执行以后主线程再执行，也可以设置堵塞时间 案例（主线程堵塞，等待子线程结束以后再执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join() # 主线程堵塞 print('---主线程结束---')if __name__ == '__main__': main() 输出 12---子线程结束------主线程结束--- 案例（主线程堵塞1s，主线程继续执行）123456789101112131415161718import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 12---主线程结束------子线程结束--- 案例（主线程堵塞1s，然后守护主线程）12345678910111213141516171819import threadingimport timedef run(): time.sleep(2) print('---子线程结束---')def main(): t1 = threading.Thread(target=run) t1.setDaemon(True) # 守护主线程 t1.start() t1.join(timeout=1) # 主线程堵塞1s print('---主线程结束---')if __name__ == '__main__': main() 输出 1---主线程结束--- 主线程等待1s后主线程结束，子线程随着主线程结束而结束 总结 1、为了保证程序能够正常的运行，主线程/主进程都会等所有的子进程/子线程执行完成以后再销毁。 2、当主进程内部非守护进程和守护进程同时存在时 主进程执行结束以后会先强制退出守护进程，再等非守护进程结束以后再销毁 3、当主线程内部非守护线程和守护线程同时存在时 主线程执行结束以后会先等待所有的非守护线程结束，再强制退出守护线程，最后销毁]]></content>
      <categories>
        <category>Standard-Library</category>
      </categories>
      <tags>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录之ubuntu16.04安装搜狗输入法]]></title>
    <url>%2F2017%2F11%2F26%2F%E8%AE%B0%E5%BD%95%E4%B9%8Bubuntu16-04%E5%AE%89%E8%A3%85%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[自己本地折腾了一个 ubuntu的desktop版的虚拟机，装完裸机一个，啥也没有。本文介绍一下自己输入法（搜狗）的安装步骤，网上教程一堆但是很多都有问题 1、更新软件源1.1、打开文件1sudo gedit /etc/apt/sources.list 1.2、在文件开头添加下面的阿里云的软件源12345678910deb http://mirrors.aliyun.com/ubuntu/ quantal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ quantal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ quantal-backports main restricted universe multiverse 1.3、更新软件源1sudo apt-get update 2、安装输入法2.1、下载搜狗输入法安装包搜狗输入法Linux版下载地址 2.2、安装12sudo apt-get install -fsudo dpkg -i sogoupinyin_2.0.0.0072_amd64.deb # deb文件要和自己下载的版本一致 2.3设置语言选项到系统设置-&gt;语言支持（System-&gt;Language Support），将键盘输入法系统由默认的iBus设置为fcitx。如下图： 2.4 重启我的重启以后直接点右上角输入法图标就可以看到了，网上有的教程还需要去输入法设置里面添加sougou 3、安装 fcitx键盘输入法系统sogou是基于fcitx的，而系统默认的键盘输入法系统是iBus。Ubuntu 16.04默认是带有fcitx的，正常安装，如果有的话，按上面步骤即可完成；但有些版本的Ubuntu，需要自己安装 fcitx，才能安装使用sogou。 1、添加以下源 sudo add-apt-repository ppa:fcitx-team/nightly 2、更新系统：sudo apt-get update 3、安装fcitx：sudo apt-get install fcitx 4、安装fcitx的配置工具：sudo apt-get install fcitx-config-gtk 5、安装fcitx的table-all软件包:sudo apt-get install fcitx-table-all 6、安装im-switch切换工具：sudo apt-get install im-switch 至此，fcitx键盘输入法系统就安装好了。第5，6步需要按键“Y”确认安装。简单测试的方法就是在终端键入“fcitx”，有各种提示就对了。安装完fcitx后，再安装sogou即可。]]></content>
      <categories>
        <category>Operating-System</category>
      </categories>
      <tags>
        <tag>ubuntu16.04</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo常用操作]]></title>
    <url>%2F2017%2F10%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Tools</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python编程之assert&raise]]></title>
    <url>%2F2017%2F04%2F03%2FPython%E7%BC%96%E7%A8%8B%E4%B9%8Bassert-raise%2F</url>
    <content type="text"><![CDATA[看Python某些库的源码经常会看到 assertor raise这两个内置函数，但是自己平常写代码好像没怎么用过，使用 try except会多一点，可能是自己太low 。。。。 语法介绍 assert assert主要的作用就是给程序断言，声明条件必须为真的判断，如果条件为假则抛出异常，并可以编写异常描述。 语法格式12assert 表达式 '错误描述# 如果表达式为真则继续往下执行，如果为假则抛出断言异常&gt;&gt;AssertionError 案例123foo = [1, 2, 3, 4]# 断言foo列表的长度大于5assert len(foo) &gt; 5, '列表长度不大于5' 输出 1234Traceback (most recent call last): File "/Users/qiangzai/Desktop/practiceCode/ElementaryClassCode/day01/01-list-questions-answers.py", line 3, in &lt;module&gt; assert len(foo) &gt; 5, '列表长度不大于5'AssertionError: 列表长度不大于5 raise raise也是用来抛出异常的，一般用在自己觉得会报错的代码当中使用 语法格式123raise TypeError('错误描述')# 注意这里的TypeError不是固定的，可以是任意自定义的内置错误类型 ValueError,NameError,PermissionError等一堆 案例123456num = 1try: num += '1'except Exception as e: raise TypeError('字符串类型不能和整型相加') 输出 1TypeError: 字符串类型不能和整型相加]]></content>
      <categories>
        <category>Python-Advanced</category>
      </categories>
      <tags>
        <tag>assert</tag>
        <tag>raise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机之UEFI和Legacy]]></title>
    <url>%2F2017%2F03%2F23%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B9%8BUEFI%E5%92%8CLegacy%2F</url>
    <content type="text"><![CDATA[今天主要讲一下UEFI 和Legacy这两种引导启动方式的区别 由于经常帮同学装电脑系统，看到boot模式装UEFI BIOS感觉得心应手，但是有的电脑（尤其是w7系统）老是出现：无法将Windows安装到磁盘0的分区这种问题，后来才明白是 UEFI+GPT 和Legacy+MBR这两种模式区别造成的 解决办法：https://wenku.baidu.com/view/6b6dfb5ef7ec4afe04a1dfb9.html 两种模式运行方式 对比采用传统的BIOS引导启动方式，UEFI BIOS减少了BIOS自检的步骤，节省了大量的时间。UEFI BIOS比传统的BIOS先进得很多，它的标准已经制定了很多年，目前新出厂的电脑基本清一色 UEFI BIOS，但是电脑磁盘的格式必须是GPT格式，这完全不同于传统的MBR格式，所以在使用UEFI BIOS格式给电脑磁盘格式为MBR的装系统时候就会出现上面的问题，需要把电脑磁盘格式从MBR—&gt;GPT]]></content>
      <categories>
        <category>Operating-System</category>
      </categories>
      <tags>
        <tag>UEFI</tag>
        <tag>Legacy</tag>
      </tags>
  </entry>
</search>
